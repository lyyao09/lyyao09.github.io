<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lyyao09.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="云原生知识星球">
<meta property="og:url" content="https://lyyao09.github.io/page/2/index.html">
<meta property="og:site_name" content="云原生知识星球">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="LeaoYao">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://lyyao09.github.io/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>云原生知识星球</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">云原生知识星球</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2023/04/05/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-VMWare%E8%99%9A%E6%8B%9F%E5%8C%96%E7%8E%AF%E5%A2%83%E4%B8%8BPod%E8%B7%A8VXLAN%E9%80%9A%E4%BF%A1%E5%BC%82%E5%B8%B8%EF%BC%88%E7%BB%AD%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/04/05/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-VMWare%E8%99%9A%E6%8B%9F%E5%8C%96%E7%8E%AF%E5%A2%83%E4%B8%8BPod%E8%B7%A8VXLAN%E9%80%9A%E4%BF%A1%E5%BC%82%E5%B8%B8%EF%BC%88%E7%BB%AD%EF%BC%89/" class="post-title-link" itemprop="url">K8S问题排查-VMWare虚拟化环境下Pod跨VXLAN通信异常（续）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-04-05 21:38:44" itemprop="dateCreated datePublished" datetime="2023-04-05T21:38:44+00:00">2023-04-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:10:03" itemprop="dateModified" datetime="2024-04-14T02:10:03+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>这个问题[1]定位到<code>VMWare</code>虚拟化层面后就一直搁置下来了，但考虑到测试环境存在大量虚拟机部署的情况，并且支持虚拟化部署也是早晚的事，所以后续也在一直关注<code>VMWare</code>和<code>Redhat</code>的相关资料。</p>
<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>幸运的是，从<code>Redhat</code>的官网里还真的找到了相关问题[2]，通过官网说明了解到，该问题出现的环境是<code>Redhat 8.3</code>和<code>8.4</code>，使用<code>vmxnet3</code>的适配器，并且使用<code>UDP</code>隧道协议，比如<code>vxlan</code>或<code>GRE</code>。</p>
<p>官方给出的解决方案是：</p>
<ol>
<li>升级<code>Redhat</code>到<code>8.5 (kernel-4.18.0-348.el8)</code>及以后版本。</li>
<li>升级<code>VMware ESXi</code>到<code>6.7P07</code>，<code>7.0U3 (7.0.3)</code> 及以后版本。</li>
</ol>
<p>上述更新包括对<code>vmxnet3 NIC</code>不支持的隧道禁用 <code>tx-checksum-ip-generic</code> 的逻辑，因此最终结果与下面的解决方法相同。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ethtool -K DEVNAME tx-checksum-ip-generic off</span><br></pre></td></tr></table></figure>

<p>但是，实测结果显示，<strong>升级<code>Redhat</code>和<code>VMware</code>均无法解决</strong>，而临时禁用的命令是可以的。考虑到临时命令还涉及到持久化的问题，还是需要另找方法。</p>
<p>既然使用<code>vmxnet3</code>网卡不行，是不是能换网卡类型？果然，根据这个思路，继续查找资料[3]，发现使用<code>E1000</code>类型的网卡可以解决该问题，实测结果也符合预期。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p><strong>临时方案：</strong>在创建虚拟机时，把网络适配器的类型改为 <code>E1000</code>或<code>E1000e</code>。</p>
<p><strong>永久方案：</strong>依然需要等待<code>VMWare</code>和<code>Redhat</code>的官方修复。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://lyyao09.github.io/2022/06/05/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-VMWare%E8%99%9A%E6%8B%9F%E5%8C%96%E7%8E%AF%E5%A2%83%E4%B8%8BPod%E8%B7%A8VXLAN%E9%80%9A%E4%BF%A1%E5%BC%82%E5%B8%B8/">https://lyyao09.github.io/2022/06/05/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-VMWare%E8%99%9A%E6%8B%9F%E5%8C%96%E7%8E%AF%E5%A2%83%E4%B8%8BPod%E8%B7%A8VXLAN%E9%80%9A%E4%BF%A1%E5%BC%82%E5%B8%B8/</a></li>
<li><a target="_blank" rel="noopener" href="https://access.redhat.com/solutions/5881451">https://access.redhat.com/solutions/5881451</a> </li>
<li><a target="_blank" rel="noopener" href="https://zhangguanzhang.github.io/2022/07/28/redhat84-vxlan-esxi">https://zhangguanzhang.github.io/2022/07/28/redhat84-vxlan-esxi</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2023/03/25/tools/%E5%B7%A5%E5%85%B7%E5%88%86%E4%BA%AB-%E4%BD%BF%E7%94%A8Kubeasz%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2K8S%E9%9B%86%E7%BE%A4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/03/25/tools/%E5%B7%A5%E5%85%B7%E5%88%86%E4%BA%AB-%E4%BD%BF%E7%94%A8Kubeasz%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2K8S%E9%9B%86%E7%BE%A4/" class="post-title-link" itemprop="url">工具分享-使用Kubeasz一键部署K8S集群</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-03-25 21:13:43" itemprop="dateCreated datePublished" datetime="2023-03-25T21:13:43+00:00">2023-03-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:10:03" itemprop="dateModified" datetime="2024-04-14T02:10:03+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>15 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>为了验证最新版本的<code>k8s</code>是否已修复某个<code>bug</code>，需要快速搭建一个<code>k8s</code>环境，本文选取资料[1]中的<code>kubeasz</code>工具，并记录部署过程及相关问题。</p>
<h2 id="部署过程"><a href="#部署过程" class="headerlink" title="部署过程"></a>部署过程</h2><p>先下载工具脚本、<code>kubeasz</code>代码、二进制、默认容器镜像。</p>
<p>使用如下命令开始安装：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 k8s]# ./ezdown -S</span><br><span class="line">2023-03-22 13:39:40 INFO Action begin: start_kubeasz_docker</span><br><span class="line">2023-03-22 13:39:41 INFO try to run kubeasz in a container</span><br><span class="line">2023-03-22 13:39:41 DEBUG get host IP: 10.10.11.49</span><br><span class="line">2023-03-22 13:39:41 DEBUG generate ssh key pair</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">10.10.11.49 SSH-2.0-OpenSSH_6.6.1</span></span><br><span class="line">f1b442b7fdaf757c7787536b17d12d76208a2dd7884d56fbd1d35817dc2e94ca</span><br><span class="line">2023-03-22 13:39:41 INFO Action successed: start_kubeasz_docker</span><br><span class="line"></span><br><span class="line">[root@node01 k8s]# docker ps</span><br><span class="line">CONTAINER ID   IMAGE                                                       COMMAND                  CREATED          STATUS          PORTS     NAMES</span><br><span class="line">f1b442b7fdaf   easzlab/kubeasz:3.5.0                                       &quot;sleep 36000&quot;            15 seconds ago   Up 14 seconds             kubeasz</span><br></pre></td></tr></table></figure>

<p>执行后看不出是成功，还是失败。根据文档说明，进入容器内手动执行命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# docker exec -it kubeasz ezctl start-aio</span><br><span class="line">2023-03-22 06:15:05 INFO get local host ipadd: 10.10.11.49</span><br><span class="line">2023-03-22 06:15:05 DEBUG generate custom cluster files in /etc/kubeasz/clusters/default</span><br><span class="line">2023-03-22 06:15:05 DEBUG set versions</span><br><span class="line">2023-03-22 06:15:05 DEBUG disable registry mirrors</span><br><span class="line">2023-03-22 06:15:05 DEBUG cluster default: files successfully created.</span><br><span class="line">2023-03-22 06:15:05 INFO next steps 1: to config &#x27;/etc/kubeasz/clusters/default/hosts&#x27;</span><br><span class="line">2023-03-22 06:15:05 INFO next steps 2: to config &#x27;/etc/kubeasz/clusters/default/config.yml&#x27;</span><br><span class="line">ansible-playbook -i clusters/default/hosts -e @clusters/default/config.yml  playbooks/90.setup.yml</span><br><span class="line">2023-03-22 06:15:05 INFO cluster:default setup step:all begins in 5s, press any key to abort:</span><br><span class="line"></span><br><span class="line">PLAY [kube_master,kube_node,etcd,ex_lb,chrony] **********************************************************************************************************************************************************</span><br><span class="line"></span><br><span class="line">TASK [Gathering Facts] **********************************************************************************************************************************************************************************</span><br><span class="line">fatal: [10.10.11.49]: UNREACHABLE! =&gt; &#123;&quot;changed&quot;: false, &quot;msg&quot;: &quot;Failed to connect to the host via ssh: root@10.10.11.49: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).&quot;, &quot;unreachable&quot;: true&#125;</span><br><span class="line"></span><br><span class="line">PLAY RECAP **********************************************************************************************************************************************************************************************</span><br><span class="line">10.10.11.49               : ok=0    changed=0    unreachable=1    failed=0    skipped=0    rescued=0    ignored=0</span><br></pre></td></tr></table></figure>

<p>从日志看，提示权限有问题。实际测试可以正常的<code>ssh</code>免密登录：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">bash-5.1# ssh-keygen</span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/root/.ssh/id_rsa):</span><br><span class="line">/root/.ssh/id_rsa already exists.</span><br><span class="line">Overwrite (y/n)?</span><br><span class="line">bash-5.1# ssh-copy-id root@10.10.11.49</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed</span><br><span class="line">expr: warning: &#x27;^ERROR: &#x27;: using &#x27;^&#x27; as the first character</span><br><span class="line">of a basic regular expression is not portable; it is ignored</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys</span><br><span class="line">root@10.10.11.49&#x27;s password:</span><br><span class="line"></span><br><span class="line">Number of key(s) added: 1</span><br><span class="line"></span><br><span class="line">Now try logging into the machine, with:   &quot;ssh &#x27;root@10.10.11.49&#x27;&quot;</span><br><span class="line">and check to make sure that only the key(s) you wanted were added.</span><br><span class="line"></span><br><span class="line">bash-5.1# ssh root@10.10.11.49</span><br><span class="line">root@10.10.11.49&#x27;s password:</span><br></pre></td></tr></table></figure>

<p>查看相关配置文件，权限正常：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 kubeasz]# ll ~/.ssh</span><br><span class="line">total 16</span><br><span class="line">-rw------- 1 root root 1752 Mar 22 14:25 authorized_keys</span><br><span class="line">-rw------- 1 root root 2602 Mar 22 14:25 id_rsa</span><br><span class="line">-rw-r--r-- 1 root root  567 Mar 22 14:25 id_rsa.pub</span><br><span class="line">-rw-r--r-- 1 root root 1295 Mar 22 13:39 known_hosts</span><br></pre></td></tr></table></figure>

<p>不清楚具体哪里有问题，参考资料[2]，尝试改为用用户名密码执行。</p>
<p>在容器内配置用户密码，检查通过：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">bash-5.1# vi /etc/ansible/hosts</span><br><span class="line">[webservers]</span><br><span class="line">10.10.11.49</span><br><span class="line"></span><br><span class="line">[webservers:vars]</span><br><span class="line">ansible_ssh_pass=&#x27;******&#x27;</span><br><span class="line">ansible_ssh_user=&#x27;root&#x27;</span><br><span class="line"></span><br><span class="line">bash-5.1# ansible webservers -m ping</span><br><span class="line">10.10.11.49 | SUCCESS =&gt; &#123;</span><br><span class="line">    &quot;ansible_facts&quot;: &#123;</span><br><span class="line">        &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;changed&quot;: false,</span><br><span class="line">    &quot;ping&quot;: &quot;pong&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>修改安装集群依赖的<code>clusters/default/hosts</code>文件，同样增加用户密码配置：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[etcd]</span><br><span class="line">10.10.11.49</span><br><span class="line"></span><br><span class="line">[etcd:vars]</span><br><span class="line">ansible_ssh_pass=&#x27;******&#x27;</span><br><span class="line">ansible_ssh_user=&#x27;root&#x27;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">master node(s)</span></span><br><span class="line">[kube_master]</span><br><span class="line">10.10.11.49</span><br><span class="line"></span><br><span class="line">[kube_master:vars]</span><br><span class="line">ansible_ssh_pass=&#x27;******&#x27;</span><br><span class="line">ansible_ssh_user=&#x27;root&#x27;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">work node(s)</span></span><br><span class="line">[kube_node]</span><br><span class="line">10.10.11.49</span><br><span class="line"></span><br><span class="line">[kube_node:vars]</span><br><span class="line">ansible_ssh_pass=&#x27;******&#x27;</span><br><span class="line">ansible_ssh_user=&#x27;root&#x27;</span><br></pre></td></tr></table></figure>

<p>执行命令，提示缺少<code>sshpass</code>工具：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 kubeasz]# docker exec -it kubeasz ezctl setup default all</span><br><span class="line">ansible-playbook -i clusters/default/hosts -e @clusters/default/config.yml  playbooks/90.setup.yml</span><br><span class="line">2023-03-22 07:35:46 INFO cluster:default setup step:all begins in 5s, press any key to abort:</span><br><span class="line"></span><br><span class="line">PLAY [kube_master,kube_node,etcd,ex_lb,chrony] **********************************************************************************************************************************************************</span><br><span class="line"></span><br><span class="line">TASK [Gathering Facts] **********************************************************************************************************************************************************************************</span><br><span class="line">fatal: [10.10.11.49]: FAILED! =&gt; &#123;&quot;msg&quot;: &quot;to use the &#x27;ssh&#x27; connection type with passwords, you must install the sshpass program&quot;&#125;</span><br><span class="line"></span><br><span class="line">PLAY RECAP **********************************************************************************************************************************************************************************************</span><br><span class="line">10.10.11.49               : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0</span><br></pre></td></tr></table></figure>

<p>安装<code>sshpass</code>依赖包：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bash-5.1# apk add sshpass</span><br><span class="line">fetch https://dl-cdn.alpinelinux.org/alpine/v3.16/main/x86_64/APKINDEX.tar.gz</span><br><span class="line">fetch https://dl-cdn.alpinelinux.org/alpine/v3.16/community/x86_64/APKINDEX.tar.gz</span><br><span class="line">(1/1) Installing sshpass (1.09-r0)</span><br><span class="line">Executing busybox-1.35.0-r17.trigger</span><br><span class="line">OK: 21 MiB in 47 packages</span><br></pre></td></tr></table></figure>

<p>重复执行命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 kubeasz]# docker exec -it kubeasz ezctl setup default all</span><br><span class="line">ansible-playbook -i clusters/default/hosts -e @clusters/default/config.yml  playbooks/90.setup.yml</span><br><span class="line">2023-03-22 07:36:37 INFO cluster:default setup step:all begins in 5s, press any key to abort:</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">TASK [kube-node : 轮询等待kube-proxy启动] *********************************************************************************************************************************************************************</span><br><span class="line">changed: [10.10.11.49]</span><br><span class="line">FAILED - RETRYING: 轮询等待kubelet启动 (4 retries left).</span><br><span class="line">FAILED - RETRYING: 轮询等待kubelet启动 (3 retries left).</span><br><span class="line">FAILED - RETRYING: 轮询等待kubelet启动 (2 retries left).</span><br><span class="line">FAILED - RETRYING: 轮询等待kubelet启动 (1 retries left).</span><br><span class="line"></span><br><span class="line">TASK [kube-node : 轮询等待kubelet启动] ************************************************************************************************************************************************************************</span><br><span class="line">fatal: [10.10.11.49]: FAILED! =&gt; &#123;&quot;attempts&quot;: 4, &quot;changed&quot;: true, &quot;cmd&quot;: &quot;systemctl is-active kubelet.service&quot;, &quot;delta&quot;: &quot;0:00:00.014621&quot;, &quot;end&quot;: &quot;2023-03-22 15:42:07.230186&quot;, &quot;msg&quot;: &quot;non-zero return code&quot;, &quot;rc&quot;: 3, &quot;start&quot;: &quot;2023-03-22 15:42:07.215565&quot;, &quot;stderr&quot;: &quot;&quot;, &quot;stderr_lines&quot;: [], &quot;stdout&quot;: &quot;activating&quot;, &quot;stdout_lines&quot;: [&quot;activating&quot;]&#125;</span><br><span class="line"></span><br><span class="line">PLAY RECAP **********************************************************************************************************************************************************************************************</span><br><span class="line">10.10.11.49               : ok=85   changed=78   unreachable=0    failed=1    skipped=123  rescued=0    ignored=0</span><br><span class="line">localhost                  : ok=33   changed=30   unreachable=0    failed=0    skipped=11   rescued=0    ignored=0</span><br></pre></td></tr></table></figure>

<p><code>kubelet</code>阶段失败，查看<code>kubelet</code>服务：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 log]# service kubelet status -l</span><br><span class="line">Redirecting to /bin/systemctl status  -l kubelet.service</span><br><span class="line">● kubelet.service - Kubernetes Kubelet</span><br><span class="line">   Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: activating (auto-restart) (Result: exit-code) since Wed 2023-03-22 15:56:31 CST; 1s ago</span><br><span class="line">     Docs: https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">  Process: 147581 ExecStart=/opt/kube/bin/kubelet --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///run/containerd/containerd.sock --hostname-override=10.10.11.49 --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --root-dir=/var/lib/kubelet --v=2 (code=exited, status=1/FAILURE)</span><br><span class="line"> Main PID: 147581 (code=exited, status=1/FAILURE)</span><br><span class="line"></span><br><span class="line">Mar 22 15:56:31 node01 kubelet[147581]: I0322 15:56:31.719832  147581 manager.go:228] Version: &#123;KernelVersion:3.10.0-862.11.6.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion: DockerAPIVersion: CadvisorVersion: CadvisorRevision:&#125;</span><br><span class="line">Mar 22 15:56:31 node01 kubelet[147581]: I0322 15:56:31.720896  147581 server.go:659] &quot;--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /&quot;</span><br><span class="line">Mar 22 15:56:31 node01 kubelet[147581]: I0322 15:56:31.721939  147581 container_manager_linux.go:267] &quot;Container manager verified user specified cgroup-root exists&quot; cgroupRoot=[]</span><br><span class="line">Mar 22 15:56:31 node01 kubelet[147581]: I0322 15:56:31.722392  147581 container_manager_linux.go:272] &quot;Creating Container Manager object based on Node Config&quot; nodeConfig=&#123;RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName:</span><br><span class="line">Mar 22 15:56:31 node01 kubelet[147581]: I0322 15:56:31.722503  147581 topology_manager.go:134] &quot;Creating topology manager with policy per scope&quot; topologyPolicyName=&quot;none&quot; topologyScopeName=&quot;container&quot;</span><br><span class="line">Mar 22 15:56:31 node01 kubelet[147581]: I0322 15:56:31.722609  147581 container_manager_linux.go:308] &quot;Creating device plugin manager&quot;</span><br><span class="line">Mar 22 15:56:31 node01 kubelet[147581]: I0322 15:56:31.722689  147581 manager.go:125] &quot;Creating Device Plugin manager&quot; path=&quot;/var/lib/kubelet/device-plugins/kubelet.sock&quot;</span><br><span class="line">Mar 22 15:56:31 node01 kubelet[147581]: I0322 15:56:31.722763  147581 server.go:66] &quot;Creating device plugin registration server&quot; version=&quot;v1beta1&quot; socket=&quot;/var/lib/kubelet/device-plugins/kubelet.sock&quot;</span><br><span class="line">Mar 22 15:56:31 node01 kubelet[147581]: I0322 15:56:31.722905  147581 state_mem.go:36] &quot;Initialized new in-memory state store&quot;</span><br><span class="line">Mar 22 15:56:31 node01 kubelet[147581]: E0322 15:56:31.726502  147581 run.go:74] &quot;command failed&quot; err=&quot;failed to run Kubelet: validate service connection: CRI v1 runtime API is not implemented for endpoint \&quot;unix:///run/containerd/containerd.sock\&quot;: rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService&quot;</span><br></pre></td></tr></table></figure>

<p>根据日志报错，参考资料[3]，删除<code> /etc/containerd/config.toml</code> 文件并重启<code> containerd</code> 即可：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> /etc/containerd/config.toml /root/config.toml.bak</span><br><span class="line">systemctl restart containerd</span><br></pre></td></tr></table></figure>

<p>重复执行命令，后台查看发现<code>calico-node</code>启动失败，查看日志如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Events:</span><br><span class="line">  Type     Reason     Age                From               Message</span><br><span class="line">  ----     ------     ----               ----               -------</span><br><span class="line">  Normal   Scheduled  41s                default-scheduler  Successfully assigned kube-system/calico-node-rqpjm to 10.10.11.49</span><br><span class="line">  Normal   Pulling    20s (x2 over 31s)  kubelet            Pulling image &quot;easzlab.io.local:5000/calico/cni:v3.23.5&quot;</span><br><span class="line">  Warning  Failed     19s (x2 over 31s)  kubelet            Failed to pull image &quot;easzlab.io.local:5000/calico/cni:v3.23.5&quot;: rpc error: code = Unknown desc = failed to pull and unpack image &quot;easzlab.io.local:5000/calico/cni:v3.23.5&quot;: failed to resolve reference &quot;easzlab.io.local:5000/calico/cni:v3.23.5&quot;: failed to do request: Head &quot;https://easzlab.io.local:5000/v2/calico/cni/manifests/v3.23.5&quot;: http: server gave HTTP response to HTTPS client</span><br><span class="line">  Warning  Failed     19s (x2 over 31s)  kubelet            Error: ErrImagePull</span><br><span class="line">  Normal   BackOff    5s (x2 over 30s)   kubelet            Back-off pulling image &quot;easzlab.io.local:5000/calico/cni:v3.23.5&quot;</span><br><span class="line">  Warning  Failed     5s (x2 over 30s)   kubelet            Error: ImagePullBackOff</span><br></pre></td></tr></table></figure>

<p>查看<code>docker</code>层面配置，并测试拉起镜像正常：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# cat /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;max-concurrent-downloads&quot;: 10,</span><br><span class="line">  &quot;insecure-registries&quot;: [&quot;easzlab.io.local:5000&quot;],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-level&quot;: &quot;warn&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;10m&quot;,</span><br><span class="line">    &quot;max-file&quot;: &quot;3&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">  &quot;data-root&quot;:&quot;/var/lib/docker&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">[root@node01 log]# docker pull easzlab.io.local:5000/calico/cni:v3.23.5</span><br><span class="line">v3.23.5: Pulling from calico/cni</span><br><span class="line">Digest: sha256:9c5055a2b5bc0237ab160aee058135ca9f2a8f3c3eee313747a02edcec482f29</span><br><span class="line">Status: Image is up to date for easzlab.io.local:5000/calico/cni:v3.23.5</span><br><span class="line">easzlab.io.local:5000/calico/cni:v3.23.5</span><br></pre></td></tr></table></figure>

<p>查看<code>containerd</code>层面，并测试拉起镜像也正常：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 log]# ctr image pull --plain-http=true easzlab.io.local:5000/calico/cni:v3.23.5</span><br><span class="line">easzlab.io.local:5000/calico/cni:v3.23.5:                                         resolved       |++++++++++++++++++++++++++++++++++++++|</span><br><span class="line">manifest-sha256:9c5055a2b5bc0237ab160aee058135ca9f2a8f3c3eee313747a02edcec482f29: done           |++++++++++++++++++++++++++++++++++++++|</span><br><span class="line">layer-sha256:cc0e45adf05a30a90384ba7024dbabdad9ae0bcd7b5a535c28dede741298fea3:    done           |++++++++++++++++++++++++++++++++++++++|</span><br><span class="line">layer-sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1:    done           |++++++++++++++++++++++++++++++++++++++|</span><br><span class="line">layer-sha256:47c5dbbec31222325790ebad8c07d270a63689bd10dc8f54115c65db7c30ad1f:    done           |++++++++++++++++++++++++++++++++++++++|</span><br><span class="line">layer-sha256:8efc3d73e2741a93be09f68c859da466f525b9d0bddb1cd2b2b633f14f232941:    done           |++++++++++++++++++++++++++++++++++++++|</span><br><span class="line">config-sha256:1c979d623de9aef043cb4ff489da5636d61c39e30676224af0055240e1816382:   done           |++++++++++++++++++++++++++++++++++++++|</span><br><span class="line">layer-sha256:4c98a4f67c5a7b1058111d463051c98b23e46b75fc943fc2535899a73fc0c9f1:    done           |++++++++++++++++++++++++++++++++++++++|</span><br><span class="line">layer-sha256:51729c6e2acda05a05e203289f5956954814d878f67feb1a03f9941ec5b4008b:    done           |++++++++++++++++++++++++++++++++++++++|</span><br><span class="line">layer-sha256:050b055d5078c5c6ad085d106c232561b0c705aa2173edafd5e7a94a1e908fc5:    done           |++++++++++++++++++++++++++++++++++++++|</span><br><span class="line">layer-sha256:7430548aa23e56c14da929bbe5e9a2af0f9fd0beca3bd95e8925244058b83748:    done           |++++++++++++++++++++++++++++++++++++++|</span><br><span class="line">elapsed: 3.1 s                                                                    total:  103.0  (33.2 MiB/s)</span><br><span class="line">unpacking linux/amd64 sha256:9c5055a2b5bc0237ab160aee058135ca9f2a8f3c3eee313747a02edcec482f29...</span><br><span class="line">done: 6.82968396s</span><br></pre></td></tr></table></figure>

<p>根据资料[4]，查看<code>containerd</code>配置，并新增私有仓库的配置：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# containerd config default &gt; /etc/containerd/config.toml</span><br><span class="line"></span><br><span class="line">[root@node01 ~]# vim  /etc/containerd/config.toml</span><br><span class="line">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry]</span><br><span class="line">      config_path = &quot;&quot;</span><br><span class="line"></span><br><span class="line">      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.auths]</span><br><span class="line"></span><br><span class="line">      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs]</span><br><span class="line"></span><br><span class="line">      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.headers]</span><br><span class="line"></span><br><span class="line">      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors]</span><br><span class="line">        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;easzlab.io.local:5000&quot;]</span><br><span class="line">          endpoint = [&quot;http://easzlab.io.local:5000&quot;]</span><br><span class="line"></span><br><span class="line">[root@node01 ~]# service containerd restart</span><br></pre></td></tr></table></figure>

<p>查看<code>pod</code>状态，又卡在了<code>ContainerCreating</code>状态：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# kubectl get pod -A</span><br><span class="line">NAMESPACE     NAME                                         READY   STATUS              RESTARTS   AGE</span><br><span class="line">kube-system   calico-kube-controllers-89b744d6c-klzwh      1/1     Running             0          5m35s</span><br><span class="line">kube-system   calico-node-wmvff                            1/1     Running             0          5m35s</span><br><span class="line">kube-system   coredns-6665999d97-mp7xc                     0/1     ContainerCreating   0          5m35s</span><br><span class="line">kube-system   dashboard-metrics-scraper-57566685b4-8q5fm   0/1     ContainerCreating   0          5m35s</span><br><span class="line">kube-system   kubernetes-dashboard-57db9bfd5b-h6jp4        0/1     ContainerCreating   0          5m35s</span><br><span class="line">kube-system   metrics-server-6bd9f986fc-njpnj              0/1     ContainerCreating   0          5m35s</span><br><span class="line">kube-system   node-local-dns-wz9bg                         1/1     Running             0          5m31s</span><br></pre></td></tr></table></figure>

<p>选择一个<code>describe</code>查看：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Events:</span><br><span class="line">  Type     Reason                  Age                   From               Message</span><br><span class="line">  ----     ------                  ----                  ----               -------</span><br><span class="line">  Warning  FailedScheduling        6m7s                  default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint &#123;node.kubernetes.io/not-ready: &#125;. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling..</span><br><span class="line">  Normal   Scheduled               5m47s                 default-scheduler  Successfully assigned kube-system/coredns-6665999d97-mp7xc to 10.10.11.49</span><br><span class="line">  Warning  FailedCreatePodSandBox  5m46s                 kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox &quot;072c164d79f4874a8d851d36115ea04b75a2155dae3cecdc764e923c9f38f86b&quot;: plugin type=&quot;calico&quot; failed (add): failed to find plugin &quot;calico&quot; in path [/opt/cni/bin]</span><br><span class="line">  Normal   SandboxChanged          33s (x25 over 5m46s)  kubelet            Pod sandbox changed, it will be killed and re-created.</span><br></pre></td></tr></table></figure>

<p>从日志看，是<code>cni</code>插件不存在的问题，手动拷贝之后，查看<code>pod</code>状态：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 bin]# cd /opt/cni/bin/</span><br><span class="line">[root@node01 bin]# chmod +x *</span><br><span class="line">[root@node01 bin]# ll -h</span><br><span class="line">total 186M</span><br><span class="line">-rwxr-xr-x 1 root root 3.7M Mar 22 17:46 bandwidth</span><br><span class="line">-rwxr-xr-x 1 root root  56M Mar 22 17:46 calico</span><br><span class="line">-rwxr-xr-x 1 root root  56M Mar 22 17:46 calico-ipam</span><br><span class="line">-rwxr-xr-x 1 root root 2.4M Mar 22 17:46 flannel</span><br><span class="line">-rwxr-xr-x 1 root root 3.1M Mar 22 17:46 host-local</span><br><span class="line">-rwxr-xr-x 1 root root  56M Mar 22 17:46 install</span><br><span class="line">-rwxr-xr-x 1 root root 3.2M Mar 22 17:46 loopback</span><br><span class="line">-rwxr-xr-x 1 root root 3.6M Mar 22 17:46 portmap</span><br><span class="line">-rwxr-xr-x 1 root root 3.3M Mar 22 17:46 tuning</span><br><span class="line"></span><br><span class="line">[root@node01 bin]# kubectl get pod -A</span><br><span class="line">NAMESPACE     NAME                                         READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system   calico-kube-controllers-89b744d6c-mpfgq      1/1     Running   0          37m</span><br><span class="line">kube-system   calico-node-h9sm2                            1/1     Running   0          37m</span><br><span class="line">kube-system   coredns-6665999d97-8pdbd                     1/1     Running   0          37m</span><br><span class="line">kube-system   dashboard-metrics-scraper-57566685b4-c2l8w   1/1     Running   0          37m</span><br><span class="line">kube-system   kubernetes-dashboard-57db9bfd5b-74lmb        1/1     Running   0          37m</span><br><span class="line">kube-system   metrics-server-6bd9f986fc-d9crl              1/1     Running   0          37m</span><br><span class="line">kube-system   node-local-dns-kvgv6                         1/1     Running   0          37m</span><br></pre></td></tr></table></figure>

<p>部署完成。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://github.com/easzlab/kubeasz/blob/master/docs/setup/quickStart.md">https://github.com/easzlab/kubeasz/blob/master/docs/setup/quickStart.md</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/c48b4a24c7d4">https://www.jianshu.com/p/c48b4a24c7d4</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/immaxfang/p/16721407.html">https://www.cnblogs.com/immaxfang/p/16721407.html</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/containerd/containerd/issues/4938">https://github.com/containerd/containerd/issues/4938</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2022/11/27/tools/%E5%B7%A5%E5%85%B7%E5%88%86%E4%BA%AB-%E5%86%85%E7%BD%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AEGoland%E7%BD%91%E9%A1%B5%E7%89%88%E6%95%99%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/27/tools/%E5%B7%A5%E5%85%B7%E5%88%86%E4%BA%AB-%E5%86%85%E7%BD%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AEGoland%E7%BD%91%E9%A1%B5%E7%89%88%E6%95%99%E7%A8%8B/" class="post-title-link" itemprop="url">工具分享-内网环境配置Goland网页版教程</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-11-27 20:25:11" itemprop="dateCreated datePublished" datetime="2022-11-27T20:25:11+00:00">2022-11-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:10:03" itemprop="dateModified" datetime="2024-04-14T02:10:03+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tools/" itemprop="url" rel="index"><span itemprop="name">tools</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>说起开发工具<code>Goland</code>，做<code>Go</code>语言开发的同学应该都不陌生，但由于大部分同学的电脑资源有限，尤其是公司里配备的电脑，在本地使用<code>Goland</code>多多少少有些不够顺畅。</p>
<p>如果公司内服务器资源充足，再加上容器化技术的加持，把<code>Goland</code>以容器的形式部署在服务器上运行是一个不错的解决方法。带着这个想法搜索资料[1]发现，<code>Goland </code>官方还真的开发了容器版，并且提供网页和客户端两种登录方式。下面给出<strong>内网环境下</strong>的配置步骤及采坑记录：</p>
<blockquote>
<p>注：</p>
<ol>
<li>以下操作同样适用于<code>Jetbrains</code>旗下的其他开发工具，比如<code>IDEA</code>。</li>
<li><code>Goland</code>的注册方法不在本次教程讨论范围之内，请自行解决。</li>
</ol>
</blockquote>
<h2 id="镜像获取"><a href="#镜像获取" class="headerlink" title="镜像获取"></a>镜像获取</h2><p><code>Goland</code>网页版功能是<code>jetbrains</code>官方[2]提供的<code>Docker</code>镜像，所以内网配置的<strong>前提</strong>是先从外网拉取到需要的镜像，然后导出镜像包并拷贝到内网中：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.jetbrains.team/p/prj/containers/projector-goland</span><br><span class="line">docker save -o projector-goland.tar registry.jetbrains.team/p/prj/containers/projector-goland</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注：如果无法拉取官方的镜像，可以在公众号后台回复 <code>docker goland</code> 即可获取<code>goland</code> 网页版镜像。</p>
</blockquote>
<h2 id="服务启动"><a href="#服务启动" class="headerlink" title="服务启动"></a>服务启动</h2><p>拿到镜像后，找一个安装了<code>docker</code>的服务器或虚机，使用<code>docker run</code>命令启动：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">docker run -itd \</span><br><span class="line">           -u root \</span><br><span class="line">           -p 8887:8887 \</span><br><span class="line">           --net=host \</span><br><span class="line">           --privileged \</span><br><span class="line">           -v /home/admin/goland-dir:/root \</span><br><span class="line">           -v /etc/localtime:/etc/localtime \</span><br><span class="line">           -v /home/admin/goland-dir/sources.list:/etc/apt/sources.list \</span><br><span class="line">           --name goland \</span><br><span class="line">           --restart always \</span><br><span class="line">           registry.jetbrains.team/p/prj/containers/projector-goland</span><br></pre></td></tr></table></figure>

<p>（<strong>重要</strong>）部分参数说明：</p>
<ol>
<li>指定用户：可选，默认不指定用户，容器启动时会使用一个<code>非root</code>用户<code>projector-user</code>，这里使用<code>root</code>用户启动是为了避免后续操作的权限问题；</li>
<li>指定主机网络：可选，方便使用代理拉取代码，没有代理的话先从外网下载也可以；</li>
<li>指定特权模式：可选，方便调试，没有开启的话直接使用<code>GoLand</code>调试会提示权限问题；</li>
<li>挂载点1：<strong>必选</strong>，默认用户下，将<code>/home/projector-user</code>挂载到本地，root用户下直接将<code>root</code>目录挂载到本地；</li>
<li>挂载点2：可选，保持容器时间与主机时间一致；</li>
<li>挂载点3：可选，配置内网依赖源，方便下载<code>gcc</code>等编译所需的依赖；</li>
</ol>
<h2 id="浏览器访问"><a href="#浏览器访问" class="headerlink" title="浏览器访问"></a>浏览器访问</h2><p>容器正常启动后，在浏览器中通过<code>http://x.x.x.x:8887</code>地址登录网页版<code>GoLand</code>。</p>
<h2 id="客户端访问"><a href="#客户端访问" class="headerlink" title="客户端访问"></a>客户端访问</h2><p>如果不习惯使用浏览器，官方还提供了原生客户端，我们通过地址[3]下载，打开后输入地址即可。</p>
<h2 id="导入项目示例"><a href="#导入项目示例" class="headerlink" title="导入项目示例"></a>导入项目示例</h2><p>以导入<code>K8S</code>源码为例，登录到容器内，使用<code>git</code>命令拉取<code>kubernetes</code>源码：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">projector-user@storage:~/go/src/github.com$ git clone https://github.com/kubernetes/kubernetes.git</span><br><span class="line">Cloning into &#x27;kubernetes&#x27;...</span><br><span class="line">fatal: unable to access &#x27;https://github.com/kubernetes/kubernetes.git/&#x27;: server certificate verification failed. CAfile: none CRLfile: none</span><br></pre></td></tr></table></figure>

<p>拉取失败，提示<code>CA</code>证书问题，通过以下命令解决：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global http.sslVerify false</span><br></pre></td></tr></table></figure>

<p>又拉取失败：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">projector-user@storage:~/go/src/github.com$ git clone https://github.com/kubernetes/kubernetes.git</span><br><span class="line">Cloning into &#x27;kubernetes&#x27;...</span><br><span class="line">fatal: unable to update url base from redirection:</span><br><span class="line">  asked for: https://github.com/kubernetes/kubernetes.git/info/refs?service=git-upload-pack</span><br><span class="line">   redirect: http://x.x.x.x/proxy.html?template=default&amp;tabs=pwd&amp;vlanid=0&amp;url=https://github.com%2Fkubernetes%2Fkubernetes.git%2Finfo%2Frefs%3Fservice%3Dgit-upload-pack</span><br></pre></td></tr></table></figure>

<p>因为未配置代理，通过以下命令解决：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">设置：git config --global http.proxy http://user:password@http://x.x.x.xx:8080</span><br><span class="line">查看：git config --get --global http.proxy</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注：密码中如果存在特殊字符，请先转义。</p>
</blockquote>
<p>再次尝试拉取，拉取成功：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">projector-user@storage:~/go/src/github.com$ git clone https://github.com/kubernetes/kubernetes.git</span><br><span class="line">Cloning into &#x27;kubernetes&#x27;...</span><br><span class="line">remote: Enumerating objects: 1258541, done.</span><br><span class="line">remote: Counting objects: 100% (316/316), done.</span><br><span class="line">remote: Compressing objects: 100% (201/201), done.</span><br><span class="line">remote: Total 1258541 (delta 131), reused 150 (delta 111), pack-reused 1258225</span><br><span class="line">Receiving objects: 100% (1258541/1258541), 773.55 MiB | 805.00 KiB/s, done.</span><br><span class="line">Resolving deltas: 100% (906256/906256), done.</span><br><span class="line">Checking out files: 100% (23196/23196), done.</span><br></pre></td></tr></table></figure>

<h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><h3 id="1-复制粘贴问题"><a href="#1-复制粘贴问题" class="headerlink" title="1. 复制粘贴问题"></a>1. 复制粘贴问题</h3><p>根据参考资料[4]，可以通过设置环境变量<code>ORG_JETBRAINS_PROJECTOR_SERVER_SSL_PROPERTIES_PATH</code>解决：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -e ORG_JETBRAINS_PROJECTOR_SERVER_SSL_PROPERTIES_PATH=/root/ssl/ssl.properties ...</span><br></pre></td></tr></table></figure>

<p><code>ssl</code>的配置文件举例：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">STORE_TYPE=JKS</span><br><span class="line">FILE_PATH=/root/ssl/keystore</span><br><span class="line">STORE_PASSWORD=xxx</span><br><span class="line">KEY_PASSWORD=xxx</span><br></pre></td></tr></table></figure>

<p>通过查看启动日志确认<code>ssl</code>是否配置成功，如下日志所示，<code>WebSocket SSL is enabled: /root/ssl/ssl.properties</code>表示配置成功，此时在浏览器用<code>https://xxx:8887/?wss</code>访问即可。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Found IDE: goland</span><br><span class="line">OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.</span><br><span class="line">[DEBUG] :: IdeState :: Starting attempts to Init ProjectorClassLoader</span><br><span class="line">[DEBUG] :: IdeState :: Starting attempts to attach IJ injector agent</span><br><span class="line">[DEBUG] :: IdeState :: Starting attempts to initialize IDEA: fix AA and disable smooth scrolling (at start)</span><br><span class="line">[DEBUG] :: IdeState :: Starting attempts to Getting IDE colors</span><br><span class="line">[DEBUG] :: ProjectorServer :: Daemon thread starts</span><br><span class="line">[DEBUG] :: IdeState :: Starting attempts to search for editors</span><br><span class="line">[INFO] :: ProjectorServer :: ProjectorServer is starting on host 0.0.0.0/0.0.0.0 and port 8887</span><br><span class="line">[INFO] :: HttpWsServerBuilder :: WebSocket SSL is enabled: /root/ssl/ssl.properties</span><br><span class="line">[INFO] :: HttpWsServer :: Server started on host 0.0.0.0/0.0.0.0 and port 8887</span><br><span class="line">[DEBUG] :: IdeState :: &quot;Init ProjectorClassLoader&quot; is done</span><br><span class="line">[DEBUG] :: IdeState :: &quot;search for editors&quot; is done</span><br></pre></td></tr></table></figure>

<p>登录后再次尝试，又可以快乐的<code>Ctrl+C</code>、<code>Ctrl+V</code>了。</p>
<h3 id="2-自定义Keymap被重置问题"><a href="#2-自定义Keymap被重置问题" class="headerlink" title="2. 自定义Keymap被重置问题"></a>2. 自定义Keymap被重置问题</h3><p>根据参考资料[4]，可以通过设置环境变量<code>ORG_JETBRAINS_PROJECTOR_SERVER_AUTO_KEYMAP=false</code>解决：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -e ORG_JETBRAINS_PROJECTOR_SERVER_AUTO_KEYMAP=false ...</span><br></pre></td></tr></table></figure>

<p>登录后再观察，发现自定义的<code>keymap</code>不会神奇的恢复了。</p>
<h3 id="3-原生客户端全局搜索结果模糊问题"><a href="#3-原生客户端全局搜索结果模糊问题" class="headerlink" title="3. 原生客户端全局搜索结果模糊问题"></a>3. 原生客户端全局搜索结果模糊问题</h3><p>模糊部分刚好是搜索的字符串，使用起来问题也不大，如果忍不了，也可以暂时使用浏览器开心玩耍（浏览器下没有该问题）。</p>
<blockquote>
<p>更新：<code>v1.0.2</code>版本已修复该问题</p>
</blockquote>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://fuckcloudnative.io/posts/run-jetbrains-ide-in-docker/">https://fuckcloudnative.io/posts/run-jetbrains-ide-in-docker/</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/JetBrains/projector-docker">https://github.com/JetBrains/projector-docker</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/JetBrains/projector-client/releases">https://github.com/JetBrains/projector-client/releases</a></li>
<li><a target="_blank" rel="noopener" href="https://jetbrains.github.io/projector-client/mkdocs/latest/ij_user_guide/server_customization/">https://jetbrains.github.io/projector-client/mkdocs/latest/ij_user_guide/server_customization/</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2022/10/23/java/Karaf%E6%A1%86%E6%9E%B6%E5%8D%87%E7%BA%A7Lg4j%E5%8E%86%E7%A8%8B%20(%E7%BB%AD)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/23/java/Karaf%E6%A1%86%E6%9E%B6%E5%8D%87%E7%BA%A7Lg4j%E5%8E%86%E7%A8%8B%20(%E7%BB%AD)/" class="post-title-link" itemprop="url">Karaf框架升级Lg4j历程（续）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-10-23 14:15:21" itemprop="dateCreated datePublished" datetime="2022-10-23T14:15:21+00:00">2022-10-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:10:03" itemprop="dateModified" datetime="2024-04-14T02:10:03+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/java/" itemprop="url" rel="index"><span itemprop="name">java</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>继《Karaf框架升级Lg4j历程》之后，今天又接到通知，需要将版本再升级到<code>2.18.0</code>，据说还是因为漏洞问题。网上查找，未发现有爆出什么漏洞，只找到了一个腾讯发布的相关通知《<a target="_blank" rel="noopener" href="https://security.tencent.com/ti/update_detail/ZN5pEF3RyiGHJCPXAwgtIkOzUjTB6Mr8">Apache Log4j官网普通更新</a>》。</p>
<h2 id="分析过程"><a href="#分析过程" class="headerlink" title="分析过程"></a>分析过程</h2><p>有了前文的分析，我们知道，要解决漏洞，有三种升级方式：</p>
<ol>
<li><strong>升级框架</strong>：这个影响就比较大了，而且框架的版本发布周期比较慢，目前还没有编译好的框架包，要升级框架就需要自己编译出所有的框架包，风险较大；</li>
<li><strong>升级依赖包</strong>：影响较小，如果没有配置依赖包的地方，可能无法升级；（实际确认，无法单独升级）</li>
<li><strong>修改当前版本依赖包并重新编译</strong>：影响较小，如果与最新版本跨度较大，可能修改点会很多；</li>
</ol>
<p>综合比较，继续考虑使用第3个方案走走看，有了前文的经验，就直接修改依赖包版本到<code>2.18.0</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pom.xml</span><br><span class="line">        &lt;version.org.apache.felix.configadmin&gt;1.9.20&lt;/version.org.apache.felix.configadmin&gt;</span><br><span class="line">        &lt;version.org.apache.felix.framework&gt;5.6.12&lt;/version.org.apache.felix.framework&gt;</span><br><span class="line">        &lt;version.org.apache.felix6.framework&gt;6.0.3&lt;/version.org.apache.felix6.framework&gt;</span><br><span class="line">-       &lt;version.org.apache.logging.log4j&gt;2.17.0&lt;/version.org.apache.logging.log4j&gt;</span><br><span class="line">+       &lt;version.org.apache.logging.log4j&gt;2.18.0&lt;/version.org.apache.logging.log4j&gt;</span><br><span class="line">        &lt;version.org.apache.servicemix.bundles.javax-inject&gt;1_3&lt;/version.org.apache.servicemix.bundles.javax-inject&gt;</span><br><span class="line">        &lt;version.org.jboss.logging&gt;3.4.1.Final&lt;/version.org.jboss.logging&gt;</span><br><span class="line">        &lt;version.org.mockito&gt;3.7.7&lt;/version.org.mockito&gt;</span><br></pre></td></tr></table></figure>

<p>编译：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">root@895be557c3cd:/home/org.ops4j.pax.logging-logging-1.11.9# mvn clean install -Dmaven.test.skip=true</span><br><span class="line">...</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Reactor Summary:</span><br><span class="line">[INFO]</span><br><span class="line">[INFO] OPS4J Pax Logging (Build POM) 1.11.9 ............... SUCCESS [  2.355 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - API (Java9) .................... SUCCESS [  2.039 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - API ............................ SUCCESS [  4.926 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Log4Jv1 implementation ......... SUCCESS [  2.235 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Log4Jv2 implementation ......... SUCCESS [  3.051 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Log4j v2 Extra packages ........ SUCCESS [  0.146 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Logback implementation ......... SUCCESS [  0.950 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Integration Tests .............. SUCCESS [  0.354 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Karaf .......................... SUCCESS [  0.014 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Karaf KAR Logger ............... SUCCESS [  0.142 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Karaf KAR ...................... SUCCESS [  1.710 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Karaf Distribution ............. SUCCESS [  2.522 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Karaf Integration Tests 1.11.9 . SUCCESS [  0.703 s]</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 22.711 s</span><br><span class="line">[INFO] Finished at: 2022-10-20T03:50:21Z</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>

<p>把新编译的<code>pax-logging-api</code>和<code>pax-logging-log4j</code>替换到依赖仓库中，重新编译交付件，发现日志功能异常，服务不再打印任何日志了，定位都无从下手；</p>
<p>从参考资料[1]的代码提交记录看，<code>org.ops4j.pax.logging</code>为了升级<code>log4j</code>依赖包，不单单是改了版本，还涉及一些代码修改点，怀疑是有关系的：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">pax-logging-api/src/main/java/org/apache/logging/log4j/LogManager.java</span><br><span class="line">pax-logging-api/src/main/java/org/apache/logging/log4j/status/StatusLogger.java</span><br><span class="line">pax-logging-api/src/main/java/org/apache/logging/log4j/util/PaxPropertySource.java</span><br><span class="line">pax-logging-it/pom.xml</span><br><span class="line">pax-logging-log4j2/src/main/java/org/apache/logging/log4j/core/config/plugins/util/ResolverUtil.java</span><br><span class="line">pax-logging-log4j2/src/main/java/org/apache/logging/log4j/core/impl/ThrowableProxy.java</span><br><span class="line">pax-logging-log4j2/src/main/java/org/apache/logging/log4j/core/pattern/DatePatternConverter.java</span><br><span class="line">pax-logging-log4j2/src/main/java/org/ops4j/pax/logging/log4j2/internal/PaxLoggingServiceImpl.java</span><br><span class="line">pax-logging-samples/fragment-log4j2/src/main/java/org/ops4j/pax/logging/log4j2/extra/ListAppender.java</span><br><span class="line">pom.xml</span><br></pre></td></tr></table></figure>

<p>对比发现，修改点不多，也不复杂，就尝试将更新的代码移植到<code>1.11.9</code>版本上；</p>
<p>然后使用前文使用过的容器编译环境编译<code>jar</code>包：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">root@895be557c3cd:/home/org.ops4j.pax.logging-logging-1.11.9# mvn clean install -Dmaven.test.skip=true</span><br><span class="line">...</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Reactor Summary:</span><br><span class="line">[INFO]</span><br><span class="line">[INFO] OPS4J Pax Logging (Build POM) 1.11.9 ............... SUCCESS [  2.355 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - API (Java9) .................... SUCCESS [  2.039 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - API ............................ SUCCESS [  4.926 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Log4Jv1 implementation ......... SUCCESS [  2.235 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Log4Jv2 implementation ......... SUCCESS [  3.051 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Log4j v2 Extra packages ........ SUCCESS [  0.146 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Logback implementation ......... SUCCESS [  0.950 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Integration Tests .............. SUCCESS [  0.354 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Karaf .......................... SUCCESS [  0.014 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Karaf KAR Logger ............... SUCCESS [  0.142 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Karaf KAR ...................... SUCCESS [  1.710 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Karaf Distribution ............. SUCCESS [  2.522 s]</span><br><span class="line">[INFO] OPS4J Pax Logging - Karaf Integration Tests 1.11.9 . SUCCESS [  0.703 s]</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 23.641 s</span><br><span class="line">[INFO] Finished at: 2022-10-20T03:55:39Z</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>

<p>升级<code>log4j</code>的版本编译成功。</p>
<p>把新编译的<code>pax-logging-api</code>和<code>pax-logging-log4j</code>替换到依赖仓库中，重新编译交付件，发现日志功能正常；</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>把新编译的<code>pax-logging-api</code>和<code>pax-logging-log4j</code>替换到依赖仓库中，重新编译交付件，日志功能正常。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/ops4j/org.ops4j.pax.logging/commit/7c007343fe9844a17e9c6eaae3a833e6c19a579a">https://github.com/ops4j/org.ops4j.pax.logging/commit/7c007343fe9844a17e9c6eaae3a833e6c19a579a</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2022/08/20/java/karaf%E6%A1%86%E6%9E%B6%E8%A7%A3%E5%86%B3CVE-2015-4000%E6%BC%8F%E6%B4%9E/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/20/java/karaf%E6%A1%86%E6%9E%B6%E8%A7%A3%E5%86%B3CVE-2015-4000%E6%BC%8F%E6%B4%9E/" class="post-title-link" itemprop="url">karaf框架解决CVE-2015-4000漏洞</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-20 09:15:21" itemprop="dateCreated datePublished" datetime="2022-08-20T09:15:21+00:00">2022-08-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:10:03" itemprop="dateModified" datetime="2024-04-14T02:10:03+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/java/" itemprop="url" rel="index"><span itemprop="name">java</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="漏洞描述"><a href="#漏洞描述" class="headerlink" title="漏洞描述"></a>漏洞描述</h2><p>漏洞原理参考资料[1]，简单来说就是，当服务器<code>SSL/TLS</code>的瞬时<code>Diffie-Hellman</code>公共密钥小于等于<code>1024</code>位时，存在可以恢复纯文本信息的风险。</p>
<p>复现方法很简单，使用<code>nmap -sV -Pn --script ssl-dh-params port ip</code> 命令扫描[2]，存在如下漏洞信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">nmap.exe -sV -Pn --script ssl-dh-params 443 192.168.1.10</span><br><span class="line">Starting Nmap 7.92 ( https://nmap.org ) at 2022-07-09 11:14</span><br><span class="line">Nmap scan report for 192.168.1.10</span><br><span class="line">Host is up (0.0033s latency).</span><br><span class="line">Not shown: 996 closed tcp ports (reset)</span><br><span class="line">…</span><br><span class="line">| ssl-dh-params:</span><br><span class="line">|   VULNERABLE:</span><br><span class="line">|   Diffie-Hellman Key Exchange Insufficient Group Strength</span><br><span class="line">|     State: VULNERABLE</span><br><span class="line">|       Transport Layer Security (TLS) services that use Diffie-Hellman groups</span><br><span class="line">|       of insufficient strength, especially those using one of a few commonly</span><br><span class="line">|       shared groups, may be susceptible to passive eavesdropping attacks.</span><br><span class="line">|     Check results:</span><br><span class="line">|       WEAK DH GROUP 1</span><br><span class="line">|             Cipher Suite: TLS_DHE_RSA_WITH_AES_256_CBC_SHA256</span><br><span class="line">|             Modulus Type: Safe prime</span><br><span class="line">|             Modulus Source: RFC2409/Oakley Group 2</span><br><span class="line">|             Modulus Length: 1024</span><br><span class="line">|             Generator Length: 8</span><br><span class="line">|             Public Key Length: 1024</span><br><span class="line">|     References:</span><br><span class="line">|_      https://weakdh.org</span><br></pre></td></tr></table></figure>

<h2 id="修复方案"><a href="#修复方案" class="headerlink" title="修复方案"></a>修复方案</h2><p>参考[3,4]，修改方案如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 etc]# cat org.ops4j.pax.web.cfg</span><br><span class="line">...</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Excluded SSL/TLS Cipher Suites comma-separated list of Regular Expressions</span></span><br><span class="line">org.ops4j.pax.web.ssl.ciphersuites.excluded=.*NULL.*,.*RC4.*,.*MD5.*,.*DES.*,.*DSS.*,TLS_DHE.*,SSL.*,.*anon.*,.*EXPORT.*</span><br></pre></td></tr></table></figure>

<p>修改后，再次使用<code>nmap -sV -Pn --script ssl-dh-params port ip</code>查看扫描结果，漏洞解决：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">nmap.exe -sV -Pn --script ssl-dh-params 443 192.168.1.10（主机IP）</span><br><span class="line">Starting Nmap 7.92 ( https://nmap.org ) at 2022-07-07 11:53</span><br><span class="line">Nmap scan report for 192.168.1.10</span><br><span class="line">Host is up (0.0032s latency).</span><br><span class="line">Not shown: 997 closed tcp ports (reset</span><br><span class="line">PORT     STATE SERVICE  VERSION</span><br><span class="line">22/tcp   open  ssh      OpenSSH 7.4 (protocol 2.0)</span><br><span class="line">111/tcp  open  rpcbind  2-4 (RPC #100000)</span><br><span class="line">...</span><br><span class="line">Service detection performed. Please report any incorrect results at https://nmap.org/submit/ .</span><br><span class="line">Nmap done: 1 IP address (1 host up) scanned in 18.74 seconds </span><br></pre></td></tr></table></figure>

<p>需要注意的是，<strong>添加完上面的参数后，可能会出现一个新的问题</strong>，扫描结果如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">nmap.exe -sV --script ssl-enum-ciphers -p 443 192.168.1.10</span><br><span class="line">Starting Nmap 6.40 ( http://nmap.org ) at 2022-08-20 22:26 CST</span><br><span class="line">Nmap scan report <span class="keyword">for</span> matrix-node1 (192.168.1.10)</span><br><span class="line">Host is up (0.000064s latency).</span><br><span class="line">PORT     STATE SERVICE    VERSION</span><br><span class="line">443/tcp open  https-alt</span><br><span class="line">| ssl-enum-ciphers:</span><br><span class="line">|   TLSv1.0:</span><br><span class="line">|     ciphers:</span><br><span class="line">|       TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA - strong</span><br><span class="line">|       ...</span><br><span class="line">|     compressors:</span><br><span class="line">|       NULL</span><br><span class="line">|   TLSv1.1:</span><br><span class="line">|     ciphers:</span><br><span class="line">|       TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA - strong</span><br><span class="line">|       ...</span><br><span class="line">|     compressors:</span><br><span class="line">|       NULL</span><br><span class="line">|   TLSv1.2:</span><br><span class="line">|     ciphers:</span><br><span class="line">|       TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA - strong</span><br><span class="line">|       ...</span><br><span class="line">|     compressors:</span><br><span class="line">|       NULL</span><br><span class="line">|_  least strength: strong</span><br></pre></td></tr></table></figure>

<p>修改配置之前，扫描结果里显示仅开启了<code>TLSv1.2</code>，而修改配置之后，发现<code>TLSv1.0</code>和<code>TLSv1.1</code>都被开启了，这俩协议也是需要关闭：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 etc]<span class="comment"># cat org.ops4j.pax.web.cfg</span></span><br><span class="line">...</span><br><span class="line"><span class="comment"># Excluded SSL/TLS Cipher Suites comma-separated list of Regular Expressions</span></span><br><span class="line">org.ops4j.pax.web.ssl.ciphersuites.excluded=.*NULL.*,.*RC4.*,.*MD5.*,.*DES.*,.*DSS.*,TLS_DHE.*,SSL.*,.*anon.*,.*EXPORT.*</span><br><span class="line">org.ops4j.pax.web.ssl.protocols.excluded=TLSv1,TLSv1.1</span><br></pre></td></tr></table></figure>

<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://access.redhat.com/zh_CN/articles/1480493">https://access.redhat.com/zh_CN/articles/1480493</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/zcg-cpdd/p/15573841.html">https://www.cnblogs.com/zcg-cpdd/p/15573841.html</a></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/30523324/how-to-config-local-jetty-ssl-to-avoid-weak-phermeral-dh-key-error">https://stackoverflow.com/questions/30523324/how-to-config-local-jetty-ssl-to-avoid-weak-phermeral-dh-key-error</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/codice/ddf/blob/master/distribution/ddf-common/src/main/resources/etc/org.ops4j.pax.web.cfg">https://github.com/codice/ddf/blob/master/distribution/ddf-common/src/main/resources/etc/org.ops4j.pax.web.cfg</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2022/07/16/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E5%A4%96%E7%BD%AE%E5%AD%98%E5%82%A8%E6%8F%92%E4%BB%B6nfs-provisioner%E6%8A%A5%E9%94%99selfLink%20was%20empty/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/16/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E5%A4%96%E7%BD%AE%E5%AD%98%E5%82%A8%E6%8F%92%E4%BB%B6nfs-provisioner%E6%8A%A5%E9%94%99selfLink%20was%20empty/" class="post-title-link" itemprop="url">K8S问题排查-外置存储插件nfs-provisioner报错selfLink was empty</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-07-16 16:13:43" itemprop="dateCreated datePublished" datetime="2022-07-16T16:13:43+00:00">2022-07-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:10:03" itemprop="dateModified" datetime="2024-04-14T02:10:03+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>本次发现的问题是踩到了新版本<code>Kubernetes</code>的坑，查找资料发现<code>zhangsi-lzq</code>大佬已经分析的很清楚了，此处转载过来仅做学习记录，并新增永久解决方案。</p>
<blockquote>
<p>作者：zhangsi-lzq<br>本文出处：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/zhangsi-lzq/p/14292628.html">https://www.cnblogs.com/zhangsi-lzq/p/14292628.html</a></p>
</blockquote>
<h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p><code>Kubernetes</code>升级为<code>1.20</code>版本后，原有的后端<code>nfs</code>存储<code>storageclass</code>无法自动创建<code>PV</code>。查看<code>PVC</code>状态一直为<code>pending</code>状态。</p>
<h2 id="分析过程"><a href="#分析过程" class="headerlink" title="分析过程"></a>分析过程</h2><p>查看<code>nfs-provisioner</code>日志：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs nfs-client-provisioner-5f696dc8bb-qhmsn</span><br><span class="line">E0118 03:01:07.352670       1 streamwatcher.go:109] Unable to decode an event from the watch stream: http2: server sent GOAWAY and closed the connection; LastStreamID=3737, ErrCode=NO_ERROR, debug=&quot;&quot;</span><br><span class="line">E0118 03:01:07.353951       1 reflector.go:322] github.com/kubernetes-incubator/external-storage/lib/controller/controller.go:668: Failed to watch *v1.StorageClass: Get https://10.96.0.1:443/apis/storage.k8s.io/v1/storageclasses?resourceVersion=604432&amp;timeoutSeconds=387&amp;watch=true: dial tcp 10.96.0.1:443: connect: connection refused</span><br><span class="line">W0118 03:01:07.366606       1 reflector.go:341] github.com/kubernetes-incubator/external-storage/lib/controller/controller.go:665: watch of *v1.PersistentVolume ended with: too old resource version: 11565 (605173)</span><br><span class="line">W0118 03:01:07.367679       1 reflector.go:341] github.com/kubernetes-incubator/external-storage/lib/controller/controller.go:662: watch of *v1.PersistentVolumeClaim ended with: too old resource version: 11565 (605173)</span><br><span class="line">I0118 03:08:28.340240       1 controller.go:987] provision &quot;default/auth-platorm-redis-data-auth-platorm-redis-cluster-0&quot; class &quot;course-nfs-storage&quot;: started</span><br><span class="line">E0118 03:08:28.343582       1 controller.go:1004] provision &quot;default/auth-platorm-redis-data-auth-platorm-redis-cluster-0&quot; class &quot;course-nfs-storage&quot;: unexpected error getting claim reference: selfLink was empty, can&#x27;t make reference</span><br><span class="line">I0118 03:16:08.373590       1 controller.go:987] provision &quot;default/auth-platorm-redis-data-auth-platorm-redis-cluster-0&quot; class &quot;course-nfs-storage&quot;: started</span><br><span class="line">E0118 03:16:08.382178       1 controller.go:1004] provision &quot;default/auth-platorm-redis-data-auth-platorm-redis-cluster-0&quot; class &quot;course-nfs-storage&quot;: unexpected error getting claim reference: selfLink was empty, can&#x27;t make reference</span><br><span class="line">I0118 03:30:41.647626       1 controller.go:987] provision &quot;default/test-pvc&quot; class &quot;course-nfs-storage&quot;: started</span><br><span class="line">E0118 03:30:41.658419       1 controller.go:1004] provision &quot;default/test-pvc&quot; class &quot;course-nfs-storage&quot;: unexpected error getting claim reference: selfLink was empty, can&#x27;t make reference</span><br><span class="line">I0118 03:31:08.373713       1 controller.go:987] provision &quot;default/auth-platorm-redis-data-auth-platorm-redis-cluster-0&quot; class &quot;course-nfs-storage&quot;: started</span><br><span class="line">I0118 03:31:08.373969       1 controller.go:987] provision &quot;default/test-pvc&quot; class &quot;course-nfs-storage&quot;: started</span><br><span class="line">E0118 03:31:08.382279       1 controller.go:1004] provision &quot;default/test-pvc&quot; class &quot;course-nfs-storage&quot;: unexpected error getting claim reference: selfLink was empty, can&#x27;t make reference</span><br><span class="line">E0118 03:31:08.382791       1 controller.go:1004] provision &quot;default/auth-platorm-redis-data-auth-platorm-redis-cluster-0&quot; class &quot;course-nfs-storage&quot;: unexpected error getting claim reference: selfLink was empty, can&#x27;t make reference</span><br></pre></td></tr></table></figure>

<p>报错信息<code>selfLink was empty</code>，于是上网查询相关内容，在官方<code>1.20</code>的<a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md">变更说明</a>中[1]看到其中一条说明为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Stop propagating SelfLink (deprecated in 1.16) in kube-apiserver</span><br></pre></td></tr></table></figure>

<p><code>selfLink</code>在<code>1.16</code>版本以后已经弃用，在<code>1.20</code>版本停用。而由于<code>nfs-provisione</code>r的实现是基于<code>selfLink</code>功能（同时也会影响其他用到<code>selfLink</code>这个功能的第三方软件），需要等<code>nfs-provisioner</code>的制作方重新提供新的解决方案。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>目前可用的临时方案是：修改<code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>文件，找到如下内容后，在最后添加一项参数：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - command:</span><br><span class="line">    - kube-apiserver    </span><br><span class="line">    - --advertise-address=192.168.210.20    </span><br><span class="line">    - --....... </span><br><span class="line">    - --feature-gates=RemoveSelfLink=false　　#添加此行</span><br></pre></td></tr></table></figure>

<p>如果是高可用的<code>k8s</code>集群，则需要在所<code>有master</code>节点上进行此操作。</p>
<p>添加后需要删除<code>apiserver</code>的所有pod进行重启</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete pod kube-apiserver-master01 -n kube-system</span><br><span class="line">kubectl delete pod kube-apiserver-master02 -n kube-system</span><br><span class="line">kubectl delete pod kube-apiserver-master03 -n kube-system</span><br></pre></td></tr></table></figure>

<p>三台<code>apiserver</code>被<code>kubelet</code>重启拉起后，再次查询<code>PVC</code>，可以看到<code>PVC</code>状态都为<code>Bound</code>，可以正常被<code>PV</code>绑定了。</p>
<p>（新增）永久解决方案：查资料发现，<code>nfs-provisioner</code>项目实际上已经放弃[2,3]，转而提供了下面3个<code>provisioner</code>，可以根据需要适配更新：</p>
<blockquote>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner">nfs-subdir-external-provisioner</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/kubernetes-sigs/nfs-ganesha-server-and-external-provisioner">nfs-ganesha-server-and-external-provisioner</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/kubernetes-csi/external-provisioner">external-provisioner</a></li>
</ul>
</blockquote>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md">https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/kubernetes-retired/nfs-provisioner">https://github.com/kubernetes-retired/nfs-provisioner</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner">https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2022/06/18/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E5%AE%89%E5%85%A8%E7%AD%96%E7%95%A5%E5%AF%BC%E8%87%B4Pod%E5%8F%8D%E5%A4%8D%E9%87%8D%E5%90%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/06/18/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E5%AE%89%E5%85%A8%E7%AD%96%E7%95%A5%E5%AF%BC%E8%87%B4Pod%E5%8F%8D%E5%A4%8D%E9%87%8D%E5%90%AF/" class="post-title-link" itemprop="url">K8S问题排查-安全策略导致Pod反复重启</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-06-18 18:47:44" itemprop="dateCreated datePublished" datetime="2022-06-18T18:47:44+00:00">2022-06-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:10:03" itemprop="dateModified" datetime="2024-04-14T02:10:03+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><ol>
<li>查看<code>kube-system</code>下的系统组件，发现<code>harbor、influxdb、coredns</code>等组件反复重启；</li>
<li>使用<code>kubectl get pod -n kube-system</code>命令查看<code>pod</code>列表，发现命令会稳定的卡<code>15s</code>；</li>
</ol>
<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>先看第一点，这几个<code>pod</code>反复重启已经遇到过几次，看过前面的问题排查文章[1,2]的应该知道，在业务高并发场景下可能出现。先使用<code>top</code>命令看一下负载情况：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">load average: 14.76  18.45  17.85</span><br><span class="line">Tasks: 1998 total,  7 running,  1937 sleeping, 0 stopped,  54 zombie</span><br><span class="line"><span class="meta prompt_">%</span><span class="language-bash">CPU: 15.2 us, 8.3 sys, 0.7 ni, 75.3 <span class="built_in">id</span></span></span><br><span class="line"></span><br><span class="line">cat /proc/cpuinfo|grep MHz| wc -l</span><br><span class="line">40</span><br></pre></td></tr></table></figure>

<p>实际负载不高，排除这个可能（实际定位过程中，也依据前面的经验，做了相关内核参数的优化，问题确实没什么改善）。那就继续看，先<code>describe</code>看几个异常<code>pod</code>的错误信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubelet     Liveness probe failed: Get &quot;http://177.177.138.139:8885/api/health&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">kubelet     Readiness probe failed: Get &quot;http://177.177.138.139:8083/&quot;&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">kubelet     Back-off restarting failed container</span><br></pre></td></tr></table></figure>

<p>错误基本一致，就是<code>kubelet</code>调用<code>pod</code>提供的健康检查接口超时了，所以被不断地<code>kill</code>再重启。为什么调不通？模拟<code>kubelet</code>的调用操作，在<code>pod</code>所在节点上使用<code>curl</code>命令调用，结果显示<code>timeout</code>，再<code>ping</code>一下看看<code>pod</code>通信有没有问题：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ping 177.177.212.186</span><br><span class="line">^C</span><br></pre></td></tr></table></figure>

<p><code>ping</code>不通！这也就讲通了为啥健康检查不过，因为<code>kubelet</code>与所在节点上的<code>Pod</code>根本就无法通信。为啥会这样？通过一顿验证，发现一个规律：**集群各节点无法访问自己节点上的<code>pod</code>，但可以访问其他节点上的<code>pod</code>**；</p>
<p>这个现象是比较奇怪的，一般来说影响节点与自己节点上<code>pod</code>通信的原因不多，对于使用<code>calic</code>的<code>cni</code>网络插件来说，可能的原因有：</p>
<ol>
<li><code>pod</code>内的<code>ip</code>&#x2F;<code>arp</code>&#x2F;路由异常；</li>
<li><code>calico</code>网卡的<code>arp</code>配置异常；</li>
<li>请求被<code>iptables</code>拦截；</li>
</ol>
<p>分别查看1,2相关配置，显示正常：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">[root@influxdb-847b74cbc5-ddxcd]# </span><span class="language-bash">ip addr</span></span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: eth0@if79: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 32:81:0e:f4:dd:3a brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 177.177.212.186/32 scope global eth0</span><br><span class="line">       valid_lft f</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">[root@influxdb-847b74cbc5-ddxcd]# </span><span class="language-bash">arp</span></span><br><span class="line">Address                  HWtype  HWaddress           Flags Mask            Iface</span><br><span class="line">gateway                  ether   ee:ee:ee:ee:ee:ee   C                     eth0</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">[root@influxdb-847b74cbc5-ddxcd]# </span><span class="language-bash">ip rou</span></span><br><span class="line">default via 169.254.1.1 dev eth0</span><br><span class="line">169.254.1.1 dev eth0 scope link</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">[root@node]# </span><span class="language-bash"><span class="built_in">cat</span> /proc/sys/net/ipv4/conf/cali7acfda72e71/proxy_arp</span></span><br><span class="line">1</span><br><span class="line"><span class="meta prompt_">[root@node]# </span><span class="language-bash"><span class="built_in">cat</span> /proc/sys/net/ipv4/conf/cali7acfda72e71/arp_ignore</span></span><br><span class="line">0</span><br></pre></td></tr></table></figure>

<p>对于第三点，通过<code>iptables</code>命令检查过，也没什么异常规则；</p>
<p>抓包看看啥情况：</p>
<ol>
<li>在节点上<code>ping</code>本节点上的pod，同时在<code>cali7acfda72e71</code>上抓包，发现请求到了<code>cali</code>网卡，但没有响应；</li>
<li>在<code>pod</code>内<code>ping</code>本节点<code>ip</code>，同时在<code>cali7acfda72e71</code>上抓包，发现<code>cali</code>网卡上有请求和响应，但依然无法<code>ping</code>通；</li>
</ol>
<p>看起来像是请求被主动丢弃了，跟问题提出人确认问题环境的基本情况，发现该集群有额外安装<code>EDR</code>防病毒软件。为了确认该软件有没有影响，先停掉防病毒软件，观察一段时候后，发现环境恢复正常。重新启动防病毒软件，一段时间后问题复现；</p>
<p>与负责防病毒软件的技术沟通确认，该集群被设置到了防病毒软件的默认策略里，触发条件是默认策略组里面有防止端口扫描和流量阈值配置，结果触发了网络防护，导致节点<code>ip</code>被封了。经过软件提供方的调整，将该集群调整到单独的策略编组，问题解决；</p>
<p>问题解决后，现象1和现象2都消失了，但回过头想想，为什么会出现现象2？当时环境好了没有细究，后来自己的测试环境也出现过这个现象，通过<code>debug</code>日志发现，卡在了调用<code>metric</code>服务的接口上，根本原因就是访问<code>metric</code>的<code>pod</code>不通，日志现象比较明显，这里就不贴了：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod -n kube-system -owide --v 10</span><br></pre></td></tr></table></figure>

<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ol>
<li>针对该集群环境，单独配置策略编组；</li>
<li>节点与<code>Pod</code>通信正常后，现象2消失；</li>
</ol>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://lyyao09.github.io/2021/06/19/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E4%B8%9A%E5%8A%A1%E9%AB%98%E5%B9%B6%E5%8F%91%E5%AF%BC%E8%87%B4Pod%E5%8F%8D%E5%A4%8D%E9%87%8D%E5%90%AF/">https://lyyao09.github.io/2021/06/19/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E4%B8%9A%E5%8A%A1%E9%AB%98%E5%B9%B6%E5%8F%91%E5%AF%BC%E8%87%B4Pod%E5%8F%8D%E5%A4%8D%E9%87%8D%E5%90%AF/</a></li>
<li><a href="https://lyyao09.github.io/2021/07/16/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E4%B8%9A%E5%8A%A1%E9%AB%98%E5%B9%B6%E5%8F%91%E5%AF%BC%E8%87%B4Pod%E5%8F%8D%E5%A4%8D%E9%87%8D%E5%90%AF(%E7%BB%AD)/">https://lyyao09.github.io/2021/07/16/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E4%B8%9A%E5%8A%A1%E9%AB%98%E5%B9%B6%E5%8F%91%E5%AF%BC%E8%87%B4Pod%E5%8F%8D%E5%A4%8D%E9%87%8D%E5%90%AF(%E7%BB%AD)/</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/409634">https://developer.aliyun.com/article/409634</a></li>
<li><a target="_blank" rel="noopener" href="https://www.css3.io/31linuxxi-tong-diao-you.htmls">https://www.css3.io/31linuxxi-tong-diao-you.htmls</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2022/06/05/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-VMWare%E8%99%9A%E6%8B%9F%E5%8C%96%E7%8E%AF%E5%A2%83%E4%B8%8BPod%E8%B7%A8VXLAN%E9%80%9A%E4%BF%A1%E5%BC%82%E5%B8%B8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/06/05/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-VMWare%E8%99%9A%E6%8B%9F%E5%8C%96%E7%8E%AF%E5%A2%83%E4%B8%8BPod%E8%B7%A8VXLAN%E9%80%9A%E4%BF%A1%E5%BC%82%E5%B8%B8/" class="post-title-link" itemprop="url">K8S问题排查-VMWare虚拟化环境下Pod跨VXLAN通信异常</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-06-05 17:38:44" itemprop="dateCreated datePublished" datetime="2022-06-05T17:38:44+00:00">2022-06-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:10:03" itemprop="dateModified" datetime="2024-04-14T02:10:03+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>为了解决<a href="https://lyyao09.github.io/2022/03/19/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Calico%E7%9A%84Vxlan%E6%A8%A1%E5%BC%8F%E4%B8%8B%E8%8A%82%E7%82%B9%E5%8F%91%E8%B5%B7K8s%20Service%E8%AF%B7%E6%B1%82%E5%BB%B6%E8%BF%9F/">节点上请求部分service延迟63s问题[1]</a>，我们临时把OS的版本换成了<code>Redhat 8.4</code>（内核版本<code>4.18.0-305</code>），在<code>VMware</code>上虚出3节点集群后部署跨三层环境失败，提示<code>Harbor</code>部署失败。</p>
<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>说明：距离定位这个问题已经有一段时间了，其实最终也没完全定位出根本原因，所以当时也没有整理记录定位过程，这里就简单描述一下，做个记录。</p>
<p>通过查看<code>Harbor</code>的日志，发现部署失败的原因是健康检查失败，因为<code>Harbor</code>的<code>podIp:port</code>请求在跨三层下超时，抓包发现请求经过<code>vxlan.calico</code>时止于<code>SYN_SENT</code>报文；</p>
<p>实测发现，该环境上不仅是<code>Harbor</code>的<code>podIp:port</code>请求超时，其他<code>pod</code>服务的请求经过跨三层的网络也同样存在问题，说明是一个共性问题，并且<code>pod</code>网段的七层如<code>http</code>请求受影响，4层的<code>icmp</code>请求不受影响）；</p>
<p>继续通过抓包确认，<code>podIp:port</code>的请求已经发送到节点网卡上，但跨三层对端的节点没有收到。所以，可以排除主机上的<code>iptables</code>和路由的影响。实际上，也确实跟踪了<code>iptables</code>的请求过程，确认请求没有被丢弃；</p>
<p>综上，初步怀疑请求被跨三层网络的中间设备（如交换机、路由器）丢弃了，之后相关同事在交换机上抓包，发现<code>ping</code>包可以抓到，但<code>http</code>请求依然抓不到，说明交换机侧也没有收到报文，问题原因进一步缩小，可能情况有：</p>
<ol>
<li><strong>出口网卡丢弃；</strong></li>
<li><strong>出网卡后，入交换机之前丢弃</strong>；</li>
</ol>
<p>通过<code>ehtool -s xxx</code>命令统计虚机网卡报文信息，没有发现丢弃情况，说明问题不在虚机的出网卡；</p>
<p>关于<code>VMware</code>丢弃报文的情况，找到一些资料[2-6]，比如混杂模式，<code>mms</code>配置都会有影响：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1) 通过ping命令指定报文长度，发现跨网段依次ping一下pod的ip均返回正常，确认不是mms问题;</span><br><span class="line">ping -s 1000 177.177.x.x</span><br><span class="line">ping -s 1472 177.177.x.x </span><br><span class="line">ping -s 1500 177.177.x.x </span><br><span class="line"></span><br><span class="line">2) 通过临时修改网卡为混杂模式，测试问题依然存在;</span><br></pre></td></tr></table></figure>

<p>进一步在服务器物理网卡上抓包（登录<code>esxi</code>后台，使用<code>pktcap-uw --uplink vmnicX --dir 2 -o result.pcap</code>，其中<code>vmnicX</code>表示虚机关联的上行口物理网卡, <code>dir 2</code>表示同时抓取双向请求）, 依然是<code>ping</code>包可以抓到，但<code>http</code>请求依然抓不到，说明服务器物理网卡上也没有收到报文；</p>
<p>最后，丢包范围缩小到<code>VMare</code>下的<strong>虚机机请求出网卡后，到服务器物理网卡前</strong>  ，这中间涉及到虚拟化的实现，具体还有什么处理就不清楚了，最后改为使用物理服务器部署；</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>临时改用物理服务器部署跨三层集群成功，如果要使用<code>VMware</code>虚机部署，还需要继续排查根因。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://lyyao09.github.io/2022/03/19/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Calico%E7%9A%84Vxlan%E6%A8%A1%E5%BC%8F%E4%B8%8B%E8%8A%82%E7%82%B9%E5%8F%91%E8%B5%B7K8s%20Service%E8%AF%B7%E6%B1%82%E5%BB%B6%E8%BF%9F/">https://lyyao09.github.io/2022/03/19/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Calico%E7%9A%84Vxlan%E6%A8%A1%E5%BC%8F%E4%B8%8B%E8%8A%82%E7%82%B9%E5%8F%91%E8%B5%B7K8s%20Service%E8%AF%B7%E6%B1%82%E5%BB%B6%E8%BF%9F/</a></li>
<li><a target="_blank" rel="noopener" href="https://kb.vmware.com/s/article/2113783">https://kb.vmware.com/s/article/2113783</a></li>
<li><a target="_blank" rel="noopener" href="https://kb.vmware.com/s/article/52936">https://kb.vmware.com/s/article/52936</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.3/troubleshooting/GUID-4E4A9468-1F2B-44E1-A474-AA048A88BF1E.html">https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.3/troubleshooting/GUID-4E4A9468-1F2B-44E1-A474-AA048A88BF1E.html</a></li>
<li><a target="_blank" rel="noopener" href="https://communities.vmware.com/t5/vCloud-Networking-and-Security/No-SYN-ACK-reply-from-VM-on-virtual-wire-VXLAN/td-p/376545">https://communities.vmware.com/t5/vCloud-Networking-and-Security/No-SYN-ACK-reply-from-VM-on-virtual-wire-VXLAN/td-p/376545</a><ol>
<li><a target="_blank" rel="noopener" href="https://kb.vmware.com/s/article/2051814?lang=zh_CN">https://kb.vmware.com/s/article/2051814?lang=zh_CN</a></li>
</ol>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2022/03/19/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Calico%E7%9A%84Vxlan%E6%A8%A1%E5%BC%8F%E4%B8%8B%E8%8A%82%E7%82%B9%E5%8F%91%E8%B5%B7K8s%20Service%E8%AF%B7%E6%B1%82%E5%BB%B6%E8%BF%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/19/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Calico%E7%9A%84Vxlan%E6%A8%A1%E5%BC%8F%E4%B8%8B%E8%8A%82%E7%82%B9%E5%8F%91%E8%B5%B7K8s%20Service%E8%AF%B7%E6%B1%82%E5%BB%B6%E8%BF%9F/" class="post-title-link" itemprop="url">K8S问题排查-Calico的Vxlan模式下节点发起K8s Service请求延迟</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-19 16:57:44" itemprop="dateCreated datePublished" datetime="2022-03-19T16:57:44+00:00">2022-03-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:10:03" itemprop="dateModified" datetime="2024-04-14T02:10:03+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>9 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p><code>K8S</code>集群中修改<code>calico</code>的网络为<code>vxlan</code>模式后，发现部分<code>service</code>在节点上无法访问（实际上是延迟访问，延迟时间稳定在<code>1min3s</code>左右）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node2 ~]# time curl -s http://10.96.91.255</span><br><span class="line">real    1m3.136s</span><br><span class="line">user    0m0.005s</span><br><span class="line">sys     0m0.005s</span><br></pre></td></tr></table></figure>

<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>先确认问题范围，在环境上找多个<code>service</code>依次测试发现，如果调用<code>service</code>的节点和实际<code>pod</code>不在同一个节点上，则出现延迟，否则请求正常。也就是说跨节点的访问才有问题。而直接用<code>service</code>对应的<code>podIP</code>访问，也不存在问题，猜测问题可能出在<code>service</code>转<code>pod</code>的过程。</p>
<p>再确认基本环境，<code>OS</code>、<code>K8S</code>、<code>calico</code>等基础环境没有发生任何变化，仅仅是把<code>calico</code>的网络模式从<code>BGP</code>改为了<code>vxlan</code>，但是这个改动改变了集群内<code>service</code>及<code>pod</code>的请求路径，也即所有的容器请求需要走节点上新增的<code>calico.vxlan</code>接口封装一下。网络模式修改前没有问题，修改后必现，之后切回<code>BGP</code>模式问题就消失了，说明问题可能跟新增的<code>calico.vxlan</code>接口有关系。</p>
<p>先看下环境情况，并触发<code>service</code>请求：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">验证环境：node2（10.10.72.11）——&gt; node1（10.10.72.10）</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">验证方法：node2上curl service:10.96.91.255 ——&gt; node1上pod:166.166.166.168:8082</span></span><br><span class="line">[root@node2 ~]# ip addr</span><br><span class="line">10: vxlan.calico: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1410 qdisc noqueue state UNKNOWN group default</span><br><span class="line">    link/ether 66:2d:bf:44:a6:8b brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 166.166.104.10/32 brd 166.166.104.10 scope global vxlan.calico</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# ip addr</span><br><span class="line">15: vxlan.calico: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1410 qdisc noqueue state UNKNOWN group default</span><br><span class="line">    link/ether 66:f9:37:c3:7e:94 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 166.166.166.175/32 brd 166.166.166.175 scope global vxlan.calico</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line">[root@node2 ~]# time curl http://10.96.91.255</span><br></pre></td></tr></table></figure>

<p>在<code>node1</code>的主机网卡上抓包看看封装后的请求是否已到达：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# tcpdump -n -vv -i eth0 host 10.10.72.11 and udp</span><br><span class="line">tcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">07:19:42.730996 IP (tos 0x0, ttl 64, id 6470, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.10.72.11.nim-vdrshell &gt; 10.10.72.10.4789: [bad udp cksum 0xffff -&gt; 0x3af0!] VXLAN, flags [I] (0x08), vni 4096</span><br><span class="line">IP (tos 0x0, ttl 64, id 39190, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    166.166.104.10.35632 &gt; 166.166.166.168.us-cli: Flags [S], cksum 0xe556 (correct), seq 3025623348, win 29200, options [mss 1460,sackOK,TS val 101892130 ecr 0,nop,wscale 7], length 0</span><br><span class="line">07:19:43.733741 IP (tos 0x0, ttl 64, id 6804, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.10.72.11.nim-vdrshell &gt; 10.10.72.10.4789: [bad udp cksum 0xffff -&gt; 0x3af0!] VXLAN, flags [I] (0x08), vni 4096</span><br><span class="line">IP (tos 0x0, ttl 64, id 39191, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    166.166.104.10.35632 &gt; 166.166.166.168.us-cli: Flags [S], cksum 0xe16b (correct), seq 3025623348, win 29200, options [mss 1460,sackOK,TS val 101893133 ecr 0,nop,wscale 7], length 0</span><br><span class="line">07:19:45.736729 IP (tos 0x0, ttl 64, id 7403, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.10.72.11.nim-vdrshell &gt; 10.10.72.10.4789: [bad udp cksum 0xffff -&gt; 0x3af0!] VXLAN, flags [I] (0x08), vni 4096</span><br><span class="line">IP (tos 0x0, ttl 64, id 39192, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    166.166.104.10.35632 &gt; 166.166.166.168.us-cli: Flags [S], cksum 0xd998 (correct), seq 3025623348, win 29200, options [mss 1460,sackOK,TS val 101895136 ecr 0,nop,wscale 7], length 0</span><br><span class="line">07:19:49.744801 IP (tos 0x0, ttl 64, id 9648, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.10.72.11.nim-vdrshell &gt; 10.10.72.10.4789: [bad udp cksum 0xffff -&gt; 0x3af0!] VXLAN, flags [I] (0x08), vni 4096</span><br><span class="line">IP (tos 0x0, ttl 64, id 39193, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    166.166.104.10.35632 &gt; 166.166.166.168.us-cli: Flags [S], cksum 0xc9f0 (correct), seq 3025623348, win 29200, options [mss 1460,sackOK,TS val 101899144 ecr 0,nop,wscale 7], length 0</span><br><span class="line">07:19:57.768735 IP (tos 0x0, ttl 64, id 12853, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.10.72.11.nim-vdrshell &gt; 10.10.72.10.4789: [bad udp cksum 0xffff -&gt; 0x3af0!] VXLAN, flags [I] (0x08), vni 4096</span><br><span class="line">IP (tos 0x0, ttl 64, id 39194, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    166.166.104.10.35632 &gt; 166.166.166.168.us-cli: Flags [S], cksum 0xaa98 (correct), seq 3025623348, win 29200, options [mss 1460,sackOK,TS val 101907168 ecr 0,nop,wscale 7], length 0</span><br><span class="line">07:20:05.087057 IP (tos 0x0, ttl 64, id 8479, offset 0, flags [none], proto UDP (17), length 164)</span><br><span class="line">    10.10.72.10.34565 &gt; 10.10.72.11.4789: [no cksum] VXLAN, flags [I] (0x08), vni 4096</span><br><span class="line">IP (tos 0x0, ttl 63, id 3425, offset 0, flags [DF], proto UDP (17), length 114)</span><br><span class="line">    166.166.166.168.57850 &gt; 166.166.104.6.domain: [udp sum ok] 10121+ AAAA? influxdb-nginx-service.kube-system.svc.kube-system.svc.cluster.local. (86)</span><br><span class="line">07:20:05.087076 IP (tos 0x0, ttl 64, id 54475, offset 0, flags [none], proto UDP (17), length 164)</span><br><span class="line">    10.10.72.10.51841 &gt; 10.10.72.11.4789: [no cksum] VXLAN, flags [I] (0x08), vni 4096</span><br><span class="line">IP (tos 0x0, ttl 63, id 3424, offset 0, flags [DF], proto UDP (17), length 114)</span><br><span class="line">    166.166.166.168.57984 &gt; 166.166.104.6.domain: [udp sum ok] 20020+ A? influxdb-nginx-service.kube-system.svc.kube-system.svc.cluster.local. (86)</span><br><span class="line">07:20:05.087671 IP (tos 0x0, ttl 64, id 13540, offset 0, flags [none], proto UDP (17), length 257)</span><br><span class="line">    10.10.72.11.60395 &gt; 10.10.72.10.4789: [no cksum] VXLAN, flags [I] (0x08), vni 4096</span><br><span class="line">IP (tos 0x0, ttl 63, id 19190, offset 0, flags [DF], proto UDP (17), length 207)</span><br><span class="line">    166.166.104.6.domain &gt; 166.166.166.168.57850: [udp sum ok] 10121 NXDomain*- q: AAAA? influxdb-nginx-service.kube-system.svc.kube-system.svc.cluster.local. 0/1/0 ns: cluster.local. SOA ns.dns.cluster.local. hostmaster.cluster.local. 1647633218 7200 1800 86400 5 (179)</span><br><span class="line">07:20:05.087702 IP (tos 0x0, ttl 64, id 13541, offset 0, flags [none], proto UDP (17), length 257)</span><br><span class="line">    10.10.72.11.48571 &gt; 10.10.72.10.4789: [no cksum] VXLAN, flags [I] (0x08), vni 4096</span><br><span class="line">IP (tos 0x0, ttl 63, id 19191, offset 0, flags [DF], proto UDP (17), length 207)</span><br><span class="line">    166.166.104.6.domain &gt; 166.166.166.168.57984: [udp sum ok] 20020 NXDomain*- q: A? influxdb-nginx-service.kube-system.svc.kube-system.svc.cluster.local. 0/1/0 ns: cluster.local. SOA ns.dns.cluster.local. hostmaster.cluster.local. 1647633218 7200 1800 86400 5 (179)</span><br><span class="line">07:20:05.088801 IP (tos 0x0, ttl 64, id 8480, offset 0, flags [none], proto UDP (17), length 152)</span><br><span class="line">    10.10.72.10.55780 &gt; 10.10.72.11.4789: [no cksum] VXLAN, flags [I] (0x08), vni 4096</span><br><span class="line">IP (tos 0x0, ttl 63, id 3427, offset 0, flags [DF], proto UDP (17), length 102)</span><br><span class="line">    166.166.166.168.56015 &gt; 166.166.104.6.domain: [udp sum ok] 19167+ AAAA? influxdb-nginx-service.kube-system.svc.svc.cluster.local. (74)</span><br><span class="line">07:20:05.089048 IP (tos 0x0, ttl 64, id 13542, offset 0, flags [none], proto UDP (17), length 245)</span><br><span class="line">    10.10.72.11.50151 &gt; 10.10.72.10.4789: [no cksum] VXLAN, flags [I] (0x08), vni 4096</span><br><span class="line">IP (tos 0x0, ttl 63, id 19192, offset 0, flags [DF], proto UDP (17), length 195)</span><br><span class="line">    166.166.104.6.domain &gt; 166.166.166.168.56015: [udp sum ok] 19167 NXDomain*- q: AAAA? influxdb-nginx-service.kube-system.svc.svc.cluster.local. 0/1/0 ns: cluster.local. SOA ns.dns.cluster.local. hostmaster.cluster.local. 1647633218 7200 1800 86400 5 (167)</span><br><span class="line">07:20:05.089212 IP (tos 0x0, ttl 64, id 8481, offset 0, flags [none], proto UDP (17), length 148)</span><br><span class="line">    10.10.72.10.50272 &gt; 10.10.72.11.4789: [no cksum] VXLAN, flags [I] (0x08), vni 4096</span><br><span class="line">IP (tos 0x0, ttl 63, id 3430, offset 0, flags [DF], proto UDP (17), length 98)</span><br><span class="line">    166.166.166.168.54926 &gt; 166.166.104.6.domain: [udp sum ok] 40948+ A? influxdb-nginx-service.kube-system.svc.cluster.local. (70)</span><br><span class="line">07:20:05.089403 IP (tos 0x0, ttl 64, id 13543, offset 0, flags [none], proto UDP (17), length 241)</span><br><span class="line">    10.10.72.11.59882 &gt; 10.10.72.10.4789: [no cksum] VXLAN, flags [I] (0x08), vni 4096</span><br><span class="line">IP (tos 0x0, ttl 63, id 19193, offset 0, flags [DF], proto UDP (17), length 191)</span><br><span class="line">    166.166.104.6.domain &gt; 166.166.166.168.54926: [udp sum ok] 40948 NXDomain*- q: A? influxdb-nginx-service.kube-system.svc.cluster.local. 0/1/0 ns: cluster.local. SOA ns.dns.cluster.local. hostmaster.cluster.local. 1647633218 7200 1800 86400 5 (163)</span><br><span class="line">07:20:05.089524 IP (tos 0x0, ttl 64, id 8482, offset 0, flags [none], proto UDP (17), length 134)</span><br><span class="line">    10.10.72.10.58964 &gt; 10.10.72.11.4789: [no cksum] VXLAN, flags [I] (0x08), vni 4096</span><br><span class="line">IP (tos 0x0, ttl 63, id 3431, offset 0, flags [DF], proto UDP (17), length 84)</span><br><span class="line">    166.166.166.168.50263 &gt; 166.166.104.6.domain: [udp sum ok] 18815+ A? influxdb-nginx-service.kube-system.svc. (56)</span><br><span class="line">07:20:05.089681 IP (tos 0x0, ttl 64, id 13544, offset 0, flags [none], proto UDP (17), length 134)</span><br><span class="line">    10.10.72.11.51874 &gt; 10.10.72.10.4789: [no cksum] VXLAN, flags [I] (0x08), vni 4096</span><br><span class="line">IP (tos 0x0, ttl 63, id 19194, offset 0, flags [DF], proto UDP (17), length 84)</span><br><span class="line">    166.166.104.6.domain &gt; 166.166.166.168.50263: [udp sum ok] 18815 ServFail- q: A? influxdb-nginx-service.kube-system.svc. 0/0/0 (56)</span><br><span class="line">07:20:05.089706 IP (tos 0x0, ttl 64, id 8483, offset 0, flags [none], proto UDP (17), length 134)</span><br><span class="line">    10.10.72.10.59891 &gt; 10.10.72.11.4789: [no cksum] VXLAN, flags [I] (0x08), vni 4096</span><br><span class="line">IP (tos 0x0, ttl 63, id 3433, offset 0, flags [DF], proto UDP (17), length 84)</span><br><span class="line">    166.166.166.168.49202 &gt; 166.166.104.6.domain: [udp sum ok] 58612+ AAAA? influxdb-nginx-service.kube-system.svc. (56)</span><br><span class="line">07:20:05.089859 IP (tos 0x0, ttl 64, id 13545, offset 0, flags [none], proto UDP (17), length 134)</span><br><span class="line">    10.10.72.11.44146 &gt; 10.10.72.10.4789: [no cksum] VXLAN, flags [I] (0x08), vni 4096</span><br><span class="line">IP (tos 0x0, ttl 63, id 19195, offset 0, flags [DF], proto UDP (17), length 84)</span><br><span class="line">    166.166.104.6.domain &gt; 166.166.166.168.49202: [udp sum ok] 58612 ServFail- q: AAAA? influxdb-nginx-service.kube-system.svc. 0/0/0 (56)</span><br></pre></td></tr></table></figure>

<p>从抓包结果看，出现一个可疑点：前几个报文中提示<code>bad udp cksum 0xffff</code>，请求通的最后几个报文提示的是<code>no cksum</code>。</p>
<p>根据这个错误信息，搜索发现是个已知<code>bug</code>，相关的详细定位可以参考[1]-[3]，这里就不细说了。大概原因如下所述：</p>
<blockquote>
<p>内核中存在一个和<code>VXLAN</code>处理有关的缺陷，该缺陷会导致<code>Checksum Offloading</code>不能正确完成。这个缺陷仅仅在很边缘的场景下才会表现出来。</p>
<p>在<code>VXLAN</code>的<code>UDP</code>头被<code>NAT</code>过的前提下，如果：</p>
<ol>
<li><code>VXLAN</code>设备禁用（这是<code>RFC</code>的建议）了<code>UDP Checksum</code></li>
<li><code>VXLAN</code>设备启用了<code>Tx Checksum Offloading</code></li>
</ol>
<p>就会导致生成错误的<code>UDP Checksum</code>。</p>
</blockquote>
<p>从资料[1]看，<code>K8S</code>的<code>v1.18.5</code>版本已经修复了这个问题，但我的问题是在<code>v1.21.0</code>上发现的，所以不确定只升级<code>K8S</code>是否可以解决该问题，或者升级后还需要额外配置什么？</p>
<p>从资料[3]和[4]看，<code>calico</code>在<code>v3.20.0</code>版本做了修改：在<code>kernels &lt; v5.7</code>时也禁用了<code>calico.vxlan</code>接口的<code>Offloading</code>功能。</p>
<p>本地临时禁用并验证：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@node2 ~]# ethtool --offload vxlan.calico rx off tx off</span><br><span class="line">Actual changes:</span><br><span class="line">rx-checksumming: off</span><br><span class="line">tx-checksumming: off</span><br><span class="line">        tx-checksum-ip-generic: off</span><br><span class="line">tcp-segmentation-offload: off</span><br><span class="line">        tx-tcp-segmentation: off [requested on]</span><br><span class="line">        tx-tcp-ecn-segmentation: off [requested on]</span><br><span class="line">        tx-tcp6-segmentation: off [requested on]</span><br><span class="line">        tx-tcp-mangleid-segmentation: off [requested on]</span><br><span class="line">udp-fragmentation-offload: off [requested on]</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node2 ~]# time curl http://10.96.91.255</span><br><span class="line">real    0m0.009s</span><br><span class="line">user    0m0.002s</span><br><span class="line">sys     0m0.007s</span><br></pre></td></tr></table></figure>

<p>请求恢复正常。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ol>
<li>临时解决： <code>ethtool --offload vxlan.calico rx off tx off</code></li>
<li>永久解决：升级<code>calico &gt;=v3.20.0</code>或升级内核到<code>5.6.13, 5.4.41, 4.19.123, 4.14.181</code>，单独升级<code>K8S &gt;= v1.18.5</code>版本待确认是否能解决</li>
</ol>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://blog.gmem.cc/nodeport-63s-delay-due-to-kernel-issue">https://blog.gmem.cc/nodeport-63s-delay-due-to-kernel-issue</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudnative.to/blog/kubernetes-1-17-vxlan-63s-delay/">https://cloudnative.to/blog/kubernetes-1-17-vxlan-63s-delay/</a></li>
<li><a target="_blank" rel="noopener" href="https://bowser1704.github.io/posts/vxlan-bug/">https://bowser1704.github.io/posts/vxlan-bug/</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/projectcalico/calico/issues/3145">https://github.com/projectcalico/calico/issues/3145</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/projectcalico/felix/pull/2811/files">https://github.com/projectcalico/felix/pull/2811/files</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2022/02/26/docker/Docker%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E5%BC%82%E5%B8%B8%E6%96%AD%E7%94%B5%E5%AF%BC%E8%87%B4Docker%E5%90%AF%E5%8A%A8%E5%8D%A1%E4%BD%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/26/docker/Docker%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E5%BC%82%E5%B8%B8%E6%96%AD%E7%94%B5%E5%AF%BC%E8%87%B4Docker%E5%90%AF%E5%8A%A8%E5%8D%A1%E4%BD%8F/" class="post-title-link" itemprop="url">Docker问题排查-异常断电导致Docker启动卡住</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-02-26 18:13:11" itemprop="dateCreated datePublished" datetime="2022-02-26T18:13:11+00:00">2022-02-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:10:03" itemprop="dateModified" datetime="2024-04-14T02:10:03+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>12 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>Docker守护进程在异常断电后卡在<code>activating</code>状态，并且内存占用在无限增加。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 docker]# service docker status</span><br><span class="line">Redirecting to /bin/systemctl status docker.service</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)</span><br><span class="line">  Drop-In: /etc/systemd/system/docker.service.d</span><br><span class="line">           └─restart.conf</span><br><span class="line">   Active: activating (start) since Tue 2021-12-07 15:44:32 CST; 1min 18s ago</span><br><span class="line">     Docs: https:/docs.docker.com</span><br><span class="line"> Main PID: 274797 (dockerd)</span><br><span class="line">    Tasks: 481</span><br><span class="line">   Memory: 15.4G  -- 占用不正常</span><br><span class="line">   CGroup: /system</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 docker]# service docker status</span><br><span class="line">Redirecting to /bin/systemctl status docker.service</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)</span><br><span class="line">  Drop-In: /etc/systemd/system/docker.service.d</span><br><span class="line">           └─restart.conf</span><br><span class="line">   Active: deactivating (stop-sigterm)</span><br><span class="line">     Docs: https:/docs.docker.com</span><br><span class="line"> Main PID: 274797 (dockerd)</span><br><span class="line">    Tasks: 481</span><br><span class="line">   Memory: 247.3G   -- 一直在疯狂增加</span><br><span class="line">   CGroup: /system.slice/docker.service</span><br><span class="line">           └─274</span><br></pre></td></tr></table></figure>

<h2 id="定位过程"><a href="#定位过程" class="headerlink" title="定位过程"></a>定位过程</h2><p>首先，查看<code>docker</code>版本和<code>docker info</code>信息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">[root@node01files]# </span><span class="language-bash">docker version</span></span><br><span class="line">Client: Docker Engine - Community</span><br><span class="line"> Version:           20.10.7</span><br><span class="line"> API version:       1.41</span><br><span class="line"> Go version:        go1.13.15</span><br><span class="line"> Git commit:        f0df350</span><br><span class="line"> Built:             Wed Jun  2 11:58:10 2021</span><br><span class="line"> OS/Arch:           linux/amd64</span><br><span class="line"> Context:           default</span><br><span class="line"> Experimental:      true</span><br><span class="line"></span><br><span class="line">Server: Docker Engine - Community</span><br><span class="line"> Engine:</span><br><span class="line">  Version:          20.10.7</span><br><span class="line">  API version:      1.41 (minimum version 1.12)</span><br><span class="line">  Go version:       go1.13.15</span><br><span class="line">  Git commit:       b0f5bc3</span><br><span class="line">runc:</span><br><span class="line">  Version:          1.0.0-rc95</span><br><span class="line">  GitCommit:        b9ee9c6314599f1b4a7f497e1f1f856fe433d3b7</span><br><span class="line"> docker-init:</span><br><span class="line">  Version:          0.19.0</span><br><span class="line">  GitCommit:        de40ad0</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 files]# docker info</span><br><span class="line">Client:</span><br><span class="line"> Context:    default</span><br><span class="line"> Debug Mode: false</span><br><span class="line"> Plugins:</span><br><span class="line">  app: Docker App (Docker Inc., v0.9.1-beta3)</span><br><span class="line">  buildx: Build with BuildKit (Docker Inc., v0.5.1-docker)</span><br><span class="line">  scan: Docker Scan (Docker Inc., v0.8.0)</span><br><span class="line"></span><br><span class="line">Server:</span><br><span class="line"> Server Version: 20.10.7</span><br><span class="line"> Storage Driver: overlay2</span><br><span class="line">  Backing Filesystem: xfs</span><br><span class="line">  Supports d_type: true</span><br><span class="line">  Native Overlay Diff: true</span><br><span class="line">  userxattr: false</span><br><span class="line"> Logging Driver: json-file</span><br><span class="line"> Cgroup Driver: systemd</span><br><span class="line"> Cgroup Version: 1</span><br><span class="line"> Plugins:</span><br><span class="line">  Volume: local</span><br><span class="line">  Network: bridge host ipvlan macvlan null overlay</span><br><span class="line">  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog</span><br><span class="line"> Swarm: inactive</span><br><span class="line"> Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc</span><br><span class="line"> Default Runtime: runc</span><br><span class="line"> Init Binary: docker-init</span><br><span class="line"> containerd version: d71fcd7d8303cbf684402823e425e9dd2e99285d</span><br><span class="line"> runc version: b9ee9c6314599f1b4a7f497e1f1f856fe433d3b7</span><br><span class="line"> init version: de40ad0</span><br><span class="line"> Security Options:</span><br><span class="line">   seccomp</span><br><span class="line">   Profile: default</span><br><span class="line"> Kernel Version: 3.10.0-957.21.3.el7.x86_64</span><br><span class="line"> Operating System: CentOS 7.6</span><br><span class="line"> OSType: linux</span><br><span class="line"> Architecture: x86_64</span><br><span class="line"> Total Memory: 256GiB</span><br><span class="line"> Name: node01</span><br><span class="line"> Docker Root Dir: /var/lib/docker</span><br><span class="line"> Debug Mode: false</span><br><span class="line">  File Descriptors: 366</span><br><span class="line">  Goroutines: 290</span><br><span class="line">  EventsListeners: 0</span><br><span class="line"> Registry: https:/index.docker.io/v1/</span><br><span class="line"> Labels:</span><br><span class="line"> Experimental: false</span><br><span class="line"> Insecure Registries:</span><br><span class="line">  127.0.0.0/8</span><br><span class="line"> Live Restore Enabled: true</span><br></pre></td></tr></table></figure>

<p>版本还算比较新，查看<code>docker</code>日志：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">Dec 07 14:57:57 node01 dockerd[239420]: time=&quot;2021-12-07T14:57:57.831370861+08:00&quot; level=info msg=&quot;Starting up&quot;</span><br><span class="line">Dec 07 14:57:57 node01 dockerd[239420]: time=&quot;2021-12-07T14:57:57.950367668+08:00&quot; level=info msg=&quot;parsed scheme: \&quot;unix\&quot;&quot; module=grpc</span><br><span class="line">Dec 07 14:57:57 node01 dockerd[239420]: time=&quot;2021-12-07T14:57:57.950430356+08:00&quot; level=info msg=&quot;scheme \&quot;unix\&quot; not registered, fallback to default scheme&quot; module=grpc</span><br><span class="line">Dec 07 14:57:57 node01 dockerd[239420]: time=&quot;2021-12-07T14:57:57.950486773+08:00&quot; level=info msg=&quot;ccResolverWrapper: sending update to cc: &#123;[&#123;unix://run/containerd/containerd.sock  &lt;nil&gt; 0 &lt;nil&gt;&#125;] &lt;nil&gt; &lt;nil&gt;&#125;&quot; module=grpc</span><br><span class="line">Dec 07 14:57:57 node01 dockerd[239420]: time=&quot;2021-12-07T14:57:57.950524941+08:00&quot; level=info msg=&quot;ClientConn switching balancer to \&quot;pick_first\&quot;&quot; module=grpc</span><br><span class="line">Dec 07 14:57:58 node01 dockerd[239420]: time=&quot;2021-12-07T14:57:58.004622322+08:00&quot; level=info msg=&quot;parsed scheme: \&quot;unix\&quot;&quot; module=grpc</span><br><span class="line">Dec 07 14:57:58 node01 dockerd[239420]: time=&quot;2021-12-07T14:57:58.004663918+08:00&quot; level=info msg=&quot;scheme \&quot;unix\&quot; not registered, fallback to default scheme&quot; module=grpc</span><br><span class="line">Dec 07 14:57:58 node01 dockerd[239420]: time=&quot;2021-12-07T14:57:58.004697382+08:00&quot; level=info msg=&quot;ccResolverWrapper: sending update to cc: &#123;[&#123;unix://run/containerd/containerd.sock  &lt;nil&gt; 0 &lt;nil&gt;&#125;] &lt;nil&gt; &lt;nil&gt;&#125;&quot; module=grpc</span><br><span class="line">Dec 07 14:57:58 node01 dockerd[239420]: time=&quot;2021-12-07T14:57:58.004726533+08:00&quot; level=info msg=&quot;ClientConn switching balancer to \&quot;pick_first\&quot;&quot; module=grpc</span><br><span class="line">Dec 07 14:58:14 node01 dockerd[239420]: time=&quot;2021-12-07T14:58:14.832862519+08:00&quot; level=info msg=&quot;[graphdriver] using prior storage driver: overlay2&quot;</span><br><span class="line">Dec 07 14:59:00 node01 dockerd[239420]: time=&quot;2021-12-07T14:59:00.039554832+08:00&quot; level=info msg=&quot;Loading containers: start.&quot;</span><br><span class="line">Dec 07 15:02:27 node01 dockerd[239420]: fatal error: runtime: out of memory</span><br><span class="line">Dec 07 15:02:27 node01 dockerd[239420]: runtime stack:</span><br><span class="line">Dec 07 15:02:27 node01 dockerd[239420]: runtime.throw(0x5604d0deee9d, 0x16)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /usr/local/go/src/runtime/panic.go:774 +0x74</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: runtime.sysMap(0xfb6c000000, 0x5c000000, 0x5604d35dab58)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /usr/local/go/src/runtime/mem_linux.go:169 +0xc7</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: runtime.(*mheap).sysAlloc(0x5604d35be240, 0x5beee000, 0x7f6fd1fbab88, 0x5604cf22bf6b)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /usr/local/go/src/runtime/malloc.go:701 +0x1cf</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: runtime.(*mheap).grow(0x5604d35be240, 0x2df77, 0xffffffff)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /usr/local/go/src/runtime/mheap.go:1255 +0xa5</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: runtime.(*mheap).allocSpanLocked(0x5604d35be240, 0x2df77, 0x5604d35dab68, 0x5604cf212927)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /usr/local/go/src/runtime/mheap.go:1170 +0x268</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: runtime.(*mheap).alloc_m(0x5604d35be240, 0x2df77, 0x7f6c01180100, 0x7f6c01184350)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /usr/local/go/src/runtime/mheap.go:1022 +0xc6</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: runtime.(*mheap).alloc.func1()</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /usr/local/go/src/runtime/mheap.go:1093 +0x4e</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: runtime.(*mheap).alloc(0x5604d35be240, 0x2df77, 0x7f6fd1010100, 0x7f6c01184350)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /usr/local/go/src/runtime/mheap.go:1092 +0x8c</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: runtime.largeAlloc(0x5beee000, 0x5604cf250001, 0x7f6c01184350)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /usr/local/go/src/runtime/malloc.go:1138 +0x99</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: runtime.mallocgc.func1()</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /usr/local/go/src/runtime/malloc.go:1033 +0x48</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: runtime.systemstack(0x0)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /usr/local/go/src/runtime/asm_amd64.s:370 +0x63</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: runtime.mstart()</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /usr/local/go/src/runtime/proc.go:1146</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: goroutine 1 [running]:</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: runtime.systemstack_switch()</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /usr/local/go/src/runtime/asm_amd64.s:330 fp=0xc0008bff70 sp=0xc0008bff68 pc=0x5604cf2528e0</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: runtime.mallocgc(0x5beee000, 0x5604d1e8fde0, 0x1, 0x0)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /usr/local/go/src/runtime/malloc.go:1032 +0x8a7 fp=0xc0008c0010 sp=0xc0008bff70 pc=0x5604cf200ef7</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: runtime.growslice(0x5604d1e8fde0, 0xef119e4000, 0x3107eaa, 0x3107eaa, 0x3107eab, 0x5604cf20146b, 0xfb6bbbfb80, 0x7f6c01184350)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /usr/local/go/src/runtime/slice.go:181 +0x1e4 fp=0xc0008c0078 sp=0xc0008c0010 pc=0x5604cf23b0b4</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/go.etcd.io/bbolt.(*Cursor).first(0xc0008c0288)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: docker/vendor/go.etcd.io/bbolt/cursor.go:182 +0x138 fp=0xc0008c00d8 sp=0xc0008c0078 pc=0x5604d012d348</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/go.etcd.io/bbolt.(*Cursor).next(0xc0008c0288, 0x0, 0x9f, 0x9f, 0x7f7150506429, 0x2b8, 0xc0008c0188, 0x5604cf23ee10)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/go.etcd.io/bbolt/cursor.go:234 +0x84 fp=0xc0008c0128 sp=0xc0008c00d8 pc=0x5604d012d684</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/go.etcd.io/bbolt.(*Cursor).Next(0xc0008c0288, 0x7f715050638a, 0x9f, 0x9f, 0xfb6bbcf720, 0x9f, 0x2b8)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/go.etcd.io/bbolt/cursor.go:75 +0x43 fp=0xc0008c0198 sp=0xc0008c0128 pc=0x5604d012ca43</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/libkv/store/boltdb.(*BoltDB).List.func1(0xc0034350a0, 0xc0008c0200, 0xc0034350a0)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/libkv/store/boltdb/boltdb.go:288 +0x19a fp=0xc0008c02b8 sp=0xc0008c0198 pc=0x5604d0142a4a</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/go.etcd.io/bbolt.(*DB).View(0xc0001dc200, 0xc0008c03f8, 0x0, 0x0)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/go.etcd.io/bbolt/db.go:725 +0xaa fp=0xc0008c0340 sp=0xc0008c02b8 pc=0x5604d0130a3a</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/libkv/store/boltdb.(*BoltDB).List(0xc00482f2c0, 0xc001a7af00, 0x5e, 0x0, 0x0, 0x0, 0x0, 0x0)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/libkv/store/boltdb/boltdb.go:279 +0x1b3 fp=0xc0008c0430 sp=0xc0008c0340 pc=0x5604d0141583</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/libnetwork/datastore.(*cache).kmap(0xc003bf5900, 0x5604d220c1a0, 0xc001e7c160, 0x0, 0x5e, 0xc001a7ae40)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/libnetwork/datastore/cache.go:43 +0x286 fp=0xc0008c0560 sp=0xc0008c0430 pc=0x5604cfcc81a6</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/libnetwork/datastore.(*cache).list(0xc003bf5900, 0x5604d220c1a0, 0xc001e7c160, 0x0, 0x0, 0x0, 0x0, 0x0)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/libnetwork/datastore/cache.go:164 +0x7e fp=0xc0008c0678 sp=0xc0008c0560 pc=0x5604cfcc988e</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/libnetwork/datastore.(*datastore).List(0xc0079ff680, 0xc001a7ae40, 0x5e, 0x5604d220c1a0, 0xc001e7c160, 0x0, 0x0, 0x0, 0x0, 0x0)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/libnetwork/datastore/datastore.go:517 +0x205 fp=0xc0008c0730 sp=0xc0008c0678 pc=0x5604cfccc745</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/libnetwork.(*network).getEndpointsFromStore(0xc00289a380, 0xc004a99258, 0x7, 0x5604d2206920, 0xc001a515f0, 0xc002f3ff48)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/libnetwork/store.go:190 +0x343 fp=0xc0008c09f8 sp=0xc0008c0730 pc=0x5604d01acaf3</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/libnetwork.(*controller).reservePools(0xc0004b7400)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/libnetwork/controller.go:977 +0x4c1 fp=0xc0008c0c28 sp=0xc0008c09f8 pc=0x5604d0159701</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/libnetwork.New(0xc001055d00, 0x9, 0x10, 0xc0007ac870, 0xc001387fb0, 0xc001055d00, 0x9)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/vendor/libnetwork/controller.go:245 +0x615 fp=0xc0008c0dd8 sp=0xc0008c0c28 pc=0x5604d0154815</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/daemon.(*Daemon).initNetworkController(0xc00000c1e0, 0xc000207080, 0xc001387fb0, 0xc000236f50, 0xc00000c1e0, 0xc0007170c8, 0xc001387fb0)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/daemon/daemon_unix.go:855 +0xa9 fp=0xc0008c0e90 sp=0xc0008c0dd8 pc=0x5604d0a49429</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/daemon.(*Daemon).restore(0xc00000c1e0, 0xc00004e5c0, 0xc000220000)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/daemon/daemon.go:490 +0x50b fp=0xc0008c1088 sp=0xc0008c0e90 pc=0x5604d0a3898b</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/daemon.NewDaemon(0x5604d21de4c0, 0xc00004e5c0, 0xc000207080, 0xc0007ac870, 0x0, 0x0, 0x0)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/daemon/daemon.go:1147 +0x2be8 fp=0xc0008c19c8 sp=0xc0008c1088 pc=0x5604d0a3c938</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: main.(*DaemonCli).start(0xc0007ab410, 0xc000218600, 0x0, 0x0)</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: /docker/cmd/dockerd/daemon.go:195 +0x743 fp=0xc0008c1d00 sp=0xc0008c19c8 pc=0x5604d0dbb3c3</span><br><span class="line">Dec 07 15:02:28 node01 dockerd[239420]: main.runDaemon(...)</span><br></pre></td></tr></table></figure>

<p>日志中存在明显的异常堆栈打印，分析可知，启动过程中走到<code>Loading containers: start.</code>之后卡住，然后打印<code>fatal error: runtime: out of memory</code>，也就是内存爆了。根据堆栈信息，可以看出异常的调用路径如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NewDaemon —&gt; restore() —&gt; initNetworkController —&gt; libnetwork.New() —&gt; reservePools() —&gt;  getEndpointsFromStore() —&gt; List —&gt; cache.list(kvObject) —&gt; cache.kmap(kvObject) —&gt; List(keyPrefix) —&gt; Next —&gt; next —&gt; first</span><br></pre></td></tr></table></figure>

<p>拉取对应版本的代码，根据上述调用过程，找到指定的代码位置：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// List returns the range of keys starting with the passed in prefix</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(b *BoltDB)</span></span> List(keyPrefix <span class="type">string</span>) ([]*store.KVPair, <span class="type">error</span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> (</span><br><span class="line">        db  *bolt.DB</span><br><span class="line">        err <span class="type">error</span></span><br><span class="line">    )</span><br><span class="line">    b.Lock()</span><br><span class="line">    <span class="keyword">defer</span> b.Unlock()</span><br><span class="line"></span><br><span class="line">    kv := []*store.KVPair&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> db, err = b.getDBhandle(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">defer</span> b.releaseDBhandle()</span><br><span class="line"></span><br><span class="line">    err = db.View(<span class="function"><span class="keyword">func</span><span class="params">(tx *bolt.Tx)</span></span> <span class="type">error</span> &#123;</span><br><span class="line">        bucket := tx.Bucket(b.boltBucket)</span><br><span class="line">        <span class="keyword">if</span> bucket == <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> store.ErrKeyNotFound</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        cursor := bucket.Cursor()</span><br><span class="line">        prefix := []<span class="type">byte</span>(keyPrefix)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> key, v := cursor.Seek(prefix); bytes.HasPrefix(key, prefix); key, v = cursor.Next() &#123;</span><br><span class="line"></span><br><span class="line">            dbIndex := binary.LittleEndian.Uint64(v[:libkvmetadatalen])</span><br><span class="line">            v = v[libkvmetadatalen:]</span><br><span class="line">            val := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="built_in">len</span>(v))</span><br><span class="line">            <span class="built_in">copy</span>(val, v)</span><br><span class="line"></span><br><span class="line">            kv = <span class="built_in">append</span>(kv, &amp;store.KVPair&#123;</span><br><span class="line">                Key:       <span class="type">string</span>(key),</span><br><span class="line">                Value:     val,</span><br><span class="line">                LastIndex: dbIndex,</span><br><span class="line">            &#125;)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(kv) == <span class="number">0</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, store.ErrKeyNotFound</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> kv, err</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看出，这段是读取<code>boltdb</code>数据库（用于缓存网络配置），从之前了解看，这个数据库在异常断电时很容易损坏，所以怀疑是数据库损坏了，导致此处的遍历读取超出了预期的循环次数，而每次循环都会创建变量，分配内存，最终被内核<code>OOM</code>。</p>
<p>在<code>docker</code>社区查找相关<code>issue</code>[1] [2]，发现确实存在<code>boltdb</code>数据库损坏的现象，不过最终报错的现象不太一样。最后，在<code>docker</code>社区也提了个<code>issue</code>[3]，社区反馈也是怀疑<code>boltdb</code>数据库损坏，并建议可以把<code>local-kv.db</code>文件删除再重启来恢复。</p>
<p>个人觉得，上面报错的地方可以优化一下，对<code>db</code>文件做一次检查，如果检查到异常，提前抛异常，而不是不停地吃内存（由于异常的环境被破坏了，这个想法需要等复现后再考虑优化）。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ol>
<li>临时解决：删除<code>local-kv.db</code>文件再重启<code>docker</code>服务。</li>
<li>永久解决：优化异常代码。</li>
</ol>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/moby/moby/issues/37238">https://github.com/moby/moby/issues/37238</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/moby/moby/issues/42099">https://github.com/moby/moby/issues/42099</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/moby/moby/issues/43072">https://github.com/moby/moby/issues/43072</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">LeaoYao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">89</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LeaoYao</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
