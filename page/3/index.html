<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lyyao09.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="云原生知识星球">
<meta property="og:url" content="https://lyyao09.github.io/page/3/index.html">
<meta property="og:site_name" content="云原生知识星球">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="LeaoYao">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://lyyao09.github.io/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>云原生知识星球</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">云原生知识星球</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/12/18/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E6%8C%82%E8%BD%BDSubpath%E7%9A%84%E5%AE%B9%E5%99%A8%E5%9C%A8Configmap%E5%8F%98%E6%9B%B4%E5%90%8E%E9%87%8D%E5%90%AF%E6%97%B6%E6%8C%82%E8%BD%BD%E5%A4%B1%E8%B4%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/18/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E6%8C%82%E8%BD%BDSubpath%E7%9A%84%E5%AE%B9%E5%99%A8%E5%9C%A8Configmap%E5%8F%98%E6%9B%B4%E5%90%8E%E9%87%8D%E5%90%AF%E6%97%B6%E6%8C%82%E8%BD%BD%E5%A4%B1%E8%B4%A5/" class="post-title-link" itemprop="url">K8S问题排查-挂载Subpath的容器在Configmap变更后重启时挂载失败</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-12-18 14:43:43" itemprop="dateCreated datePublished" datetime="2021-12-18T14:43:43+00:00">2021-12-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:09:04" itemprop="dateModified" datetime="2024-04-14T02:09:04+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>本次发现的问题是踩到了老版本<code>Kubernetes</code>的坑，查找资料发现<code>fatedier</code>大佬已经做了很棒的分析，此处转载过来仅做学习记录。</p>
<blockquote>
<p>作者：<a target="_blank" rel="noopener" href="http://blog.fatedier.com/">fatedier</a><br>本文出处：<a target="_blank" rel="noopener" href="https://blog.fatedier.com/2020/04/17/pod-loopcrash-of-k8s-subpath/">https://blog.fatedier.com/2020/04/17/pod-loopcrash-of-k8s-subpath/</a><br>文章版权归本人所有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文链接，否则保留追究法律责任的权利。</p>
</blockquote>
<h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p><code>Kubernetes</code>对于挂载了 <code>subpath</code> 的容器，在<code> configmap</code> 或其他 <code>volume</code> 变更后，如果容器因为意外退出后，就会持续<code>crash</code>，无法正常启动。</p>
<p>社区相关 issue <a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubernetes/issues/68211">#68211</a>，问题已经在<code>v1.19</code>版本解决。</p>
<h2 id="分析过程"><a href="#分析过程" class="headerlink" title="分析过程"></a>分析过程</h2><h3 id="复现步骤"><a href="#复现步骤" class="headerlink" title="复现步骤"></a>复现步骤</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span> </span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">configMap:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">extra-cfg</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">extra-cfg</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ubuntu:bionic</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;sleep&quot;</span>, <span class="string">&quot;30&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">extra-cfg</span></span><br><span class="line">        <span class="attr">mountPath:</span> <span class="string">/etc/extra.ini</span></span><br><span class="line">        <span class="attr">subPath:</span> <span class="string">extra.ini</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">extra.ini:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    somedata</span></span><br><span class="line"><span class="string"></span><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">extra-cfg</span></span><br></pre></td></tr></table></figure>

<p><code>Apply</code> 此配置，<code>Pod</code> 启动完成后，修改 <code>configmap </code>的内容，等待 <code>30</code> 秒后容器自动退出，<code>kubelet</code> 重启容器，此时观察到容器持续 <code>mount</code> 失败。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error: failed to start container &quot;test&quot;: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused &quot;process_linux.go:424: container init caused \&quot;rootfs_linux.go:58: mounting \\\&quot;/var/lib/kubelet/pods/e044883a-48da-4d28-b304-1a57dcb32203/volume-subpaths/extra-cfg/test/0\\\&quot; to rootfs \\\&quot;/var/lib/docker/overlay2/31b076d0012aad47aa938b482de24ecda8b41505489a22f63b8a3e4ce39b43ba/merged\\\&quot; at \\\&quot;/var/lib/docker/overlay2/31b076d0012aad47aa938b482de24ecda8b41505489a22f63b8a3e4ce39b43ba/merged/etc/extra.ini\\\&quot; caused \\\&quot;no such file or directory\\\&quot;\&quot;&quot;: unknown</span><br></pre></td></tr></table></figure>

<h3 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h3><h4 id="Configmap-Volume-的更新"><a href="#Configmap-Volume-的更新" class="headerlink" title="Configmap Volume 的更新"></a>Configmap Volume 的更新</h4><p>当容器第一次启动前，<code>kubelet</code> 先将 <code>configmap</code> 中的内容下载到 <code>Pod</code> 对应的 <code>Volume</code> 目录下，例如 <code>/var/lib/kubelet/pods/&#123;Pod UID&#125;/volumes/kubernetes.io~configmap/extra-cfg</code>。</p>
<p>同时为了保证对此 <code>volume</code> 下内容的更新是原子的(更新目录时)，所以会通过软链接的方式进行更新，目录中文件如下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">drwxrwxrwx 3 root root 4.0K Mar 29 03:12 .</span><br><span class="line">drwxr-xr-x 3 root root 4.0K Mar 29 03:12 ..</span><br><span class="line">drwxr-xr-x 2 root root 4.0K Mar 29 03:12 ..2020_03_29_03_12_44.788930127</span><br><span class="line">lrwxrwxrwx 1 root root   31 Mar 29 03:12 ..data -&gt; ..2020_03_29_03_12_44.788930127</span><br><span class="line">lrwxrwxrwx 1 root root   16 Mar 29 03:12 extra.ini -&gt; ..data/extra.ini</span><br></pre></td></tr></table></figure>

<p><code>extra.ini</code> 是 <code>..data/extra.ini</code> 的软链，<code>..data</code> 是 <code>..2020_03_29_03_12_44.788930127</code> 的软链，命名为时间戳的目录存放真实内容。</p>
<p>当 <code>configmap</code> 更新后，会生成新的时间戳的目录存放更新后的内容。</p>
<p>创建新的软链 <code>..data_tmp</code> 指向新的时间戳目录，之后重命名为 <code>..data</code>，重命名是一个原子操作。</p>
<p>最后删除旧的时间戳目录。</p>
<h4 id="容器挂载-subpath-Volume-的准备"><a href="#容器挂载-subpath-Volume-的准备" class="headerlink" title="容器挂载 subpath Volume 的准备"></a>容器挂载 subpath Volume 的准备</h4><p>当 <code>configmap Volume</code> 准备完成后，<code>kubelet</code> 会将 <code>configmap</code> 中 <code>subpath</code> 指定的文件 <code>bind mount</code> 到一个特殊的目录下: <code>/var/lib/kubelet/pods/&#123;Pod UID&#125;/volume-subpaths/extra-cfg/&#123;container name&#125;/0</code>。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/self/mountinfo|grep extra</span><br><span class="line">2644 219 8:1 /var/lib/kubelet/pods/&#123;Pod UID&#125;/volumes/kubernetes.io~configmap/extra-cfg/..2020_03_29_03_12_13.444136014/extra.ini /var/lib/kubelet/pods/&#123;Pod UID&#125;/volume-subpaths/extra-cfg/test/0 rw,relatime shared:99 - ext4 /dev/sda1 rw,data=ordered</span><br></pre></td></tr></table></figure>

<p>可以看出，<code>bind mount</code> 的文件其实是真实文件的时间戳目录下的内容。</p>
<p>当 <code>Configmap</code> 更新后，此时间戳目录会被删除，源文件加上了 <code>//deleted</code>。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/self/mountinfo|grep extra</span><br><span class="line">2644 219 8:1 /var/lib/kubelet/pods/&#123;Pod UID&#125;/volumes/kubernetes.io~configmap/extra-cfg/..2020_03_29_03_12_13.444136014/extra.ini//deleted /var/lib/kubelet/pods/&#123;Pod UID&#125;/volume-subpaths/extra-cfg/test/0 rw,relatime shared:99 - ext4 /dev/sda1 rw,data=ordered</span><br></pre></td></tr></table></figure>

<h4 id="Bind-Mount"><a href="#Bind-Mount" class="headerlink" title="Bind Mount"></a>Bind Mount</h4><p>当容器启动时，需要将 <code>/var/lib/kubelet/pods/&#123;Pod UID&#125;/volume-subpaths/extra-cfg/test/0</code> 挂载到容器中。</p>
<p>如果原来的时间戳目录被删除，则 <code>mount</code> 会出错: <code>mount: mount(2) failed: No such file or directory</code>。</p>
<p>通过简单的命令模拟这个问题:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">touch</span> a b c</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">mount --<span class="built_in">bind</span> a b</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">rm</span> -f a</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">mount --<span class="built_in">bind</span> b c</span></span><br><span class="line">mount: mount(2) failed: No such file or directory</span><br></pre></td></tr></table></figure>

<p>可以看到，当 <code>a</code> 删除后，<code>b</code> 挂载点无法再被 <code>mount</code>。所以，当容器异常退出需要重启后，如果 <code>configmap</code> 被更新，原先的时间戳文件被删除，这个 <code>subpath</code> 就无法再被 <code>mount</code> 到容器中。</p>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="Configmap-变更后-Unmount"><a href="#Configmap-变更后-Unmount" class="headerlink" title="Configmap 变更后 Unmount"></a>Configmap 变更后 Unmount</h4><p>社区相关 PR: <a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubernetes/pull/82784">https://github.com/kubernetes/kubernetes/pull/82784</a></p>
<p>在容器重启前，检查 <code>subpath</code> 挂载点的源文件和新的目标 <code>subpath</code> 文件是否一致。</p>
<p>当 <code>configmap</code> 被更新后，时间戳目录变更，则检查到不一致。将 <code>/var/lib/kubelet/pods/&#123;Pod UID&#125;/volume-subpaths/extra-cfg/test/0</code> <code>Unmount</code>，再重新 <code>Bind Mount</code> 当前最新的时间戳目录下的对应文件。</p>
<p>根据社区 <code>PR</code> 中的 <code>comments</code> 来看，此方案可能存在一定风险，尚不明确(有人指出在 4.18 以下内核是不安全的 <a target="_blank" rel="noopener" href="https://github.com/es-container/kubernetes/pull/24/files#diff-f0ba2b2ac6f7b574258c97a4001460b2R829">链接</a>)，所以很长时间都没有进展。</p>
<p>通过一段时间的测试，尚未发现明显的问题。</p>
<h4 id="不使用-subpath"><a href="#不使用-subpath" class="headerlink" title="不使用 subpath"></a>不使用 subpath</h4><p>使用其他方式绕过这个问题。</p>
<p>例如可以将 <code>Configmap</code> 整个 <code>Mount</code> 到容器的其他目录下，再在容器启动时通过软链的方式链接到对应的路径下。</p>
<h3 id="为什么使用间接-Bind-Mount-而不是直接-Mount-软链接"><a href="#为什么使用间接-Bind-Mount-而不是直接-Mount-软链接" class="headerlink" title="为什么使用间接 Bind Mount 而不是直接 Mount 软链接"></a>为什么使用间接 Bind Mount 而不是直接 Mount 软链接</h3><p>参考 <a target="_blank" rel="noopener" href="https://kubernetes.io/blog/2018/04/04/fixing-subpath-volume-vulnerability/">https://kubernetes.io/blog/2018/04/04/fixing-subpath-volume-vulnerability/</a> 这篇文章。</p>
<p>可以看出原先使用的就是直接 <code>Mount</code> 软链接的方式，但是存在安全漏洞，<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Symlink_race">symlink race</a> 。恶意程序可以通过构造一个软链接，使特权程序(<code>kubelet</code>) 将超出权限范围之外的文件内容挂载到用户容器中。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">my-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">initContainers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">prep-symlink</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">&quot;busybox&quot;</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;bin/sh&quot;</span>, <span class="string">&quot;-ec&quot;</span>, <span class="string">&quot;ln -s / /mnt/data/symlink-door&quot;</span>]</span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">my-volume</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/mnt/data</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">my-container</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">&quot;busybox&quot;</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;/bin/sh&quot;</span>, <span class="string">&quot;-ec&quot;</span>, <span class="string">&quot;ls /mnt/data; sleep 999999&quot;</span>]</span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/mnt/data</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">my-volume</span></span><br><span class="line">      <span class="attr">subPath:</span> <span class="string">symlink-door</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">my-volume</span></span><br><span class="line">  <span class="attr">emptyDir:</span> &#123;&#125;</span><br></pre></td></tr></table></figure>

<p>使用如上的配置，通过 <code>emptyDir</code>，在 <code>initContainer</code> 中在挂载的 <code>Volume</code> 目录中创建了一个指向根目录的软链接。</p>
<p>之后正常的容器启动，但是指定了 <code>subpath</code>，如果 <code>kubelet</code> 直接 <code>Mount</code> 软链接，会将宿主机的根目录 <code>Mount</code> 到用户容器中。</p>
<p>为了解决这个问题，需要解析出软链接对应的真实文件路径，并且判断此路径是否是在 <code>Volume</code> 目录下，校验通过后才能挂载到容器中。但是由于校验和挂载之间存在时间差，此文件还是有可能会被篡改。</p>
<p>社区讨论后，通过引入中间 <code>Bind Mount</code> 的机制，相当于给这个文件加锁，将原文件的路径固化，之后再 <code>Mount</code> 到容器中时，只会 <code>Mount</code> 当时创建挂载点时的源文件。</p>
<h3 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h3><p>给社区提交的修复 <code>PR</code> 已经被合入 <a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubernetes/pull/89629">89629</a> 。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/10/24/share/%E6%80%BB%E7%BB%93%E5%88%86%E4%BA%AB-Wireshark%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/24/share/%E6%80%BB%E7%BB%93%E5%88%86%E4%BA%AB-Wireshark%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">总结分享-Wireshark常用命令总结</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-10-24 21:43:43" itemprop="dateCreated datePublished" datetime="2021-10-24T21:43:43+00:00">2021-10-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:09:04" itemprop="dateModified" datetime="2024-04-14T02:09:04+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/share/" itemprop="url" rel="index"><span itemprop="name">share</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="功能总结"><a href="#功能总结" class="headerlink" title="功能总结"></a>功能总结</h2><h3 id="导出数据包"><a href="#导出数据包" class="headerlink" title="导出数据包"></a>导出数据包</h3><p>方法：File | Export Packet Dissections | as”CSV”(Comma Separated Values packet summary)file…</p>
<blockquote>
<ol>
<li>导出格式有纯文本、CSV、XML、JSON等；</li>
<li>不仅可以输出选中列，还可以<strong>输出当前页面展示的列</strong>，以及所有列（在导出弹框中设置）；</li>
<li>可以设置包格式，比如导出统计行、统计头、包详细数据等；</li>
</ol>
</blockquote>
<h3 id="添加展示列"><a href="#添加展示列" class="headerlink" title="添加展示列"></a><strong>添加展示列</strong></h3><p>方法：Package Details 面板中展开包详情，找到指定字段右键单击并选择Apply as Column 选项</p>
<h3 id="显示一个TCP-UDP-会话"><a href="#显示一个TCP-UDP-会话" class="headerlink" title="显示一个TCP&#x2F;UDP 会话"></a>显示一个TCP&#x2F;UDP 会话</h3><p>方法1：选中一个包，右键选择Conversation Filter|[TCPIUDP]命令</p>
<p>方法2：选中一个包，右键选择Follow[TCPIUDP] Stream 命令</p>
<p>方法3：工具栏选择Statistics|Conversations命令</p>
<p>方法4：在TCP 头部，通过右键单击stream index 字段并选择Apply as Filter 命令</p>
<h2 id="命令总结"><a href="#命令总结" class="headerlink" title="命令总结"></a>命令总结</h2><h3 id="捕获过滤命令"><a href="#捕获过滤命令" class="headerlink" title="捕获过滤命令"></a>捕获过滤命令</h3><p>捕获过滤器仅支持协议过滤。</p>
<h4 id="1-主机相关过滤命令"><a href="#1-主机相关过滤命令" class="headerlink" title="1. 主机相关过滤命令*"></a>1. 主机相关过滤命令*</h4><ul>
<li>host 10.3.1.1: 捕获到达&#x2F;来自10.3.1.1主机的数据（支持IPv6地址）。</li>
<li>not host 10.3.1.1: 捕获除了到达&#x2F;来自10.3.1.1主机的所有数据。</li>
<li>src host 10.3.1.1: 捕获来自10.3.1.1 主机上的数据。</li>
<li>dst host 10.3.1.1: 捕获到达10.3.1.1 主机上的数据。</li>
<li>host 10.3.1.1 or host 10.3.1.2: 捕获到达&#x2F;来自10.3.1.1主机上的数据，和到达&#x2F;来自10.3.1.2 主机的数据。</li>
</ul>
<h4 id="2-端口相关过滤命令"><a href="#2-端口相关过滤命令" class="headerlink" title="2. 端口相关过滤命令*"></a>2. 端口相关过滤命令*</h4><ul>
<li>port 53: 捕获到达&#x2F;来自端口号为53的UDP&#x2F;TCP 数据（典型的DNS 数据）。</li>
<li>not port 53 : 捕获除到达&#x2F;来自端口号为53的所有UDP&#x2F;TCP 数据。</li>
<li>port 80: 捕获到达&#x2F;来自端口号为80的UDP&#x2F;TCP 数据（典型的HTTP 数据）。</li>
<li>udp port 67 : 捕获到达&#x2F;来自端口号为67的UDP 数据（典型的DHCP 数据）。</li>
<li>tcp port 21: 捕获到达&#x2F;来自端口号为21的TCP 数据（典型的FTP 数据）。</li>
<li>portrange 1-80: 捕获到达&#x2F;来自1~80端口号的UDP&#x2F;TCP 数据。</li>
<li>tcp portrange 1-80: 捕获到达&#x2F;来自1~80端口号的TCP 数据。</li>
</ul>
<h4 id="3-主机和端口混合过滤命令"><a href="#3-主机和端口混合过滤命令" class="headerlink" title="3. 主机和端口混合过滤命令*"></a>3. 主机和端口混合过滤命令*</h4><ul>
<li>port 20 or port 21 :捕获到达&#x2F;来自20 或21 端口号的所有UDP&#x2F;TCP 数据。</li>
<li>host 10.3.1.1 and port 80: 捕获到达&#x2F;来自端口号为80, 并且是到达&#x2F;来自10.3.1.1主机的UDP&#x2F;TCP 数据。</li>
<li>host 10.3.1.1 and not port 80: 捕获到I来自10.3.1.1 主机，并且是非80 端口的UDP&#x2F;TCP 数据。</li>
<li>udp src port 68 and udp dst port 67: 捕获来自端口为68, 目标端口号为67 的所有UDP 数据（典型的DHCP 客户端到DHCP 服务器的数据） 。</li>
<li>udp src port 67 and udp dst port 68: 捕获来自端口号为67, 目标端口号为68 的所有UDP 数据（典型的DHCP 服务器到DHCP 客户端的数据）。</li>
</ul>
<h4 id="4-IP地址范围过滤命令"><a href="#4-IP地址范围过滤命令" class="headerlink" title="4. IP地址范围过滤命令"></a>4. IP地址范围过滤命令</h4><ul>
<li>net 192.168.0.0&#x2F;24：捕获到达&#x2F;来自192.168.0.0网络中任何主机的数据。</li>
<li>net 192.168.0.0 mask 255.255.255.0: 捕获到达&#x2F;来自192.168.0.0网络中任何主机的<br>数据。</li>
<li>ip6 net 2406:daOO:ff00::&#x2F;64: 捕获到达&#x2F;来自2406:daOO:ffDO:OOOO ( IPv6) 网络中任<br>何主机的数据。</li>
<li>not dst net 192.168.0.0&#x2F;24: 捕获除目的IP地址是192.168.0.0网络外的所有数据。</li>
<li>dst net 192.168.0.0&#x2F;24：捕获到达IP地址为192.168.0.0网络内的所有数据。</li>
<li>src net 192.168.0.0&#x2F;24: 捕获来自IP地址为192.168.0.0网络内的所有数据。</li>
</ul>
<h4 id="5-广播或多播地址过滤命令"><a href="#5-广播或多播地址过滤命令" class="headerlink" title="5. 广播或多播地址过滤命令"></a>5. 广播或多播地址过滤命令</h4><ul>
<li>ip broadcast: 捕获到255.255.255.255 的数据。</li>
<li>ip multicast: 捕获通过224.0.0.0~239.255.255.255的数据。</li>
<li>dst host ff02::1: 捕获所有主机到IPv6多播地址的数据。</li>
<li>dst host ff02::2: 捕获所有路由到IPv6多播地址的数据。（<em>跟上一个有什么区别？</em>）</li>
</ul>
<h4 id="6-MAC地址过滤命令"><a href="#6-MAC地址过滤命令" class="headerlink" title="6. MAC地址过滤命令"></a>6. MAC地址过滤命令</h4><ul>
<li>ether host 00:08:15:00:08:15: 捕获到达&#x2F;来自00:08:15:00:08:15主机的数据。</li>
<li>ether src 02:0A:42:23:41:AC: 捕获来自02:0A:42:23:41:AC 主机的数据。</li>
<li>ether dst 02:0A:42:23:41:AC: 捕获到达02:0A:42:23:41:AC 主机的数据。</li>
<li>not ether host 00:08:15:00:08:15:捕获到达&#x2F;来自除了00:08:15:00:08:15的任何MAC<br>地址的流量。</li>
</ul>
<h4 id="7-特定ICMP协议过滤命令"><a href="#7-特定ICMP协议过滤命令" class="headerlink" title="7. 特定ICMP协议过滤命令"></a>7. 特定ICMP协议过滤命令</h4><ul>
<li>icmp：捕获所有ICMP 数据包。</li>
<li>icmp[0]&#x3D;8 : 捕获所有ICMP 字段类型为8 (Echo Request) 的数据包。</li>
<li>icmp[0]&#x3D;17: 捕获所有ICMP 字段类型为17 (Address Mask Request) 的数据包。</li>
<li>icmp[0]&#x3D;8 or icmp[0]&#x3D;0: 捕获所有ICMP 字段类型为8 (Echo Request) 或ICMP<br>字段类型为0 (Echo Reply) 的数据包。</li>
<li>icmp[0]&#x3D;3 and not icmp[1]&#x3D;4 ：捕获所有ICMP 字段类型为3 (Destination<br>Unreachable) 的包，除了ICMP 字段类型为3&#x2F;代码为4 (Fragmentation Needed and<br>Don’t Fragment was Set) 的数据包。</li>
</ul>
<h3 id="显示过滤命令"><a href="#显示过滤命令" class="headerlink" title="显示过滤命令"></a>显示过滤命令</h3><p>显示过滤器可以帮助用户在捕捉结果中进行数据查找。该过滤器可以在得到的捕捉结果中修改，以显示有用数据。</p>
<p>既支持协议过滤也支持内容过滤。</p>
<h4 id="1-通用语法格式"><a href="#1-通用语法格式" class="headerlink" title="1. 通用语法格式"></a>1. 通用语法格式</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Protocol | [String1] [String2] | Comparison-Operator | Value | Logical-Operations | Other-expression</span><br><span class="line">协议（2~7层）      协议子类               比较运算符         比较值       逻辑运算符             其他表达式</span><br></pre></td></tr></table></figure>

<p>其中比较运算符有如下6个：</p>
<table>
<thead>
<tr>
<th>英文写法</th>
<th>C 语言写法</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>eq</td>
<td>&#x3D;&#x3D;</td>
<td>等于</td>
</tr>
<tr>
<td>ne</td>
<td>!&#x3D;</td>
<td>不等于</td>
</tr>
<tr>
<td>gt</td>
<td>&gt;</td>
<td>大于</td>
</tr>
<tr>
<td>lt</td>
<td>&lt;</td>
<td>小于</td>
</tr>
<tr>
<td>ge</td>
<td>&gt;&#x3D;</td>
<td>大于等于</td>
</tr>
<tr>
<td>le</td>
<td>&lt;&#x3D;</td>
<td>小于等于</td>
</tr>
<tr>
<td>contains</td>
<td>-</td>
<td>包含</td>
</tr>
<tr>
<td>matches</td>
<td>-</td>
<td>匹配</td>
</tr>
</tbody></table>
<p>逻辑运算符有如下4个：</p>
<table>
<thead>
<tr>
<th>英文写法</th>
<th>C 语言写法</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>and</td>
<td>&amp;&amp;</td>
<td>逻辑与</td>
</tr>
<tr>
<td>or</td>
<td>||</td>
<td>逻辑或</td>
</tr>
<tr>
<td>xor</td>
<td>^^</td>
<td>逻辑异或</td>
</tr>
<tr>
<td>not</td>
<td>!</td>
<td>逻辑非</td>
</tr>
</tbody></table>
<h4 id="2-协议过滤命令"><a href="#2-协议过滤命令" class="headerlink" title="2. 协议过滤命令*"></a>2. 协议过滤命令*</h4><ul>
<li>arp: 显示所有ARP 流量，包括免费ARP 、ARP 请求和ARP 应答。</li>
<li>ip(v6): 显示所有IPv4&#x2F;IPv6 流量，包括有IPv4(IPv6) 头部嵌入式的包（如ICMP 目标不可达的数据包，返回到ICMP 头后进入到IPv4 头部）。<ul>
<li>ip(v6).src</li>
<li>ip(v6).dst</li>
<li>ip(v6).host</li>
<li>ip(v6).addr</li>
</ul>
</li>
<li>tcp: 显示所有基于TCP 的流量数据。</li>
</ul>
<h4 id="3-应用过滤命令"><a href="#3-应用过滤命令" class="headerlink" title="3. 应用过滤命令*"></a>3. 应用过滤命令*</h4><ul>
<li>bootp: 显示所有DHCP 流量（ipv4下基于BOOTP，ipv6下不是基于BOOTP，过滤时使用dhcpv6) 。</li>
<li>dns: 显示所有DNS 流量，包括基于TCP 传输和UDP 的DNS 请求和响应。</li>
<li>tftp: 显示所有TFTP （简单文件传输协议）流量。</li>
<li>http: 显示所有HTTP 命令、响应和数据传输包。但是不显示TCP 握手包、TCP确认包或TCP 断开连接的包。</li>
<li>http contains “GET”: 显示HTTP 客户端发送给HTTP 服务器的所有GET 请求数据。</li>
<li>icmp: 显示所有ICMP 流量。</li>
</ul>
<h4 id="4-字段存在过滤命令"><a href="#4-字段存在过滤命令" class="headerlink" title="4. 字段存在过滤命令"></a>4. 字段存在过滤命令</h4><ul>
<li>bootp.option.hostname: 显示所有DHCP 流量，包含主机名( DHCP 是基于BOOTP) 。</li>
<li>http.host: 显示所有包含有HTTP 主机名字段的HTTP 包。该包通常是由客户端发送给一个Web 服务器的请求。</li>
<li>ftp.request.command: 显示所有FTP 命令数据，如USER 、PASS 或RETR 命令。</li>
<li>ftp.request.arg matches “admin”: 显示匹配admin 字符串的数据。</li>
<li>tcp.analysis.flags: 显示所有与TCP 标识相关的包，包括丢包、重发或者零窗口标志。</li>
<li>tcp.analysis.zero_window: 显示被标志的包，来表示发送方的缓冲空间已满。</li>
</ul>
<h4 id="5-逻辑运算过滤命令"><a href="#5-逻辑运算过滤命令" class="headerlink" title="5. 逻辑运算过滤命令"></a>5. 逻辑运算过滤命令</h4><ul>
<li>&amp;＆或and: ip.src&#x3D;l0.2.2.2 &amp;&amp; tcp.port&#x3D;80，表示显示源地址10.2.2.2 主机，并且端口号为80 的所有IPv4 流量。</li>
<li>||或or: tcp.port&#x3D;80 || tcp.port&#x3D;43，表示显示到达&#x2F;来自80 或443 端口的所有TCP数据。</li>
<li>！或not: !arp，表示查看除ARP 外的所有数据。</li>
<li>!＝或ne: tcp.flags.syn !&#x3D; 1，表示查看TCP SYN 标志位不是1 的TCP 数据帧。</li>
</ul>
<blockquote>
<p>注：</p>
<p>ip.addr !&#x3D; 10.2.2.2  表示显示IP 源或目标地址字段非10.2.2.2 的数据包。如果一个包的源或目标IP 地址字段中不包含10.2.2.2, 则显示该数据包。在该语法中使用了一个隐含或，并且不会过滤掉任何数据包。</p>
<p>!ip.addr &#x3D;&#x3D; 10.2.2.2 表示显示在IP 源和目标地址字段不包含10.2.2.2 的数据包。当排除到达&#x2F;来自一个特定IP 地址的数据时，这是一个合适的过滤器语法。</p>
<p>!tcp.flags.syn&#x3D;&#x3D;l 表示显示TCP SYN 标志位不等于1的所有TCP 包和其他协议包，如UDP 、ARP数据包将匹配该过滤器。因为UDP 和ARP 协议中没有TCP SYN 标志位为1 的数据包。</p>
<p>tcp.flags.syn !&#x3D; 1 表示仅显示包括SYN 设置为0 的TCP 包。</p>
</blockquote>
<h4 id="6-时间过滤命令"><a href="#6-时间过滤命令" class="headerlink" title="6. 时间过滤命令"></a>6. 时间过滤命令</h4><ul>
<li>frame.time_delta &gt; 1，表示时间延迟超过1 秒的数据，显示捕获文件中所有包的时间。</li>
<li>tcp.time_delta &gt; 1，表示TCP 时间差大于1 秒的数据。</li>
</ul>
<blockquote>
<p>注：上述命令主要用于判断各种网络延迟。</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/09/26/linux/%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-IPv6%E7%8E%AF%E5%A2%83%E4%B8%8BVIP%E5%9C%B0%E5%9D%80%E4%B8%8D%E9%80%9A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/26/linux/%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-IPv6%E7%8E%AF%E5%A2%83%E4%B8%8BVIP%E5%9C%B0%E5%9D%80%E4%B8%8D%E9%80%9A/" class="post-title-link" itemprop="url">网络问题排查-IPv6环境下VIP地址不通</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-09-26 09:15:21" itemprop="dateCreated datePublished" datetime="2021-09-26T09:15:21+00:00">2021-09-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:09:04" itemprop="dateModified" datetime="2024-04-14T02:09:04+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/linux/" itemprop="url" rel="index"><span itemprop="name">linux</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>IPv6环境下，在浏览器中通过<code>http://[vip:port]</code>访问<code>web</code>业务，提示无法访问此网站，<code>[vip]</code>的响应时间过长。</p>
<h2 id="分析过程"><a href="#分析过程" class="headerlink" title="分析过程"></a>分析过程</h2><p>之前碰到过多次在PC浏览器上无法访问<code>vip</code>的情况，排查方法也很明确：</p>
<ol>
<li>在集群的<code>vip</code>所在节点上访问是否正常；</li>
<li>在集群范围内其他节点上访问是否正常；</li>
<li>在集群之外的同网段<code>linux</code>环境上访问是否正常；</li>
<li>在其他环境的PC浏览器上访问是否正常；</li>
</ol>
<p>验证发现，直接在<code>vip</code>所在节点上访问竟然不通！登录<code>vip</code>所在节点执行<code>ip addr</code>可以看到该地址确实是正确配置了，但 <code>ping6</code>该地址无回应，对应的<code>ipv4</code>地址 <code>ping</code>有回应。按说<code>ping</code>本机的地址不应该和链路的状态有关系，那会是什么原因呢？在仔细检查地址配置情况后发现该地址有个标记<code>tentative dadfailed </code>；</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">17: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 0c:da:41:1d:a8:62 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.10.10.17/16 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 2000::10:18/128 scope global tentative dadfailed</span><br><span class="line">       valid_lft forever preferred_lft 0sec</span><br><span class="line">    inet6 fe80::eda:41ff:fe1d:a862/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>

<p><em><a target="_blank" rel="noopener" href="https://manpages.debian.org/ip-address(8)">ip-address(8)</a></em> 查到对该标记的解释如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tentative</span><br><span class="line">   (IPv6 only) only list addresses which have not yet passed duplicate address detection.</span><br></pre></td></tr></table></figure>

<p>显然该地址没有通过地址重复探测（<code>duplicate address detection</code>，简称<code>dad</code>），而且这种检查机制只针对<code>IPv6</code>。<strong>经确认，该环境的<code>IPv6</code>网段只有自己在用，且未手工配置过<code>IPv6</code>地址，但该环境曾经发生过切主</strong>；</p>
<p>至此问题基本明确了，切主时会把老的主节点上的<code>vip</code>删除，再到新的主节点上把<code>vip</code>添加上去。如果一切正常，按照这个顺序切主没有问题，但也存在某些异常情况（比如老主上的<code>vip</code>没有及时删掉，而新主上已经添加好了），此时就会触发<code>dad</code>机制。经过验证，一旦出现<code>dadfailed</code>，即使地址冲突解决了，该地址依然无法访问；</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>方案1：在<code>sysctl</code>配置中增加如下内核参数：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net.ipv6.conf.all.accept_dad = 0</span><br><span class="line">net.ipv6.conf.default.accept_dad = 0</span><br><span class="line">net.ipv6.conf.eth0.accept_dad = 0</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">IPv6 Privacy Extensions (RFC 4941)</span></span><br><span class="line">net.ipv6.conf.all.use_tempaddr = 0</span><br><span class="line">net.ipv6.conf.default.use_tempaddr = 0</span><br></pre></td></tr></table></figure>

<p>方案2：在<code>ip addr add</code>命令执行时增加<code>nodad</code>标识：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip addr add 2000::10:18/128 dev eth0 nodad</span><br></pre></td></tr></table></figure>

<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://blog.clanzx.net/network/ipv6-dad.html">https://blog.clanzx.net/network/ipv6-dad.html</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/09/25/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E9%99%84%E5%8A%A0%E7%BD%91%E7%BB%9CPod%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/25/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E9%99%84%E5%8A%A0%E7%BD%91%E7%BB%9CPod%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8/" class="post-title-link" itemprop="url">K8S问题排查-附加网络Pod无法启动</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-09-25 19:21:50" itemprop="dateCreated datePublished" datetime="2021-09-25T19:21:50+00:00">2021-09-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:09:04" itemprop="dateModified" datetime="2024-04-14T02:09:04+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>使用附加网络的Pod在服务器重启后启动异常，报错信息如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Events:</span><br><span class="line">Type 		Reason 				  Age 		From 			Message</span><br><span class="line">Normal 		Scheduled 			  53m 		default-scheduler Successfully assigned xxx/xxx1-64784c458b-q67tx to node001</span><br><span class="line">Warning 	FailedCreatePodSandBox 53m 		 kubelet, node001   Failed to create pod sandbox: rpc er or: code = Unknown desc = failed to set up sandbox container &quot;xxx&quot; network for pod &quot;xxxl-64784c458b-q67tx&quot;: NetworkPlugin cni failed to set up pod &quot;xxx1-64784c458b-q67tx_xxx&quot; network: Multus: Err adding pod to network &quot;net-netl-nodeOOl&quot;: Multus: error in invoke Delegate add - &quot;macvlan&quot;: failed to create macvlan: device or resource busy</span><br><span class="line">Warning 	FailedCreatePodSandBox 53m 		 kubelet, node001   Failed to create pod sandbox: rpc er or: code = Unknown desc = failed to set up sandbox container &quot;xxx&quot; network for pod &quot;xxxl-64784c458b-q67tx&quot;: NetworkPlugin cni failed to set up pod &quot;xxx1-64784c458b-q67tx_xxx&quot; network: Multus: Err adding pod to network &quot;net-netl-nodeOOl&quot;: Multus: error in invoke Delegate add - &quot;macvlan&quot;: failed to create macvlan: device or resource busy</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h2 id="分析过程"><a href="#分析过程" class="headerlink" title="分析过程"></a>分析过程</h2><p>从日志初步看，创建Pod的<code>sandbox</code>异常，具体是Multus无法将Pod添加到<code>net-netl-nodeOOl</code>网络命名空间内，再具体点是Multus无法创建<code>macvlan</code>网络，原因是<code>device or resource busy</code>；</p>
<p>最后的这个错误信息还是比较常见的，从字面理解，就是设备或资源忙，常见于共享存储的卸载场景。那这里也应该类似，有什么设备或资源处于被占用状态，所以执行<code>macvlan</code>的创建失败，既然是附加网络的问题，那优先查看了下附加网络相关的CRD资源，没什么异常；</p>
<p>网上根据日志搜索一番，也没有什么比较相关的问题，那就看代码吧，首先找到Multus的源码，根据上述日志找相关处理逻辑，没有找到。再一想，Multus实现<code>macvlan</code>网络使用的是<code>macvlan</code>插件，再下载插件代码，找到了相关处理逻辑：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">plugins/main/macvlan/macvlan.<span class="keyword">go</span>:<span class="number">169</span></span><br><span class="line"><span class="keyword">if</span> err := netlink.LinkAdd(mv); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">&quot;failed to create macvlan: %v&quot;</span>, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// LinkAdd adds a new link device. The type and features of the device</span></span><br><span class="line"><span class="comment">// are taken from the parameters in the link object.</span></span><br><span class="line"><span class="comment">// Equivalent to: `ip link add $link`</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">LinkAdd</span><span class="params">(link Link)</span></span> <span class="type">error</span> &#123;</span><br><span class="line">	<span class="keyword">return</span> pkgHandle.LinkAdd(link)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// LinkAdd adds a new link device. The type and features of the device</span></span><br><span class="line"><span class="comment">// are taken from the parameters in the link object.</span></span><br><span class="line"><span class="comment">// Equivalent to: `ip link add $link`</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h *Handle)</span></span> LinkAdd(link Link) <span class="type">error</span> &#123;</span><br><span class="line">	<span class="keyword">return</span> h.linkModify(link, unix.NLM_F_CREATE|unix.NLM_F_EXCL|unix.NLM_F_ACK)</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>根据上述代码和注释简单的看，是在执行<code>ip link add $link</code>命令时报错，实际验证看看：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node001 ~] ip link add link bond1 name macvlan1 type macvlan mode bridge</span><br><span class="line">RTNETLINK answers: Device or resource busy</span><br></pre></td></tr></table></figure>

<p>确实如此，在<code>bond1</code>接口上无法配置<code>macvlan</code>，那换一个接口试试：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node001 ~] ip link add link bond0 name macvlan1 type macvlan mode bridge</span><br><span class="line">[root@node001 ~] ip link show</span><br><span class="line">...</span><br><span class="line">110: macvlan1@bond0: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether ea:31:c9:7f:d9:a4 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>配置成功，说明<code>bond1</code>接口有什么问题，看看这俩接口有没有差异：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@node001 ~] ip addr show</span><br><span class="line">...</span><br><span class="line">2: bond0: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000</span><br><span class="line">    link/ether 0c:da:41:1d:6f:ca brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet x.x.x.x/16 brd x.x.255.255 scope global bond0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::eda:41ff:fe1d:6fca/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">...</span><br><span class="line">17: bond1: &lt;BROADCAST,MULTICAST,MASTER,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000</span><br><span class="line">    link/ether 0c:da:41:1d:a8:62 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>对比两个接口可以发现两个差异点：</p>
<ol>
<li><code>bond0</code>配置了IP地址，而<code>bond1</code>没有配置；</li>
<li><code>bond0</code>是MASTER角色，<code>bond1</code>既是MASTER，又是SLAVE角色；</li>
</ol>
<p>考虑到<code>bond0</code>接口是用来建集群的，<code>bond1</code>接口是给<code>Multus</code>创建<code>macvlan</code>网络用的，所以第一个差异点属于正常现象。第二个是什么情况呢？一般来说，配置<code>bond</code>的目的是把几个物理接口作为SLAVE角色聚合成<code>bond</code>接口，这样既能增加服务器的可靠性，又增加了可用网络宽带，为用户提供不间断的网络服务。配置后，实际的物理接口应该是SLAVE角色，而聚合后的<code>bond</code>接口应该是MASTER角色，所以正常来说，不会同时出现两个角色才对；</p>
<p>查看两个<code>bond</code>的相关配置，没有发现什么异常，反过来讲，如果配置的有问题，那初次部署就应该报错了，而不是重启节点才发现。所以，<strong>问题的关键是重启导致的</strong>。也就是说，可能是在重启后的启动脚本里加了什么配置影响的；</p>
<p>搜索相关资料[1]，发现在配置过程中可能有这么一个操作：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">4、在/etc/rc.d/rc.local文件中加入如下语句，使系统启动自动运行</span><br><span class="line">ifenslave  bond0  eth0  eth1</span><br></pre></td></tr></table></figure>

<p>查看问题环境上怎么配置的：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node001 ~] cat /etc/rc.local</span><br><span class="line">...</span><br><span class="line">touch /var/lock/subsys/local</span><br><span class="line">ifenslave bond0 bond1 enp661s0f0 enp661s0f1 ens1f0 ens1f1</span><br></pre></td></tr></table></figure>

<p>发现有类似的配置，但不同的是，问题环境上配置了两个<code>bond</code>，并且配置在了一个命令里。感觉不是太对，个人理解这么配置应该会把<code>bond1</code>也认为是<code>bond0</code>的SLAVE，修改一下试试：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node001 ~] cat /etc/rc.local</span><br><span class="line">...</span><br><span class="line">touch /var/lock/subsys/local</span><br><span class="line">ifenslave bond0 enp661s0f0 enp661s0f1</span><br><span class="line">ifenslave bond1 ens1f0 ens1f1</span><br><span class="line">[root@node001 ~] systemctl restart network</span><br></pre></td></tr></table></figure>

<p>再观察两个bond接口的角色，发现恢复正常，再看看异常Pod，也都起来了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node001 ~] kubectl get pod -A |grep -v Running</span><br><span class="line">NAMESPACE		NAME		READY		STATUS		RESTARTS		AGE</span><br></pre></td></tr></table></figure>

<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>将<code>rc.local</code>里的两个<code>bond</code>的命令拆开分别配置即可。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/geaozhang/p/6763876.html">https://www.cnblogs.com/geaozhang/p/6763876.html</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/08/14/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E5%88%A0%E9%99%A4Pod%E5%90%8E%E5%A4%84%E4%BA%8ETerminating%E7%8A%B6%E6%80%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/14/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E5%88%A0%E9%99%A4Pod%E5%90%8E%E5%A4%84%E4%BA%8ETerminating%E7%8A%B6%E6%80%81/" class="post-title-link" itemprop="url">K8S问题排查-删除Pod后处于Terminating状态</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-08-14 17:20:51" itemprop="dateCreated datePublished" datetime="2021-08-14T17:20:51+00:00">2021-08-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:09:04" itemprop="dateModified" datetime="2024-04-14T02:09:04+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>11 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>通过<code>kubectl delete</code>命令删除某个业务Pod后，该Pod一直处于<code>Terminating</code>状态。</p>
<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>根据现象看，应该是删除过程中有哪个流程异常，导致最终的删除卡在了<code>Terminating</code>状态。先<code>describe</code>看一下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl describe pod -n xxx cam1-78b6fc6bc8-cjsw5</span><br><span class="line">// 没有发现什么异常信息，这里就不贴日志了</span><br></pre></td></tr></table></figure>

<p><code>Event</code>事件中未见明显异常，那就看负责删除Pod的<code>kubelet</code>组件日志（已过滤出关键性日志）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">I0728 16:24:57.339295    9744 kubelet.go:1904] SyncLoop (DELETE, &quot;api&quot;): &quot;cam1-78b6fc6bc8-cjsw5_cam(5c948341-c030-4996-b888-f032577d97b0)&quot;</span><br><span class="line">I0728 16:24:57.339720    9744 kuberuntime_container.go:581] Killing container &quot;docker://a73082a4a9a4cec174bb0d1c256cc11d804d93137551b9bfd3e6fa1522e98589&quot; with 60 second grace period</span><br><span class="line">I0728 16:25:18.259418    9744 kubelet.go:1904] SyncLoop (DELETE, &quot;api&quot;): &quot;cam1-78b6fc6bc8-cjsw5_cam(5c948341-c030-4996-b888-f032577d97b0)&quot;</span><br><span class="line">2021-07-28 16:25:19.247 [INFO][394011] ipam.go 1173: Releasing all IPs with handle &#x27;cam.cam1-78b6fc6bc8-cjsw5&#x27;</span><br><span class="line">2021-07-28 16:25:19.254 [INFO][393585] k8s.go 498: Teardown processing complete.</span><br><span class="line"></span><br><span class="line">// 可疑点1：没有获取到pod IP</span><br><span class="line">W0728 16:25:19.303513    9744 docker_sandbox.go:384] failed to read pod IP from plugin/docker: NetworkPlugin cni failed on the status hook for pod &quot;cam1-78b6fc6bc8-cjsw5_cam&quot;: Unexpected command output Device &quot;eth0&quot; does not exist.</span><br><span class="line"> with error: exit status 1</span><br><span class="line"> </span><br><span class="line">I0728 16:25:19.341068    9744 kubelet.go:1933] SyncLoop (PLEG): &quot;cam1-78b6fc6bc8-cjsw5_cam(5c948341-c030-4996-b888-f032577d97b0)&quot;, event: &amp;pleg.PodLifecycleEvent&#123;ID:&quot;5c948341-c030-4996-b888-f032577d97b0&quot;, Type:&quot;ContainerDied&quot;, Data:&quot;a73082a4a9a4cec174bb0d1c256cc11d804d93137551b9bfd3e6fa1522e98589&quot;&#125;</span><br><span class="line">I0728 16:25:20.578095    9744 kubelet.go:1933] SyncLoop (PLEG): &quot;cam1-78b6fc6bc8-cjsw5_cam(5c948341-c030-4996-b888-f032577d97b0)&quot;, event: &amp;pleg.PodLifecycleEvent&#123;ID:&quot;5c948341-c030-4996-b888-f032577d97b0&quot;, Type:&quot;ContainerDied&quot;, Data:&quot;c3b992465cd2085300995066526a36665664558446ff6e1756135c3a5b6df2e6&quot;&#125;</span><br><span class="line"></span><br><span class="line">I0728 16:25:20.711967    9744 kubelet_pods.go:1090] Killing unwanted pod &quot;cam1-78b6fc6bc8-cjsw5&quot;</span><br><span class="line"></span><br><span class="line">// 可疑点2：Unmount失败</span><br><span class="line">E0728 16:25:20.939400    9744 nestedpendingoperations.go:301] Operation for &quot;&#123;volumeName:kubernetes.io/glusterfs/5c948341-c030-4996-b888-f032577d97b0-cam-pv-50g podName:5c948341-c030-4996-b888-f032577d97b0 nodeName:&#125;&quot; failed. No retries permitted until 2021-07-28 16:25:21.439325811 +0800 CST m=+199182.605079651 (durationBeforeRetry 500ms). Error: &quot;UnmountVolume.TearDown failed for volume \&quot;diag-log\&quot; (UniqueName: \&quot;kubernetes.io/glusterfs/5c948341-c030-4996-b888-f032577d97b0-cam-pv-50g\&quot;) pod \&quot;5c948341-c030-4996-b888-f032577d97b0\&quot; (UID: \&quot;5c948341-c030-4996-b888-f032577d97b0\&quot;) : Unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/5c948341-c030-4996-b888-f032577d97b0/volumes/kubernetes.io~glusterfs/cam-pv-50g\nOutput: umount: /var/lib/kubelet/pods/5c948341-c030-4996-b888-f032577d97b0/volumes/kubernetes.io~glusterfs/cam-pv-50g：目标忙。\n        (有些情况下通过 lsof(8) 或 fuser(1) 可以\n         找到有关使用该设备的进程的有用信息。)\n\n&quot;</span><br></pre></td></tr></table></figure>

<p>从删除Pod的日志看，有2个可疑点：</p>
<ol>
<li><code>docker_sandbox.go:384</code>打印的获取<code>pod IP</code>错误；</li>
<li><code>nestedpendingoperations.go:301</code>打印的<code>Unmount</code>失败错误；</li>
</ol>
<p>先看第1点，根据日志定位到代码[1]位置如下，<code>IP</code>没有拿到所以打印了个告警并返回空<code>IP</code>地址；</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">pkg/kubelet/dockershim/docker_sandbox.<span class="keyword">go</span>:<span class="number">348</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ds *dockerService)</span></span> getIP(podSandboxID <span class="type">string</span>, sandbox *dockertypes.ContainerJSON) <span class="type">string</span> &#123;</span><br><span class="line">	<span class="keyword">if</span> sandbox.NetworkSettings == <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> networkNamespaceMode(sandbox) == runtimeapi.NamespaceMode_NODE &#123;</span><br><span class="line">		<span class="comment">// For sandboxes using host network, the shim is not responsible for</span></span><br><span class="line">		<span class="comment">// reporting the IP.</span></span><br><span class="line">		<span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Don&#x27;t bother getting IP if the pod is known and networking isn&#x27;t ready</span></span><br><span class="line">	ready, ok := ds.getNetworkReady(podSandboxID)</span><br><span class="line">	<span class="keyword">if</span> ok &amp;&amp; !ready &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	ip, err := ds.getIPFromPlugin(sandbox)</span><br><span class="line">	<span class="keyword">if</span> err == <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> ip</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">if</span> sandbox.NetworkSettings.IPAddress != <span class="string">&quot;&quot;</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> sandbox.NetworkSettings.IPAddress</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> sandbox.NetworkSettings.GlobalIPv6Address != <span class="string">&quot;&quot;</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> sandbox.NetworkSettings.GlobalIPv6Address</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 错误日志在这里</span></span><br><span class="line">	klog.Warningf(<span class="string">&quot;failed to read pod IP from plugin/docker: %v&quot;</span>, err)</span><br><span class="line">	<span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>继续看<code>getIP</code>方法的调用处代码，这里如果没有拿到<code>IP</code>，也没有什么异常，直接把空值放到<code>PodSandboxStatusResponse</code>中并返回；</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">pkg/kubelet/dockershim/docker_sandbox.<span class="keyword">go</span>:<span class="number">404</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ds *dockerService)</span></span> PodSandboxStatus(ctx context.Context, req *runtimeapi.PodSandboxStatusRequest) (*runtimeapi.PodSandboxStatusResponse, <span class="type">error</span>) &#123;</span><br><span class="line">	podSandboxID := req.PodSandboxId</span><br><span class="line"></span><br><span class="line">	r, metadata, err := ds.getPodSandboxDetails(podSandboxID)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Parse the timestamps.</span></span><br><span class="line">	createdAt, _, _, err := getContainerTimestamps(r)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">&quot;failed to parse timestamp for container %q: %v&quot;</span>, podSandboxID, err)</span><br><span class="line">	&#125;</span><br><span class="line">	ct := createdAt.UnixNano()</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Translate container to sandbox state.</span></span><br><span class="line">	state := runtimeapi.PodSandboxState_SANDBOX_NOTREADY</span><br><span class="line">	<span class="keyword">if</span> r.State.Running &#123;</span><br><span class="line">		state = runtimeapi.PodSandboxState_SANDBOX_READY</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 调用getIP方法的位置</span></span><br><span class="line">	<span class="keyword">var</span> IP <span class="type">string</span></span><br><span class="line">	<span class="keyword">if</span> IP = ds.determinePodIPBySandboxID(podSandboxID); IP == <span class="string">&quot;&quot;</span> &#123;</span><br><span class="line">		IP = ds.getIP(podSandboxID, r)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">    labels, annotations := extractLabels(r.Config.Labels)</span><br><span class="line">	status := &amp;runtimeapi.PodSandboxStatus&#123;</span><br><span class="line">		Id:          r.ID,</span><br><span class="line">		State:       state,</span><br><span class="line">		CreatedAt:   ct,</span><br><span class="line">		Metadata:    metadata,</span><br><span class="line">		Labels:      labels,</span><br><span class="line">		Annotations: annotations,</span><br><span class="line">		Network: &amp;runtimeapi.PodSandboxNetworkStatus&#123;</span><br><span class="line">			Ip: IP,</span><br><span class="line">		&#125;,</span><br><span class="line">		Linux: &amp;runtimeapi.LinuxPodSandboxStatus&#123;</span><br><span class="line">			Namespaces: &amp;runtimeapi.Namespace&#123;</span><br><span class="line">				Options: &amp;runtimeapi.NamespaceOption&#123;</span><br><span class="line">					Network: networkNamespaceMode(r),</span><br><span class="line">					Pid:     pidNamespaceMode(r),</span><br><span class="line">					Ipc:     ipcNamespaceMode(r),</span><br><span class="line">				&#125;,</span><br><span class="line">			&#125;,</span><br><span class="line">		&#125;,</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> &amp;runtimeapi.PodSandboxStatusResponse&#123;Status: status&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>到此看不出这个错误会不会中断删除流程，那就本地构造一下试试。修改上面的代码，在调用<code>getIP</code>方法的位置后面增加调试日志（从本地验证结果看，Pod正常删除，说明异常问题与此处无关）；</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 调用getIP方法的位置</span></span><br><span class="line"><span class="keyword">var</span> IP <span class="type">string</span></span><br><span class="line"><span class="keyword">if</span> IP = ds.determinePodIPBySandboxID(podSandboxID); IP == <span class="string">&quot;&quot;</span> &#123;</span><br><span class="line">	IP = ds.getIP(podSandboxID, r)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 新加调试日志，如果是指定的Pod，强制将IP置空</span></span><br><span class="line">isTestPod := strings.Contains(metadata.GetName(), <span class="string">&quot;testpod&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> isTestPod &#123;</span><br><span class="line">	IP = <span class="string">&quot;&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>再看第2点，这个是<code>ERROR</code>级别的错误，问题出在<code>Unmount</code>挂载点时失败。那么卸载挂载点失败会导致卸载流程提前终止吗？网上关于Pod删除流程的源码分析文章很多，我们就直接找几篇[2,3,4]看看能不能解答上面的问题。</p>
<p><strong>简单总结来说，删除一个Pod的流程如下：</strong></p>
<ol>
<li>调用<code>kube-apiserver</code>的<code>DELETE</code>接口（默认带<code>grace-period=30s</code>）；</li>
<li>第一次的删除只是更新Pod对象的元信息（<code>DeletionTimestamp</code>字段和<code>DeletionGracePeriodSeconds</code>字段），并没有在<code>Etcd</code>中删除记录；</li>
<li><code>kubectl</code>命令的执行会阻塞并显示正在删除Pod；</li>
<li><code>kubelet</code>组件监听到Pod对象的更新事件，执行<code>killPod()</code>方法；</li>
<li><code>kubelet</code>组件监听到pod的删除事件，第二次调用<code>kube-apiserver</code>的<code>DELETE</code>接口（带<code>grace-period=0</code>）</li>
<li><code>kube-apiserver</code>的<code>DELETE</code>接口去<code>etcd</code>中删除Pod对象；</li>
<li><code>kubectl</code>命令的执行返回，删除Pod成功；</li>
</ol>
<p>从前面<code>kubelet</code>删除异常的日志看，确实有两次<code>DELETE</code>操作，并且中间有个<code>Killing container</code>的日志，但从上面的删除流程看，两次<code>DELETE</code>操作之间应该是调用<code>killPod()</code>方法，通过查看源码，对应的日志应该是<code>Killing unwanted pod</code>，所以，实际上第二次的<code>DELETE</code>操作并没有触发。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">pkg/kubelet/kubelet_pods.<span class="keyword">go</span>:<span class="number">1073</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kl *Kubelet)</span></span> podKiller() &#123;</span><br><span class="line">	killing := sets.NewString()</span><br><span class="line">	<span class="comment">// guard for the killing set</span></span><br><span class="line">	lock := sync.Mutex&#123;&#125;</span><br><span class="line">	<span class="keyword">for</span> podPair := <span class="keyword">range</span> kl.podKillingCh &#123;</span><br><span class="line">		runningPod := podPair.RunningPod</span><br><span class="line">		apiPod := podPair.APIPod</span><br><span class="line"></span><br><span class="line">		lock.Lock()</span><br><span class="line">		exists := killing.Has(<span class="type">string</span>(runningPod.ID))</span><br><span class="line">		<span class="keyword">if</span> !exists &#123;</span><br><span class="line">			killing.Insert(<span class="type">string</span>(runningPod.ID))</span><br><span class="line">		&#125;</span><br><span class="line">		lock.Unlock()</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 这里在调用killPod方法前会打印v2级别的日志</span></span><br><span class="line">		<span class="keyword">if</span> !exists &#123;</span><br><span class="line">			<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(apiPod *v1.Pod, runningPod *kubecontainer.Pod)</span></span> &#123;</span><br><span class="line">				klog.V(<span class="number">2</span>).Infof(<span class="string">&quot;Killing unwanted pod %q&quot;</span>, runningPod.Name)</span><br><span class="line">				err := kl.killPod(apiPod, runningPod, <span class="literal">nil</span>, <span class="literal">nil</span>)</span><br><span class="line">				<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">					klog.Errorf(<span class="string">&quot;Failed killing the pod %q: %v&quot;</span>, runningPod.Name, err)</span><br><span class="line">				&#125;</span><br><span class="line">				lock.Lock()</span><br><span class="line">				killing.Delete(<span class="type">string</span>(runningPod.ID))</span><br><span class="line">				lock.Unlock()</span><br><span class="line">			&#125;(apiPod, runningPod)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>怎么确认第二次的<code>DELETE</code>操作有没有触发呢？很简单，看代码或者实际验证都可以。这里我就在测试环境删除个Pod看下相关日志：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@node2 ~]# kubectl delete pod -n xxx  testpodrc2-7b749f6c9c-qh68l</span><br><span class="line">pod &quot;testpodrc2-7b749f6c9c-qh68l&quot; deleted</span><br><span class="line"></span><br><span class="line">// 已过滤出关键性日志</span><br><span class="line">[root@node2 ~]# tailf kubelet.log</span><br><span class="line">I0730 13:27:31.854178   24588 kubelet.go:1904] SyncLoop (DELETE, &quot;api&quot;): &quot;testpodrc2-7b749f6c9c-qh68l_testpod(85ee282f-a843-4f10-a99c-79d447f83f2a)&quot;</span><br><span class="line">I0730 13:27:31.854511   24588 kuberuntime_container.go:581] Killing container &quot;docker://e2a1cd5f2165e12cf0b46e12f9cd4d656d593f75e85c0de058e0a2f376a5557e&quot; with 30 second grace period</span><br><span class="line">I0730 13:27:32.203167   24588 kubelet.go:1904] SyncLoop (DELETE, &quot;api&quot;): &quot;testpodrc2-7b749f6c9c-qh68l_testpod(85ee282f-a843-4f10-a99c-79d447f83f2a)&quot;</span><br><span class="line"></span><br><span class="line">I0730 13:27:32.993294   24588 kubelet.go:1933] SyncLoop (PLEG): &quot;testpodrc2-7b749f6c9c-qh68l_testpod(85ee282f-a843-4f10-a99c-79d447f83f2a)&quot;, event: &amp;pleg.PodLifecycleEvent&#123;ID:&quot;85ee282f-a843-4f10-a99c-79d447f83f2a&quot;, Type:&quot;ContainerDied&quot;, Data:&quot;e2a1cd5f2165e12cf0b46e12f9cd4d656d593f75e85c0de058e0a2f376a5557e&quot;&#125;</span><br><span class="line">I0730 13:27:32.993428   24588 kubelet.go:1933] SyncLoop (PLEG): &quot;testpodrc2-7b749f6c9c-qh68l_testpod(85ee282f-a843-4f10-a99c-79d447f83f2a)&quot;, event: &amp;pleg.PodLifecycleEvent&#123;ID:&quot;85ee282f-a843-4f10-a99c-79d447f83f2a&quot;, Type:&quot;ContainerDied&quot;, Data:&quot;c6a587614976beed0cbb6e5fabf70a2d039eec6c160154fce007fe2bb1ba3b4f&quot;&#125;</span><br><span class="line"></span><br><span class="line">I0730 13:27:34.072494   24588 kubelet_pods.go:1090] Killing unwanted pod &quot;testpodrc2-7b749f6c9c-qh68l&quot;</span><br><span class="line"></span><br><span class="line">I0730 13:27:40.084182   24588 kubelet.go:1904] SyncLoop (DELETE, &quot;api&quot;): &quot;testpodrc2-7b749f6c9c-qh68l_testpod(85ee282f-a843-4f10-a99c-79d447f83f2a)&quot;</span><br><span class="line">I0730 13:27:40.085735   24588 kubelet.go:1898] SyncLoop (REMOVE, &quot;api&quot;): &quot;testpodrc2-7b749f6c9c-qh68l_testpod(85ee282f-a843-4f10-a99c-79d447f83f2a)&quot;</span><br></pre></td></tr></table></figure>

<p>对比正常和异常场景下的日志可以看出，正常的删除操作下，<code>Killing unwanted pod</code>日志之后会有<code>DELETE</code>和<code>REMOVE</code>的操作，这也就说明问题出在第二次<code>DELETE</code>操作没有触发。查看相关代码：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">pkg/kubelet/status/status_manager.<span class="keyword">go</span>:<span class="number">470</span></span><br><span class="line"><span class="comment">//kubelet组件有一个statusManager模块，它会for循环调用syncPod()方法</span></span><br><span class="line"><span class="comment">//方法内部有机会调用kube-apiserver的DELETE接口(强制删除，非平滑)</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *manager)</span></span> syncPod(uid types.UID, status versionedPodStatus) &#123;</span><br><span class="line">	...</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//当pod带有DeletionTimestamp字段，并且其内容器已被删除、持久卷已被删除等的多条件下，才会进入if语句内部</span></span><br><span class="line">    <span class="keyword">if</span> m.canBeDeleted(pod, status.status) &#123;</span><br><span class="line">        deleteOptions := metav1.NewDeleteOptions(<span class="number">0</span>)</span><br><span class="line">        deleteOptions.Preconditions = metav1.NewUIDPreconditions(<span class="type">string</span>(pod.UID))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//强制删除pod对象：kubectl delete pod podA --grace-period=0</span></span><br><span class="line">        err = m.kubeClient.CoreV1().Pods(pod.Namespace).Delete(pod.Name, deleteOptions) </span><br><span class="line">	...</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从源码可以看出，第二次<code>DELETE</code>操作是否触发依赖于<code>canBeDeleted</code>方法的校验结果，而这个方法内会检查持久卷是否已经被删除：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">pkg/kubelet/status/status_manager.<span class="keyword">go</span>:<span class="number">538</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *manager)</span></span> canBeDeleted(pod *v1.Pod, status v1.PodStatus) <span class="type">bool</span> &#123;</span><br><span class="line">	<span class="keyword">if</span> pod.DeletionTimestamp == <span class="literal">nil</span> || kubepod.IsMirrorPod(pod) &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> m.podDeletionSafety.PodResourcesAreReclaimed(pod, status)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pkg/kubelet/kubelet_pods.<span class="keyword">go</span>:<span class="number">900</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kl *Kubelet)</span></span> PodResourcesAreReclaimed(pod *v1.Pod, status v1.PodStatus) <span class="type">bool</span> &#123;</span><br><span class="line">	...</span><br><span class="line">    </span><br><span class="line">	<span class="comment">// 这里会判断挂载卷是否已卸载</span></span><br><span class="line">	<span class="keyword">if</span> kl.podVolumesExist(pod.UID) &amp;&amp; !kl.keepTerminatedPodVolumes &#123;</span><br><span class="line">		<span class="comment">// We shouldnt delete pods whose volumes have not been cleaned up if we are not keeping terminated pod volumes</span></span><br><span class="line">		klog.V(<span class="number">3</span>).Infof(<span class="string">&quot;Pod %q is terminated, but some volumes have not been cleaned up&quot;</span>, format.Pod(pod))</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> kl.kubeletConfiguration.CgroupsPerQOS &#123;</span><br><span class="line">		pcm := kl.containerManager.NewPodContainerManager()</span><br><span class="line">		<span class="keyword">if</span> pcm.Exists(pod) &#123;</span><br><span class="line">			klog.V(<span class="number">3</span>).Infof(<span class="string">&quot;Pod %q is terminated, but pod cgroup sandbox has not been cleaned up&quot;</span>, format.Pod(pod))</span><br><span class="line">			<span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结合出问题的日志，基本能确认是<code>Unmount</code>挂载点失败导致的异常。那么，挂载点为啥会<code>Unmount</code>失败？</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// umount失败关键日志</span><br><span class="line">Unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/xxx/volumes/kubernetes.io~glusterfs/cam-pv-50g\nOutput: umount: /var/lib/kubelet/pods/xxx/volumes/kubernetes.io~glusterfs/cam-pv-50g：目标忙。\n        (有些情况下通过 lsof(8) 或 fuser(1) 可以\n         找到有关使用该设备的进程的有用信息。)\n\n&quot;</span><br></pre></td></tr></table></figure>

<p>仔细看卸载失败的日志，可以看到这个挂载点的后端存储是<code>glusterfs</code>，而<code>目标忙</code>一般来说是存储设备侧在使用，所以无法卸载。那就找找看是不是哪个进程使用了这个挂载目录（以下定位由负责<code>glusterfs</code>的同事提供）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# fuser -mv /var/lib/kubelet/pods/xxx/volumes/kubernetes.io~glusterfs/cam-pv-50g</span><br><span class="line">用户  进程号  权限  命令</span><br><span class="line">root  kernel mount /var/lib/kubelet/pods/xxx/volumes/kubernetes.io~glusterfs/cam-dialog-gl.uster-pv-50g</span><br><span class="line">root  94549  f.... glusterfs</span><br></pre></td></tr></table></figure>

<p>除了内核的<code>mount</code>，还有个<code>pid=94549</code>的<code>glusterfs</code>进程在占用挂载点所在目录，看看是什么进程：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# ps -ef| grep 94549</span><br><span class="line">root 94549 1 0 7月26 ? 00:01:13 /usr/sbin/glusterfs --log-level=ERR0R --log-file=/var/lib/kubelet/plugins/kubernetes.io/glusterfs/global-diaglog-pv/web-fddf96444-stxpf-glusterfs.log --fuse-mountopts=auto_unmount --process-name fuse --volfile-server=xxx --volfile-server=xxx --tfolfile-server=xxx --volfile-id=global-diaglog --fuse-mountopts=auto_unmount /var/lib/kubelet/pods/xxx/volumes/kubernetef.io-glusterfs/global-diaglog-pv</span><br></pre></td></tr></table></figure>

<p>发现这个进程维护的是<code>web-xxx</code>的挂载信息，而<code>web-xxx</code>和<code>cam-xxx</code>没有任何关联。由此推断出是<code>glusterfs</code>管理的挂载信息发送错乱导致，具体错乱原因就转给相关负责的同事看了。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>从分析结果看，是共享存储卷未正常卸载导致的删除Pod异常，非K8S问题。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubernetes/tree/v1.15.12">Kubernetes v1.15.12源码</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/nangonghen/article/details/109305635">kubernetes删除pod的流程的源码简析</a></li>
<li><a target="_blank" rel="noopener" href="https://juejin.cn/post/6844903842321039368">Kubernetes源码分析之Pod的删除</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/608727">kubernetes grace period 失效问题排查</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/08/07/linux/%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Web%E5%BA%94%E7%94%A8%E9%A1%B5%E9%9D%A2%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/07/linux/%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Web%E5%BA%94%E7%94%A8%E9%A1%B5%E9%9D%A2%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE/" class="post-title-link" itemprop="url">网络问题排查-Web应用页面无法访问</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-08-07 21:30:51" itemprop="dateCreated datePublished" datetime="2021-08-07T21:30:51+00:00">2021-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:09:04" itemprop="dateModified" datetime="2024-04-14T02:09:04+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/linux/" itemprop="url" rel="index"><span itemprop="name">linux</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>部署在服务器上的Web应用因为机房迁移，导致PC上无法正常访问Web页面。</p>
<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>本次遇到的问题纯属网络层面问题，不用多想，先登录到服务器上，查看服务端口的监听状态：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">[root@node2]# </span><span class="language-bash">netstat -anp|grep 443</span></span><br><span class="line">tcp6       0      0 :::443                 :::*                    LISTEN      8450/java</span><br></pre></td></tr></table></figure>

<p>在服务器所在节点、服务器之前的其他节点上<code>curl</code>监听端口看看是否有响应：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">[root@node2]# </span><span class="language-bash">curl -i -k https://192.168.10.10:443</span></span><br><span class="line">HTTP/1.1 302 Found</span><br><span class="line">Location: https://127.0.0.1:443</span><br><span class="line">Content-Length: 0</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">[root@node2]# </span><span class="language-bash">curl -i -k https://192.168.10.11:443</span></span><br><span class="line">HTTP/1.1 302 Found</span><br><span class="line">Location: https://192.168.10.11:443</span><br><span class="line">Content-Length: 0</span><br></pre></td></tr></table></figure>

<p>到此为止，说明Web服务运行正常，<strong>问题出在了PC到服务器这个通信过程</strong>。本地<code>wireshark</code>抓包看看，相关异常报文如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">371 70.961626   3.2.253.177     172.30.31.151   TCP     66  52541 → 443 [SYN] Seq=0 Win=8192 Len=0 MSS=1460 WS=4 SACK_PERM=1</span><br><span class="line">373 70.962516   172.30.31.151   3.2.253.177     TCP     66  443 → 52541 [SYN, ACK] Seq=0 Ack=1 Win=29200 Len=0 MSS=1460 SACK_PERM=1 WS=128</span><br><span class="line">375 70.962563   3.2.253.177     172.30.31.151   TCP     54  52541 → 443 [ACK] Seq=1 Ack=1 Win=65700 Len=0</span><br><span class="line">377 70.963248   3.2.253.177     172.30.31.151   TLSv1.2 571 Client Hello</span><br><span class="line">379 70.964323   172.30.31.151   3.2.253.177     TCP     60  443 → 52541 [ACK] Seq=1 Ack=518 Win=30336 Len=0</span><br><span class="line">381 70.965327   172.30.31.151   3.2.253.177     TLSv1.2 144 Server Hello</span><br><span class="line">383 70.965327   172.30.31.151   3.2.253.177     TLSv1.2 105 Change Cipher Spec, Encrypted Handshake Message</span><br><span class="line">385 70.965364   3.2.253.177     172.30.31.151   TCP     54  52541 → 443 [ACK] Seq=518 Ack=142 Win=65556 Len=0</span><br><span class="line">387 70.967194   3.2.253.177     172.30.31.151   TLSv1.2 61  Alert (Level: Fatal, Description: Certificate Unknown)</span><br><span class="line">388 70.967233   3.2.253.177     172.30.31.151   TCP     54  52541 → 443 [FIN, ACK] Seq=525 Ack=142 Win=65556 Len=0</span><br><span class="line">391 70.968320   172.30.31.151   3.2.253.177     TLSv1.2 85  Encrypted Alert</span><br><span class="line">392 70.968321   172.30.31.151   3.2.253.177     TCP     60  443 → 52541 [FIN, ACK] Seq=173 Ack=526 Win=30336 Len=0</span><br><span class="line">394 70.968356   3.2.253.177     172.30.31.151   TCP     54  52541 → 443 [RST, ACK] Seq=526 Ack=173 Win=0 Len=0</span><br><span class="line">395 70.968370   3.2.253.177     172.30.31.151   TCP     54  52541 → 443 [RST] Seq=526 Win=0 Len=0</span><br></pre></td></tr></table></figure>

<p>关键是最后两个，可以看出报文存在复位标志<code>RST</code>。与提供环境的人了解到PC与服务器之间使用的交换机是通过<code>GRE隧道</code>打通的网络，基本怀疑是交换机配置存在问题；</p>
<p>同时观察到PC访问集群的<code>ftp</code>也存在异常，说明是一个通用问题，而PC上<code>ping</code>和<code>ssh</code>服务器都没有问题，说明是配置导致的部分协议的连接问题；</p>
<p>后来提供环境的人排查交换机配置，发现<code>GRE隧道</code>的默认<code>MTU</code>为<code>1464</code>，而集群网卡上的<code>MTU</code>为<code>1500</code>，最后协商出的<code>MSS</code>为<code>1460</code>（见抓包中的前两个报文）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[leaf11]dis interface Tunnel</span><br><span class="line">Tunnel0</span><br><span class="line">Current state: UP</span><br><span class="line">Line protocol state: UP</span><br><span class="line">Description: Tunnel0 Interface</span><br><span class="line">Bandwidth: 64 kbps</span><br><span class="line">Maximum transmission unit: 1464</span><br><span class="line">Internet protocol processing: Disabled</span><br><span class="line">Last clearing of counters: Never</span><br><span class="line">Tunnel source 3.1.1.11, destination 2.1.1.222</span><br><span class="line">Tunnel protocol/transport UDP_VXLAN/IP</span><br><span class="line">Last 300 seconds input rate: 0 bytes/sec, 0 bits/sec, 0 packets/sec</span><br><span class="line">Last 300 seconds output rate: 0 bytes/sec, 0 bits/</span><br></pre></td></tr></table></figure>

<p>这种情况下，最大的报文发到交换机后，因为交换机允许的最大报文数为<code>1464-40=1424</code>字节，所以出现了上述现象，同时也解释了<code>http</code>和<code>ftp</code>有问题（长报文），而<code>ping</code>和<code>ssh</code>没有问题（短报文）。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>方案1：修改隧道口和物理口的<code>MTU</code>值，但是取值不好定，因为不知道应用最长报文的长度。<br>方案2：<code>GRE</code>隧道口配置<code>TCP</code>的<code>MSS</code>，超出后分片处理。</p>
<p>设置<code>TCP</code>的<code>MSS</code>参考命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">【命令】</span><br><span class="line">tcp mss value</span><br><span class="line">undo tcp mss</span><br><span class="line">【缺省情况】</span><br><span class="line">未配置接口的TCP最大报文段长度。</span><br><span class="line">【视图】</span><br><span class="line">接口视图</span><br><span class="line">【缺省用户角色】</span><br><span class="line">network-admin</span><br><span class="line">mdc-admin</span><br><span class="line">【参数】</span><br><span class="line">value：TCP最大报文段长度，取值范围为128～（接口的最大MTU值-40），单位为字节。</span><br><span class="line">【使用指导】</span><br><span class="line">TCP最大报文段长度（Max Segment Size，MSS）表示TCP连接的对端发往本端的最大TCP报文段的长度，目前作为TCP连接建立时的一个选项来协商：当一个TCP连接建立时，连接的双方要将MSS作为TCP报文的一个选项通告给对端，对端会记录下这个MSS值，后续在发送TCP报文时，会限制TCP报文的大小不超过该MSS值。当对端发送的TCP报文的长度小于本端的TCP最大报文段长度时，TCP报文不需要分段；否则，对端需要对TCP报文按照最大报文段长度进行分段处理后再发给本端。</span><br><span class="line">该配置仅对新建的TCP连接生效，对于配置前已建立的TCP连接不生效。</span><br><span class="line">该配置仅对IP报文生效，当接口上配置了MPLS功能后，不建议再配置本功能。 </span><br></pre></td></tr></table></figure>

<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43684922/article/details/105300934">https://blog.csdn.net/qq_43684922/article/details/105300934</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/07/24/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Pod%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E9%AB%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/24/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Pod%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E9%AB%98/" class="post-title-link" itemprop="url">K8S问题排查-Pod内存占用高</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-24 16:17:21" itemprop="dateCreated datePublished" datetime="2021-07-24T16:17:21+00:00">2021-07-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:09:04" itemprop="dateModified" datetime="2024-04-14T02:09:04+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>如下所示，用户使用<code>kubectl top</code>命令看到其中一个节点上的Harbor占用内存约3.7G（其他业务Pod也存在类似现象），整体上来说，有点偏高。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@node02 ~]# kubectl get node -owide</span><br><span class="line">NAME   STATUS   ROLES    AGE   VERSION    INTERNAL-IP   EXTERNAL-IP       </span><br><span class="line">node01   Ready    master   10d   v1.15.12   100.1.0.10    &lt;none&gt;   </span><br><span class="line">node02   Ready    master   12d   v1.15.12   100.1.0.11    &lt;none&gt;  </span><br><span class="line">node03   Ready    master   10d   v1.15.12   100.1.0.12    &lt;none&gt; </span><br><span class="line"></span><br><span class="line">[root@node02 ~]# kubectl top pod -A |grep harbor</span><br><span class="line">kube-system         harbor-master1-sxg2l                          15m          150Mi</span><br><span class="line">kube-system         harbor-master2-ncvb8                          8m           3781Mi</span><br><span class="line">kube-system         harbor-master3-2gdsn                          14m          227Mi</span><br></pre></td></tr></table></figure>

<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>我们知道，查看容器的内存占用，可以使用<code>kubectl top</code>命令，也可以使用<code>docker stats</code>命令，并且理论上来说，<code>docker stats</code>命令查的结果应该比<code>kubectl top</code>查到的更准确。查看并统计发现，实际上Harbor总内存占用约为140M左右，远没有达到3.7G：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@node02 ~]# docker stats |grep harbor</span><br><span class="line">CONTAINER ID        NAME                                      CPU %    MEM USAGE / LIMIT     MEM %</span><br><span class="line">10a230bee3c7        k8s_nginx_harbor-master2-xxx              0.02%    14.15MiB / 94.26GiB   0.01%</span><br><span class="line">6ba14a04fd77        k8s_harbor-portal_harbor-master2-xxx      0.01%    13.73MiB / 94.26GiB   0.01%</span><br><span class="line">324413da20a9        k8s_harbor-jobservice_harbor-master2-xxx  0.11%    21.54MiB / 94.26GiB   0.02%</span><br><span class="line">d880b61cf4cb        k8s_harbor-core_harbor-master2-xxx        0.12%    33.2MiB / 94.26GiB    0.03%</span><br><span class="line">186c064d0930        k8s_harbor-registryctl_harbor-master2-xxx 0.01%    8.34MiB / 94.26GiB    0.01%</span><br><span class="line">52a50204a962        k8s_harbor-registry_harbor-master2-xxx    0.06%    29.99MiB / 94.26GiB   0.03%</span><br><span class="line">86031ddd0314        k8s_harbor-redis_harbor-master2-xxx       0.14%    11.51MiB / 94.26GiB   0.01%</span><br><span class="line">6366207680f2        k8s_harbor-database_harbor-master2-xxx    0.45%    8.859MiB / 94.26GiB   0.01%</span><br></pre></td></tr></table></figure>

<p>这是什么情况？两个命令查到的结果差距也太大了。查看资料[1]可以知道：</p>
<ol>
<li><code>kubectl top</code>命令的计算公式：<code>memory.usage_in_bytes - inactive_file</code>；</li>
<li><code>docker stats</code>命令的计算公式：<code>memory.usage_in_bytes - cache</code>；</li>
</ol>
<p>可以看出，两种方式收集机制不一样，如果<code>cache</code>比较大，<code>kubectl top</code>命令看到的结果会偏高。根据上面的计算公式验证看看是否正确：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">curl -s --unix-socket /var/run/docker.sock http:/v1.24/containers/xxx/stats | jq .&quot;memory_stats&quot;</span><br><span class="line">&quot;memory_stats&quot;: &#123;</span><br><span class="line">    &quot;usage&quot;: 14913536,</span><br><span class="line">    &quot;max_usage&quot;: 15183872,</span><br><span class="line">    &quot;stats&quot;: &#123;</span><br><span class="line">      &quot;active_anon&quot;: 14835712,</span><br><span class="line">      &quot;active_file&quot;: 0,</span><br><span class="line">      &quot;cache&quot;: 77824,</span><br><span class="line">      &quot;dirty&quot;: 0,</span><br><span class="line">      &quot;hierarchical_memory_limit&quot;: 101205622784,</span><br><span class="line">      &quot;hierarchical_memsw_limit&quot;: 9223372036854772000,</span><br><span class="line">      &quot;inactive_anon&quot;: 4096,</span><br><span class="line">      &quot;inactive_file&quot;: 73728,</span><br><span class="line">      ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&quot;memory_stats&quot;: &#123;</span><br><span class="line">    &quot;usage&quot;: 14405632,</span><br><span class="line">    &quot;max_usage&quot;: 14508032,</span><br><span class="line">    &quot;stats&quot;: &#123;</span><br><span class="line">      &quot;active_anon&quot;: 14397440,</span><br><span class="line">      &quot;active_file&quot;: 0,</span><br><span class="line">      &quot;cache&quot;: 8192,</span><br><span class="line">      &quot;dirty&quot;: 0,</span><br><span class="line">      &quot;hierarchical_memory_limit&quot;: 101205622784,</span><br><span class="line">      &quot;hierarchical_memsw_limit&quot;: 9223372036854772000,</span><br><span class="line">      &quot;inactive_anon&quot;: 4096,</span><br><span class="line">      &quot;inactive_file&quot;: 4096,</span><br><span class="line">      ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&quot;memory_stats&quot;: &#123;</span><br><span class="line">    &quot;usage&quot;: 26644480,</span><br><span class="line">    &quot;max_usage&quot;: 31801344,</span><br><span class="line">    &quot;stats&quot;: &#123;</span><br><span class="line">      &quot;active_anon&quot;: 22810624,</span><br><span class="line">      &quot;active_file&quot;: 790528,</span><br><span class="line">      &quot;cache&quot;: 3833856,</span><br><span class="line">      &quot;dirty&quot;: 0,</span><br><span class="line">      &quot;hierarchical_memory_limit&quot;: 101205622784,</span><br><span class="line">      &quot;hierarchical_memsw_limit&quot;: 9223372036854772000,</span><br><span class="line">      &quot;inactive_anon&quot;: 0,</span><br><span class="line">      &quot;inactive_file&quot;: 3043328,</span><br><span class="line">      ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&quot;memory_stats&quot;: &#123;</span><br><span class="line">    &quot;usage&quot;: 40153088,</span><br><span class="line">    &quot;max_usage&quot;: 90615808,</span><br><span class="line">    &quot;stats&quot;: &#123;</span><br><span class="line">      &quot;active_anon&quot;: 35123200,</span><br><span class="line">      &quot;active_file&quot;: 1372160,</span><br><span class="line">      &quot;cache&quot;: 5029888,</span><br><span class="line">      &quot;dirty&quot;: 0,</span><br><span class="line">      &quot;hierarchical_memory_limit&quot;: 101205622784,</span><br><span class="line">      &quot;hierarchical_memsw_limit&quot;: 9223372036854772000,</span><br><span class="line">      &quot;inactive_anon&quot;: 0,</span><br><span class="line">      &quot;inactive_file&quot;: 3657728,</span><br><span class="line">      ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&quot;memory_stats&quot;: &#123;</span><br><span class="line">    &quot;usage&quot;: 10342400,</span><br><span class="line">    &quot;max_usage&quot;: 12390400,</span><br><span class="line">    &quot;stats&quot;: &#123;</span><br><span class="line">    &quot;active_anon&quot;: 8704000,</span><br><span class="line">    &quot;active_file&quot;: 241664,</span><br><span class="line">    &quot;cache&quot;: 1638400,</span><br><span class="line">    &quot;dirty&quot;: 0,</span><br><span class="line">    &quot;hierarchical_memory_limit&quot;: 101205622784,</span><br><span class="line">    &quot;hierarchical_memsw_limit&quot;: 9223372036854772000,</span><br><span class="line">    &quot;inactive_anon&quot;: 0,</span><br><span class="line">    &quot;inactive_file&quot;: 1396736,</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&quot;memory_stats&quot;: &#123;</span><br><span class="line">    &quot;usage&quot;: 5845127168,</span><br><span class="line">    &quot;max_usage&quot;: 22050988032,</span><br><span class="line">    &quot;stats&quot;: &#123;</span><br><span class="line">    &quot;active_anon&quot;: 31576064,</span><br><span class="line">    &quot;active_file&quot;: 3778052096,</span><br><span class="line">    &quot;cache&quot;: 5813551104,</span><br><span class="line">    &quot;dirty&quot;: 0,</span><br><span class="line">    &quot;hierarchical_memory_limit&quot;: 101205622784,</span><br><span class="line">    &quot;hierarchical_memsw_limit&quot;: 9223372036854772000,</span><br><span class="line">    &quot;inactive_anon&quot;: 0,</span><br><span class="line">    &quot;inactive_file&quot;: 2035499008,</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&quot;memory_stats&quot;: &#123;</span><br><span class="line">    &quot;usage&quot;: 13250560,</span><br><span class="line">    &quot;max_usage&quot;: 34791424,</span><br><span class="line">    &quot;stats&quot;: &#123;</span><br><span class="line">    &quot;active_anon&quot;: 12070912,</span><br><span class="line">    &quot;active_file&quot;: 45056,</span><br><span class="line">    &quot;cache&quot;: 1179648,</span><br><span class="line">    &quot;dirty&quot;: 0,</span><br><span class="line">    &quot;hierarchical_memory_limit&quot;: 101205622784,</span><br><span class="line">    &quot;hierarchical_memsw_limit&quot;: 9223372036854772000,</span><br><span class="line">    &quot;inactive_anon&quot;: 0,</span><br><span class="line">    &quot;inactive_file&quot;: 1134592,</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&quot;memory_stats&quot;: &#123;</span><br><span class="line">    &quot;usage&quot;: 50724864,</span><br><span class="line">    &quot;max_usage&quot;: 124682240,</span><br><span class="line">    &quot;stats&quot;: &#123;</span><br><span class="line">    &quot;active_anon&quot;: 23502848,</span><br><span class="line">    &quot;active_file&quot;: 13864960,</span><br><span class="line">    &quot;cache&quot;: 41435136,</span><br><span class="line">    &quot;dirty&quot;: 0,</span><br><span class="line">    &quot;hierarchical_memory_limit&quot;: 101205622784,</span><br><span class="line">    &quot;hierarchical_memsw_limit&quot;: 9223372036854772000,</span><br><span class="line">    &quot;inactive_anon&quot;: 6836224,</span><br><span class="line">    &quot;inactive_file&quot;: 6520832,</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>根据上面提供的计算公式和实际获取的<code>memory_stats</code>数据，验证<code>kubectl top</code>结果和<code>docker stats</code>结果符合预期。那为什么Harbor缓存会占用那么高呢？</p>
<p>通过实际环境分析看，Harbor中占用缓存较高的组件是<code>registry</code>（如下所示，缓存有5.4G），考虑到<code>registry</code>负责<code>docker</code>镜像的存储，在处理镜像时会有大量的镜像层文件的读写操作，所以正常情况下这些操作确实会比较耗缓存；</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&quot;memory_stats&quot;: &#123;</span><br><span class="line">    &quot;usage&quot;: 5845127168,</span><br><span class="line">    &quot;max_usage&quot;: 22050988032,</span><br><span class="line">    &quot;stats&quot;: &#123;</span><br><span class="line">    &quot;active_anon&quot;: 31576064,</span><br><span class="line">    &quot;active_file&quot;: 3778052096,</span><br><span class="line">    &quot;cache&quot;: 5813551104,</span><br><span class="line">    &quot;dirty&quot;: 0,</span><br><span class="line">    &quot;hierarchical_memory_limit&quot;: 101205622784,</span><br><span class="line">    &quot;hierarchical_memsw_limit&quot;: 9223372036854772000,</span><br><span class="line">    &quot;inactive_anon&quot;: 0,</span><br><span class="line">    &quot;inactive_file&quot;: 2035499008,</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>与用户沟通，说明<code>kubectl top</code>看到的结果包含了容器内使用的<code>cache</code>，结果会偏高，这部分缓存在内存紧张情况下会被系统回收，或者手工操作也可以释放，建议使用<code>docker stats</code>命令查看实际内存使用率。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/xyclianying/article/details/108513122">https://blog.csdn.net/xyclianying/article/details/108513122</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/07/16/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E4%B8%9A%E5%8A%A1%E9%AB%98%E5%B9%B6%E5%8F%91%E5%AF%BC%E8%87%B4Pod%E5%8F%8D%E5%A4%8D%E9%87%8D%E5%90%AF(%E7%BB%AD)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/16/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E4%B8%9A%E5%8A%A1%E9%AB%98%E5%B9%B6%E5%8F%91%E5%AF%BC%E8%87%B4Pod%E5%8F%8D%E5%A4%8D%E9%87%8D%E5%90%AF(%E7%BB%AD)/" class="post-title-link" itemprop="url">K8S问题排查-业务高并发导致Pod反复重启(续)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-16 21:10:40" itemprop="dateCreated datePublished" datetime="2021-07-16T21:10:40+00:00">2021-07-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:09:04" itemprop="dateModified" datetime="2024-04-14T02:09:04+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>接上次的<a href="https://lyyao09.github.io/2021/06/19/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E4%B8%9A%E5%8A%A1%E9%AB%98%E5%B9%B6%E5%8F%91%E5%AF%BC%E8%87%B4Pod%E5%8F%8D%E5%A4%8D%E9%87%8D%E5%90%AF/">问题</a>，一段时间后，环境再次出现<code>harbor</code>和<code>calico</code>因为健康检查不过反复重启的问题，并且使用<code>kubectl</code>命令进入Pod也响应非常慢甚至超时。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# kubectl exec -it -n system node1-59c9475bc6-zkhq5 bash</span><br><span class="line">^</span><br></pre></td></tr></table></figure>

<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>反复重启的原因上次已定位，这次上环境简单看还是因为健康检查超时的问题，并且现象也一样，<code>TCP</code>的连接卡在了第一次握手的<code>SYN_SENT</code>阶段。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# netstat -anp|grep 23380</span><br><span class="line">tcp        0      0 127.0.0.1:23380         0.0.0.0:*               LISTEN      38914/kubelet</span><br><span class="line">tcp        0      0 127.0.0.1:38983         127.0.0.1:23380         SYN_SENT    -</span><br></pre></td></tr></table></figure>

<p>也就是说，除了<code>TCP</code>连接队列的问题，还存在其他问题会导致该现象。先看看上次的参数还在不在：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# cat /etc/sysctl.conf</span><br><span class="line">net.ipv4.tcp_max_syn_backlog = 32768</span><br><span class="line">net.core.somaxconn = 32768</span><br></pre></td></tr></table></figure>

<p>再看下上次修改的参数是否生效：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# ss -lnt</span><br><span class="line">State      Recv-Q   Send-Q     Local Address:Port      Peer Address:Port              </span><br><span class="line">LISTEN     0        32768      127.0.0.1:23380         *:*</span><br></pre></td></tr></table></figure>

<p>参数的修改也生效了，那为什么还会卡在<code>SYN_SENT</code>阶段呢？从现有情况，看不出还有什么原因会导致该问题，只能摸索看看。</p>
<ol>
<li>在问题节点和非问题节点上分别抓包，看报文交互是否存在什么异常；</li>
<li>根据参考资料[1]，排查是否为相同问题；</li>
<li>根据参考资料[2]，排查是否相同问题；</li>
<li>…</li>
</ol>
<p>摸索一番，没发现什么异常。回过头来想想，既然是业务下发大量配置导致的，并且影响是全局的（除了业务Pod自身，其他组件也受到了影响），说明大概率原因还是系统层面存在的性能瓶颈。业务量大的影响除了CPU、一般还有内存、磁盘、连接数等等，与开发人员确认他们的连接还是长连接，那么连接数很大的情况下会受到什么内核参数的影响呢？其中一个就是我们熟知的文件句柄数。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# lsof -p 45775 | wc -l</span><br><span class="line">17974</span><br><span class="line"></span><br><span class="line">[root@node01 ~]# lsof -p 45775|grep &quot;sock&quot;| wc -l</span><br><span class="line">12051</span><br></pre></td></tr></table></figure>

<p>嗯，打开了<strong>1w+的文件句柄数并且基本都是<code>sock</code>连接</strong>，而我们使用的操作系统默认情况下每个进程的文件句柄数限制为1024，查看确认一下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# ulimit  -n</span><br><span class="line">1024</span><br></pre></td></tr></table></figure>

<p>超额使用了这么多，业务Pod竟然没有<code>too many open files</code>错误：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# kubectl logs -n system node1-59c9475bc6-zkhq5</span><br><span class="line">start config</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>临时修改一下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# ulimit -n 65535</span><br><span class="line">[root@node01 ~]# ulimit  -n</span><br><span class="line">65535</span><br></pre></td></tr></table></figure>

<p>再次使用<code>kubectl</code>命令进入业务Pod，响应恢复正常，并且查看连接也不再有卡住的<code>SYN_SENT</code>阶段：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# kubectl exec -it -n system node1-59c9475bc6-zkhq5 bash</span><br><span class="line"><span class="meta prompt_">[root@node1-59c9475bc6-zkhq5]# </span><span class="language-bash"><span class="built_in">exit</span></span></span><br><span class="line">[root@node01 ~]# kubectl exec -it -n system node1-59c9475bc6-zkhq5 bash</span><br><span class="line"><span class="meta prompt_">[root@node1-59c9475bc6-zkhq5]# </span><span class="language-bash"><span class="built_in">exit</span></span></span><br><span class="line">[root@node01 ~]# kubectl exec -it -n system node1-59c9475bc6-zkhq5 bash</span><br><span class="line"><span class="meta prompt_">[root@node1-59c9475bc6-zkhq5]# </span><span class="language-bash"><span class="built_in">exit</span></span></span><br><span class="line"></span><br><span class="line">[root@node01 ~]# netstat -anp|grep 23380</span><br><span class="line">tcp        0      0 127.0.0.1:23380         0.0.0.0:*               LISTEN      38914/kubelet</span><br><span class="line">tcp        0      0 127.0.0.1:56369         127.0.0.1:23380         TIME_WAIT   -</span><br><span class="line">tcp        0      0 127.0.0.1:23380         127.0.0.1:57601         TIME_WAIT   -</span><br><span class="line">tcp        0      0 127.0.0.1:23380         127.0.0.1:57479         TIME_WAIT   -</span><br></pre></td></tr></table></figure>

<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ol>
<li>业务根据实际情况调整文件句柄数。</li>
<li>针对业务量大的环境，强烈建议整体做一下操作系统层面的性能优化，否则，不定哪个系统参数就成了性能瓶颈，网上找了个调优案例[3]，感兴趣的可以参考。</li>
</ol>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/pyxllq/article/details/80351827">https://blog.csdn.net/pyxllq/article/details/80351827</a></li>
<li><a target="_blank" rel="noopener" href="http://mdba.cn/2015/03/10/tcp-socket%E6%96%87%E4%BB%B6%E5%8F%A5%E6%9F%84%E6%B3%84%E6%BC%8F">http://mdba.cn/2015/03/10/tcp-socket文件句柄泄漏</a></li>
<li><a target="_blank" rel="noopener" href="https://www.shuzhiduo.com/A/RnJW7NLyJq/">https://www.shuzhiduo.com/A/RnJW7NLyJq/</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/07/10/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Influxdb%E7%9B%91%E6%8E%A7%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96%E5%BC%82%E5%B8%B8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/10/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Influxdb%E7%9B%91%E6%8E%A7%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96%E5%BC%82%E5%B8%B8/" class="post-title-link" itemprop="url">K8S问题排查-Influxdb监控数据获取异常</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-10 11:11:46" itemprop="dateCreated datePublished" datetime="2021-07-10T11:11:46+00:00">2021-07-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:09:04" itemprop="dateModified" datetime="2024-04-14T02:09:04+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>K8S集群内，<code>Influxdb</code>监控数据获取异常，最终CPU、内存和磁盘使用率都无法获取。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">监控项         使用率</span><br><span class="line">CPU(核)        3%</span><br><span class="line">内存(GB)       18%</span><br><span class="line">磁盘空间(GB)    0%</span><br><span class="line"></span><br><span class="line">监控项         使用率</span><br><span class="line">CPU(核)        7%</span><br><span class="line">内存(GB)       18%</span><br><span class="line">磁盘空间(GB)    1%</span><br><span class="line"></span><br><span class="line">监控项         使用率</span><br><span class="line">CPU(核)        0%</span><br><span class="line">内存(GB)       0%</span><br><span class="line">磁盘空间(GB)    0%</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p><code>Influxdb</code>监控架构图参考[1]，其中<code>Load Balancer</code>采用<code>nginx</code>实现：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">        ┌─────────────────┐                 </span><br><span class="line">        │writes &amp; queries │                 </span><br><span class="line">        └─────────────────┘                 </span><br><span class="line">                 │                          </span><br><span class="line">                 ▼                          </span><br><span class="line">         ┌───────────────┐                  </span><br><span class="line">         │               │                  </span><br><span class="line">┌────────│ Load Balancer │─────────┐        </span><br><span class="line">│        │               │         │        </span><br><span class="line">│        └──────┬─┬──────┘         │        </span><br><span class="line">│               │ │                │        </span><br><span class="line">│               │ │                │        </span><br><span class="line">│        ┌──────┘ └────────┐       │        </span><br><span class="line">│        │ ┌─────────────┐ │       │┌──────┐</span><br><span class="line">│        │ │/write or UDP│ │       ││/query│</span><br><span class="line">│        ▼ └─────────────┘ ▼       │└──────┘</span><br><span class="line">│  ┌──────────┐      ┌──────────┐  │        </span><br><span class="line">│  │ InfluxDB │      │ InfluxDB │  │        </span><br><span class="line">│  │ Relay    │      │ Relay    │  │        </span><br><span class="line">│  └──┬────┬──┘      └────┬──┬──┘  │        </span><br><span class="line">│     │    |              |  │     │        </span><br><span class="line">│     |  ┌─┼──────────────┘  |     │        </span><br><span class="line">│     │  │ └──────────────┐  │     │        </span><br><span class="line">│     ▼  ▼                ▼  ▼     │        </span><br><span class="line">│  ┌──────────┐      ┌──────────┐  │        </span><br><span class="line">│  │          │      │          │  │        </span><br><span class="line">└─▶│ InfluxDB │      │ InfluxDB │◀─┘        </span><br><span class="line">   │          │      │          │           </span><br><span class="line">   └──────────┘      └──────────┘</span><br></pre></td></tr></table></figure>

<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>因为获取的数据来源是<code>influxdb</code>数据库，所以先搞清楚异常的原因是请求路径上的问题，还是<code>influxdb</code>数据库自身没有数据的问题：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">找到influxdb-nginx的service</span></span><br><span class="line">kubectl get svc  -n kube-system -owide</span><br><span class="line">NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE   SELECTOR</span><br><span class="line">grafana-service          ClusterIP   10.96.177.245   &lt;none&gt;        3000/TCP                 21d   app=grafana</span><br><span class="line">heapster                 ClusterIP   10.96.239.225   &lt;none&gt;        80/TCP                   21d   app=heapster</span><br><span class="line">influxdb-nginx-service   ClusterIP   10.96.170.72    &lt;none&gt;        7076/TCP                 21d   app=influxdb-nginx</span><br><span class="line">influxdb-relay-service   ClusterIP   10.96.196.45    &lt;none&gt;        9096/TCP                 21d   app=influxdb-relay</span><br><span class="line">influxdb-service         ClusterIP   10.96.127.45    &lt;none&gt;        8086/TCP                 21d   app=influxdb</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在集群节点上检查访问influxdb-nginx的service是否正常</span></span><br><span class="line">curl -i 10.96.170.72:7076/query</span><br><span class="line">HTTP/1.1 401 Unauthorized</span><br><span class="line">Server: nginx/1.17.2</span><br></pre></td></tr></table></figure>

<p>可以看出，请求发送到<code>influxdb-nginx</code>的<code>service</code>是正常的，也就是请求可以正常发送到后端的<code>influxdb</code>数据库。那就继续确认<code>influxdb</code>数据库自身没有数据的问题：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">找到influxdb数据库的pod</span></span><br><span class="line">kubectl get pod -n kube-system -owide |grep influxdb</span><br><span class="line">influxdb-nginx-4x8pr                       1/1     Running   3          21d   177.177.52.201    node3</span><br><span class="line">influxdb-nginx-tpngh                       1/1     Running   6          21d   177.177.41.214    node1</span><br><span class="line">influxdb-nginx-wh6kc                       1/1     Running   5          21d   177.177.250.180   node2</span><br><span class="line">influxdb-relay-rs-65c94bbf5f-dp7s4         1/1     Running   2          21d   177.177.250.148   node2</span><br><span class="line">influxdb1-6ff9466d46-q6w5r                 1/1     Running   3          21d   177.177.41.230    node1</span><br><span class="line">influxdb2-d6d6697f5-zzcnk                  1/1     Running   3          21d   177.177.250.161   node2</span><br><span class="line">influxdb3-65ddfc7476-hxhr8                 1/1     Running   4          21d   177.177.52.217    node3</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">登录任意一个influxdb容器内并进入交互式命令</span></span><br><span class="line">kubectl exec -it -n kube-systme influxdb-rs3-65ddfc7476-hxhr8 bash</span><br><span class="line">root@influxdb-rs3-65ddfc7476-hxhr8:/# influx</span><br><span class="line">Connected to http://localhost:8086 version 1.7.7</span><br><span class="line">InfluxDB shell version: 1.7.7</span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">auth</span></span><br><span class="line">username: admin</span><br><span class="line">password: xxx</span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">use xxx;</span></span><br><span class="line">Using database xxx</span><br></pre></td></tr></table></figure>

<p>根据业务层面的查询语句，在<code>influxdb</code>交互式命令下手工查询验证：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="keyword">select</span> <span class="built_in">sum</span>(value) from <span class="string">&quot;cpu/node_capacity&quot;</span> <span class="built_in">where</span> <span class="string">&quot;type&quot;</span> = <span class="string">&#x27;node&#x27;</span> and <span class="string">&quot;nodename&quot;</span> = <span class="string">&#x27;node1&#x27;</span> and time &gt; now() - 2m</span></span><br><span class="line"><span class="meta prompt_">&gt;</span></span><br></pre></td></tr></table></figure>

<p>结果发现确实没有查到数据，既然<code>2min</code>内的数据没有，那把时间线拉长一些看看呢？</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">不限制时间范围的查询</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="keyword">select</span> <span class="built_in">sum</span>(value) from <span class="string">&quot;cpu/node_capacity&quot;</span>&gt; <span class="keyword">select</span> <span class="built_in">sum</span>(value) from <span class="string">&quot;cpu/node_capacity&quot;</span>;</span></span><br><span class="line">name: cpu/node_capacity</span><br><span class="line">time sum</span><br><span class="line">---- ---</span><br><span class="line">0    5301432000</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查询72min内的数据</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="keyword">select</span> <span class="built_in">sum</span>(value) from <span class="string">&quot;cpu/node_capacity&quot;</span> <span class="built_in">where</span> <span class="string">&quot;type&quot;</span> = <span class="string">&#x27;node&#x27;</span> and <span class="string">&quot;nodename&quot;</span> = <span class="string">&#x27;node1&#x27;</span> and time &gt; now() - 72m</span></span><br><span class="line">name: cpu/node_capacity</span><br><span class="line">time                sum</span><br><span class="line">----                ---</span><br><span class="line">1624348319900503945 72000</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">sleep</span> 1min，继续查询72min内的数据</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="keyword">select</span> <span class="built_in">sum</span>(value) from <span class="string">&quot;cpu/node_capacity&quot;</span> <span class="built_in">where</span> <span class="string">&quot;type&quot;</span> = <span class="string">&#x27;node&#x27;</span> and <span class="string">&quot;nodename&quot;</span> = <span class="string">&#x27;node1&#x27;</span> and time &gt; now() - 72m</span></span><br><span class="line">name: cpu/node_capacity</span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash"></span><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查询73min内的数据</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="keyword">select</span> <span class="built_in">sum</span>(value) from <span class="string">&quot;cpu/node_capacity&quot;</span> <span class="built_in">where</span> <span class="string">&quot;type&quot;</span> = <span class="string">&#x27;node&#x27;</span> and <span class="string">&quot;nodename&quot;</span> = <span class="string">&#x27;node1&#x27;</span> and time &gt; now() - 73m</span></span><br><span class="line">name: cpu/node_capacity</span><br><span class="line">time                sum</span><br><span class="line">----                ---</span><br><span class="line">1624348319900503945 72000</span><br></pre></td></tr></table></figure>

<p>根据查询结果看，不添加时间范围的查询是有记录的，并且通过多次验证看，<strong>数据无法获取的原因是数据在某个时间点不再写入导致的</strong>。查看<code>influxdb</code>的日志看看有没有什么相关日志：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs -n kube-systme influxdb-rs3-65ddfc7476-hxhr8</span><br><span class="line">ts=2021-06-22T09:56:49.658621Z lvl=warn msg=&quot;max-values-per-tag limit may be exceeded soon&quot; log_id=0UYIcREl000 service=store perc=100% n=100000 max=100000 db_instance=xxx measurement=network/rx tag=pod_name</span><br><span class="line">ts=2021-06-22T09:56:49.658702Z lvl=warn msg=&quot;max-values-per-tag limit may be exceeded soon&quot; log_id=0UYIcREl000 service=store perc=100% n=100000 max=100000 db_instance=xxx measurement=network/rx_errors tag=pod_name</span><br><span class="line">ts=2021-06-22T09:56:49.658815Z lvl=warn msg=&quot;max-values-per-tag limit may be exceeded soon&quot; log_id=0UYIcREl000 service=store perc=100% n=100000 max=100000 db_instance=xxx measurement=network/tx tag=pod_name</span><br><span class="line">ts=2021-06-22T09:56:49.658893Z lvl=warn msg=&quot;max-values-per-tag limit may be exceeded soon&quot; log_id=0UYIcREl000 service=store perc=100% n=100000 max=100000 db_instance=xxx measurement=network/tx_errors tag=pod_name</span><br><span class="line">ts=2021-06-22T09:56:49.659062Z lvl=warn msg=&quot;max-values-per-tag limit may be exceeded soon&quot; log_id=0UYIcREl000 service=store perc=100% n=100003 max=100000 db_instance=xxx measurement=uptime tag=pod_name</span><br></pre></td></tr></table></figure>

<p>果然，有大量<code>warn</code>日志，提示<code>max-values-per-tag limit may be exceeded soon</code>，从日志可以看出，这个参数的默认值为<code>100000</code>。通过搜索，找到了这个参数引入的issue[2]，引入原因大概意思是：</p>
<blockquote>
<p>如果不小心加载了大量的cardinality数据，那么当我们删除数据的时候，InfluxDB很容易会发生OOM。</p>
</blockquote>
<p>通过临时修改<code>max-values-per-tag</code>参数，验证问题是否解决</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cat influxdb.conf</span><br><span class="line">[meta]</span><br><span class="line">  dir = &quot;/var/lib/influxdb/meta&quot;</span><br><span class="line">[data]</span><br><span class="line">  dir = &quot;/var/lib/influxdb/data&quot;</span><br><span class="line">  engine = &quot;tsm1&quot;</span><br><span class="line">  wal-dir = &quot;/var/lib/influxdb/wal&quot;</span><br><span class="line">  max-series-per-database = 0</span><br><span class="line">  max-values-per-tag = 0</span><br><span class="line">[http]</span><br><span class="line">  auth-enabled = true</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete pod -n kube-system influxdb-rs1-6ff9466d46-q6w5r</span><br><span class="line">pod &quot;influxdb-rs1-6ff9466d46-q6w5r&quot; deleted</span><br><span class="line"></span><br><span class="line">kubectl delete pod -n kube-system influxdb-rs2-d6d6697f5-zzcnk</span><br><span class="line">pod &quot;influxdb-rs2-d6d6697f5-zzcnk&quot; deleted</span><br><span class="line"></span><br><span class="line">kubectl delete pod -n kube-system influxdb-rs3-65ddfc7476-hxhr8</span><br><span class="line">pod &quot;influxdb-rs3-65ddfc7476-hxhr8&quot; deleted</span><br></pre></td></tr></table></figure>

<p>再次观察业务层面获取的<code>Influxdb</code>监控数据，最终CPU、内存和磁盘使用率正常获取。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">监控项         使用率</span><br><span class="line">CPU(核)        19%</span><br><span class="line">内存(GB)       22%</span><br><span class="line">磁盘空间(GB)    2%</span><br></pre></td></tr></table></figure>

<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>根据业务情况，将<code>influxdb</code>的<code>max-values-per-tag</code>参数调整到合适值。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/influxdata/influxdb-relay">https://github.com/influxdata/influxdb-relay</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/influxdata/influxdb/issues/7146">https://github.com/influxdata/influxdb/issues/7146</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/06/26/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Pod%E9%97%B4%E9%80%9A%E8%BF%87%E6%9C%8D%E5%8A%A1%E5%90%8D%E8%AE%BF%E9%97%AE%E5%BC%82%E5%B8%B8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/26/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Pod%E9%97%B4%E9%80%9A%E8%BF%87%E6%9C%8D%E5%8A%A1%E5%90%8D%E8%AE%BF%E9%97%AE%E5%BC%82%E5%B8%B8/" class="post-title-link" itemprop="url">K8S问题排查-Pod间通过服务名访问异常</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-26 16:27:14" itemprop="dateCreated datePublished" datetime="2021-06-26T16:27:14+00:00">2021-06-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-14 02:09:04" itemprop="dateModified" datetime="2024-04-14T02:09:04+00:00">2024-04-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>K8S集群内，PodA使用服务名称访问PodB，请求出现异常。其中，PodA在<code>node1</code>节点上，PodB在<code>node2</code>节点上。</p>
<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>先上<code>tcpdump</code>，观察请求是否有异常：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# tcpdump -n -i ens192 port 50300</span><br><span class="line">...</span><br><span class="line">13:48:17.630335 IP 177.177.176.150.distinct -&gt; 10.96.22.136.50300:  UDP, length 214</span><br><span class="line">13:48:17.630407 IP 192.168.7.21.distinct  -&gt;  10.96.22.136.50300:   UDP, length 214</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>从抓包数据可以看出，请求源地址端口号为<code>177.177.176.150:50901</code>，目标地址端口号为<code>10.96.22.136:50300 </code>，其中<code>10.96.22.136</code>是PodA使用<code>server-svc</code>这个<code>serviceName</code>请求得到的目的地址，也就是<code>server-svc</code>对应的<code>serviceIP</code>，那就确认一下这个地址有没有问题：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl get pod -A -owide|grep server</span><br><span class="line">ss  server-xxx-xxx  1/1  Running 0 20h  177.177.176.150  node1</span><br><span class="line">ss  server-xxx-xxx  1/1  Running 0 20h  177.177.254.245  node2</span><br><span class="line">ss  server-xxx-xxx  1/1  Running 0 20h  177.177.18.152   node3</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl get svc -A -owide|grep server</span><br><span class="line">ss  server-svc  ClusterIP  10.96.182.195 &lt;none&gt;  50300/UDP</span><br></pre></td></tr></table></figure>

<p>可以看出，源地址没有问题，但目标地址跟预期不符，实际查到的服务名<code>server-svc</code>对应的地址为<code>10.96.182.195</code>，这是怎么回事儿呢？我们知道，K8S从v1.13版本开始默认使用<code>CoreDNS</code>作为服务发现，PodA使用服务名<code>server-svc</code>发起请求时，需要经过<code>CoreDNS</code>的解析，将服务名解析为<code>serviceIP</code>，那就登录到PodA内，验证域名解析是不是有问题：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl exec -it -n ss server-xxx-xxx -- cat /etc/resolve.conf</span><br><span class="line">nameserver 10.96.0.10</span><br><span class="line">search ss.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# kubectl exec -it -n ss server-xxx-xxx -- nslookup server-svc</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line"></span><br><span class="line">Name:    ss</span><br><span class="line">Address: 10.96.182.195</span><br></pre></td></tr></table></figure>

<p>从查看结果看，域名解析没有问题，PodA内也可以正确解析出<code>server-svc</code>对应的<code>serviceIP</code>为<code>10.96.182.195</code>，那最初使用<code>tcpdump</code>命令抓到的<code>serviceIP</code> 为<code>10.96.22.136</code>，难道这个地址是其他业务的服务，或者是残留的iptables规则，或者是有什么相关路由？分别查一下看看：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl get svc -A -owide|grep 10.96.22.136</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# iptables-save|grep 10.96.22.136</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# ip route|grep 10.96.22.136</span><br></pre></td></tr></table></figure>

<p>结果是，集群上根本不存在<code>10.96.22.136</code>这个地址，那PodA请求的目标地址为什么是它？既然主机上抓包时，目标地址已经是<code>10.96.22.136</code>，那再确认下出PodA时目标地址是什么：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# ip route|grep 177.177.176.150</span><br><span class="line">177.177.176.150 dev cali9afa4438787 scope link</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# tcpdump -n -i cali9afa4438787 port 50300</span><br><span class="line">...</span><br><span class="line">14:16:40.821511 IP 177.177.176.150.50902 -&gt;  10.96.22.136.50300:  UDP, length 214</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>原来出PodA时，目标地址已经是错误的<code>serviceIP</code>。而结合上面的域名解析的验证结果看，请求出PodA时的域名解析应该不存在问题。综合上面的定位情况，基本可以推测出，<strong>问题出在发送方</strong>。</p>
<p>为了进一步区分出，是PodA内的所有发送请求都存在问题，还是只有业务自身的发送请求存在问题，我们使用<code>nc</code>命令在PodA内模拟发送一个<code>UDP</code>数据包，然后在主机上抓包验证（PodA内恰巧有<code>nc</code>命令，如果没有，感兴趣的同学可以使用&#x2F;dev&#x2F;{tcp|udp}模拟[1]）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl exec -it -n ss server-xxx-xxx -- echo “test” | nc -u server-svc 50300 -p 9999</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# tcpdump -n -i cali9afa4438787 port 50300</span><br><span class="line">...</span><br><span class="line">15:46:45.871580 IP 177.177.176.150.50902 -&gt;  10.96.182.195.50300:  UDP, length 54</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>可以看出，PodA内模拟发送的请求，目标地址是可以正确解析的，也就把问题限定在了<strong>业务自身的发送请求存在问题</strong>。因为问题是服务名没有解析为正确的IP地址，所以怀疑是业务使用了什么缓存，如果猜想正确，那么重启PodA，理论上可以解决。而考虑到业务是多副本的，我们重启其中一个，其他副本上的问题环境还可以保留，跟开发沟通后重启并验证业务的请求：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# docker ps |grep server-xxx-xxx | grep -v POD |awk &#x27;&#123;print $1&#125;&#x27; |xargs docker restart</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# tcpdump -n -i ens192 port 50300</span><br><span class="line">...</span><br><span class="line">15:58:17.150535 IP 177.177.176.150.distinct -&gt; 10.96.182.195.50300:  UDP, length 214</span><br><span class="line">15:58:17.150607 IP 192.168.7.21.distinct  -&gt;  10.96.182.195.50300:   UDP, length 214</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>验证符合预期，进一步证明了业务可能是使用了什么缓存。与开发同学了解，业务的发送使用的是java原生的API发送<code>UDP</code>数据，会不会是java在使用域名建立socket时默认会做缓存呢？</p>
<p>通过一番搜索，找了一篇相关博客[2]，关键内容附上：</p>
<blockquote>
<p>在通过DNS查找域名的过程中，可能会经过多台中间DNS服务器才能找到指定的域名，因此，在DNS服务器上查找域名是非常昂贵的操作。在Java中为了缓解这个问题，提供了DNS缓存。当InetAddress类第一次使用某个域名创建InetAddress对象后，JVM就会将这个域名和它从DNS上获得的信息（如IP地址）都保存在DNS缓存中。当下一次InetAddress类再使用这个域名时，就直接从DNS缓存里获得所需的信息，而无需再访问DNS服务器。</p>
</blockquote>
<p>还真是，继续看怎么解决：</p>
<blockquote>
<p>DNS缓存在默认时将永远保留曾经访问过的域名信息，但我们可以修改这个默认值。一般有两种方法可以修改这个默认值：</p>
<ol>
<li><p>在程序中通过java.security.Security.setProperty方法设置安全属性networkaddress.cache.ttl的值（单位：秒）</p>
</li>
<li><p>设置java.security文件中的networkaddress.cache.negative.ttl属性。假设JDK的安装目录是C:&#x2F;jdk1.6，那么java.security文件位于c:&#x2F;jdk1.6&#x2F;jre&#x2F;lib&#x2F;security目录中。打开这个文件，找到networkaddress.cache.ttl属性，并将这个属性值设为相应的缓存超时（单位：秒）</p>
</li>
</ol>
<p> 注：如果将networkaddress.cache.ttl属性值设为-1，那么DNS缓存数据将永远不会释放。</p>
</blockquote>
<p>至此，问题定位结束。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>业务侧根据业务场景调整DNS缓存的设置。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/michaelwoshi/article/details/101107042">https://blog.csdn.net/michaelwoshi/article/details/101107042</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/turkeyzhou/article/details/5510960">https://blog.csdn.net/turkeyzhou/article/details/5510960</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">LeaoYao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">88</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LeaoYao</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
