<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lyyao09.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="云原生知识星球">
<meta property="og:url" content="https://lyyao09.github.io/page/3/index.html">
<meta property="og:site_name" content="云原生知识星球">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="LeaoYao">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://lyyao09.github.io/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>云原生知识星球</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">云原生知识星球</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/10/24/share/%E6%80%BB%E7%BB%93%E5%88%86%E4%BA%AB-Wireshark%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/24/share/%E6%80%BB%E7%BB%93%E5%88%86%E4%BA%AB-Wireshark%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">总结分享-Wireshark常用命令总结</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-10-24 21:43:43" itemprop="dateCreated datePublished" datetime="2021-10-24T21:43:43+00:00">2021-10-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-07 09:04:28" itemprop="dateModified" datetime="2024-04-07T09:04:28+00:00">2024-04-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/share/" itemprop="url" rel="index"><span itemprop="name">share</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="功能总结"><a href="#功能总结" class="headerlink" title="功能总结"></a>功能总结</h2><h3 id="导出数据包"><a href="#导出数据包" class="headerlink" title="导出数据包"></a>导出数据包</h3><p>方法：File | Export Packet Dissections | as”CSV”(Comma Separated Values packet summary)file…</p>
<blockquote>
<ol>
<li>导出格式有纯文本、CSV、XML、JSON等；</li>
<li>不仅可以输出选中列，还可以<strong>输出当前页面展示的列</strong>，以及所有列（在导出弹框中设置）；</li>
<li>可以设置包格式，比如导出统计行、统计头、包详细数据等；</li>
</ol>
</blockquote>
<h3 id="添加展示列"><a href="#添加展示列" class="headerlink" title="添加展示列"></a><strong>添加展示列</strong></h3><p>方法：Package Details 面板中展开包详情，找到指定字段右键单击并选择Apply as Column 选项</p>
<h3 id="显示一个TCP-UDP-会话"><a href="#显示一个TCP-UDP-会话" class="headerlink" title="显示一个TCP&#x2F;UDP 会话"></a>显示一个TCP&#x2F;UDP 会话</h3><p>方法1：选中一个包，右键选择Conversation Filter|[TCPIUDP]命令</p>
<p>方法2：选中一个包，右键选择Follow[TCPIUDP] Stream 命令</p>
<p>方法3：工具栏选择Statistics|Conversations命令</p>
<p>方法4：在TCP 头部，通过右键单击stream index 字段并选择Apply as Filter 命令</p>
<h2 id="命令总结"><a href="#命令总结" class="headerlink" title="命令总结"></a>命令总结</h2><h3 id="捕获过滤命令"><a href="#捕获过滤命令" class="headerlink" title="捕获过滤命令"></a>捕获过滤命令</h3><p>捕获过滤器仅支持协议过滤。</p>
<h4 id="1-主机相关过滤命令"><a href="#1-主机相关过滤命令" class="headerlink" title="1. 主机相关过滤命令*"></a>1. 主机相关过滤命令*</h4><ul>
<li>host 10.3.1.1: 捕获到达&#x2F;来自10.3.1.1主机的数据（支持IPv6地址）。</li>
<li>not host 10.3.1.1: 捕获除了到达&#x2F;来自10.3.1.1主机的所有数据。</li>
<li>src host 10.3.1.1: 捕获来自10.3.1.1 主机上的数据。</li>
<li>dst host 10.3.1.1: 捕获到达10.3.1.1 主机上的数据。</li>
<li>host 10.3.1.1 or host 10.3.1.2: 捕获到达&#x2F;来自10.3.1.1主机上的数据，和到达&#x2F;来自10.3.1.2 主机的数据。</li>
</ul>
<h4 id="2-端口相关过滤命令"><a href="#2-端口相关过滤命令" class="headerlink" title="2. 端口相关过滤命令*"></a>2. 端口相关过滤命令*</h4><ul>
<li>port 53: 捕获到达&#x2F;来自端口号为53的UDP&#x2F;TCP 数据（典型的DNS 数据）。</li>
<li>not port 53 : 捕获除到达&#x2F;来自端口号为53的所有UDP&#x2F;TCP 数据。</li>
<li>port 80: 捕获到达&#x2F;来自端口号为80的UDP&#x2F;TCP 数据（典型的HTTP 数据）。</li>
<li>udp port 67 : 捕获到达&#x2F;来自端口号为67的UDP 数据（典型的DHCP 数据）。</li>
<li>tcp port 21: 捕获到达&#x2F;来自端口号为21的TCP 数据（典型的FTP 数据）。</li>
<li>portrange 1-80: 捕获到达&#x2F;来自1~80端口号的UDP&#x2F;TCP 数据。</li>
<li>tcp portrange 1-80: 捕获到达&#x2F;来自1~80端口号的TCP 数据。</li>
</ul>
<h4 id="3-主机和端口混合过滤命令"><a href="#3-主机和端口混合过滤命令" class="headerlink" title="3. 主机和端口混合过滤命令*"></a>3. 主机和端口混合过滤命令*</h4><ul>
<li>port 20 or port 21 :捕获到达&#x2F;来自20 或21 端口号的所有UDP&#x2F;TCP 数据。</li>
<li>host 10.3.1.1 and port 80: 捕获到达&#x2F;来自端口号为80, 并且是到达&#x2F;来自10.3.1.1主机的UDP&#x2F;TCP 数据。</li>
<li>host 10.3.1.1 and not port 80: 捕获到I来自10.3.1.1 主机，并且是非80 端口的UDP&#x2F;TCP 数据。</li>
<li>udp src port 68 and udp dst port 67: 捕获来自端口为68, 目标端口号为67 的所有UDP 数据（典型的DHCP 客户端到DHCP 服务器的数据） 。</li>
<li>udp src port 67 and udp dst port 68: 捕获来自端口号为67, 目标端口号为68 的所有UDP 数据（典型的DHCP 服务器到DHCP 客户端的数据）。</li>
</ul>
<h4 id="4-IP地址范围过滤命令"><a href="#4-IP地址范围过滤命令" class="headerlink" title="4. IP地址范围过滤命令"></a>4. IP地址范围过滤命令</h4><ul>
<li>net 192.168.0.0&#x2F;24：捕获到达&#x2F;来自192.168.0.0网络中任何主机的数据。</li>
<li>net 192.168.0.0 mask 255.255.255.0: 捕获到达&#x2F;来自192.168.0.0网络中任何主机的<br>数据。</li>
<li>ip6 net 2406:daOO:ff00::&#x2F;64: 捕获到达&#x2F;来自2406:daOO:ffDO:OOOO ( IPv6) 网络中任<br>何主机的数据。</li>
<li>not dst net 192.168.0.0&#x2F;24: 捕获除目的IP地址是192.168.0.0网络外的所有数据。</li>
<li>dst net 192.168.0.0&#x2F;24：捕获到达IP地址为192.168.0.0网络内的所有数据。</li>
<li>src net 192.168.0.0&#x2F;24: 捕获来自IP地址为192.168.0.0网络内的所有数据。</li>
</ul>
<h4 id="5-广播或多播地址过滤命令"><a href="#5-广播或多播地址过滤命令" class="headerlink" title="5. 广播或多播地址过滤命令"></a>5. 广播或多播地址过滤命令</h4><ul>
<li>ip broadcast: 捕获到255.255.255.255 的数据。</li>
<li>ip multicast: 捕获通过224.0.0.0~239.255.255.255的数据。</li>
<li>dst host ff02::1: 捕获所有主机到IPv6多播地址的数据。</li>
<li>dst host ff02::2: 捕获所有路由到IPv6多播地址的数据。（<em>跟上一个有什么区别？</em>）</li>
</ul>
<h4 id="6-MAC地址过滤命令"><a href="#6-MAC地址过滤命令" class="headerlink" title="6. MAC地址过滤命令"></a>6. MAC地址过滤命令</h4><ul>
<li>ether host 00:08:15:00:08:15: 捕获到达&#x2F;来自00:08:15:00:08:15主机的数据。</li>
<li>ether src 02:0A:42:23:41:AC: 捕获来自02:0A:42:23:41:AC 主机的数据。</li>
<li>ether dst 02:0A:42:23:41:AC: 捕获到达02:0A:42:23:41:AC 主机的数据。</li>
<li>not ether host 00:08:15:00:08:15:捕获到达&#x2F;来自除了00:08:15:00:08:15的任何MAC<br>地址的流量。</li>
</ul>
<h4 id="7-特定ICMP协议过滤命令"><a href="#7-特定ICMP协议过滤命令" class="headerlink" title="7. 特定ICMP协议过滤命令"></a>7. 特定ICMP协议过滤命令</h4><ul>
<li>icmp：捕获所有ICMP 数据包。</li>
<li>icmp[0]&#x3D;8 : 捕获所有ICMP 字段类型为8 (Echo Request) 的数据包。</li>
<li>icmp[0]&#x3D;17: 捕获所有ICMP 字段类型为17 (Address Mask Request) 的数据包。</li>
<li>icmp[0]&#x3D;8 or icmp[0]&#x3D;0: 捕获所有ICMP 字段类型为8 (Echo Request) 或ICMP<br>字段类型为0 (Echo Reply) 的数据包。</li>
<li>icmp[0]&#x3D;3 and not icmp[1]&#x3D;4 ：捕获所有ICMP 字段类型为3 (Destination<br>Unreachable) 的包，除了ICMP 字段类型为3&#x2F;代码为4 (Fragmentation Needed and<br>Don’t Fragment was Set) 的数据包。</li>
</ul>
<h3 id="显示过滤命令"><a href="#显示过滤命令" class="headerlink" title="显示过滤命令"></a>显示过滤命令</h3><p>显示过滤器可以帮助用户在捕捉结果中进行数据查找。该过滤器可以在得到的捕捉结果中修改，以显示有用数据。</p>
<p>既支持协议过滤也支持内容过滤。</p>
<h4 id="1-通用语法格式"><a href="#1-通用语法格式" class="headerlink" title="1. 通用语法格式"></a>1. 通用语法格式</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Protocol | [String1] [String2] | Comparison-Operator | Value | Logical-Operations | Other-expression</span><br><span class="line">协议（2~7层）      协议子类               比较运算符         比较值       逻辑运算符             其他表达式</span><br></pre></td></tr></table></figure>

<p>其中比较运算符有如下6个：</p>
<table>
<thead>
<tr>
<th>英文写法</th>
<th>C 语言写法</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>eq</td>
<td>&#x3D;&#x3D;</td>
<td>等于</td>
</tr>
<tr>
<td>ne</td>
<td>!&#x3D;</td>
<td>不等于</td>
</tr>
<tr>
<td>gt</td>
<td>&gt;</td>
<td>大于</td>
</tr>
<tr>
<td>lt</td>
<td>&lt;</td>
<td>小于</td>
</tr>
<tr>
<td>ge</td>
<td>&gt;&#x3D;</td>
<td>大于等于</td>
</tr>
<tr>
<td>le</td>
<td>&lt;&#x3D;</td>
<td>小于等于</td>
</tr>
<tr>
<td>contains</td>
<td>-</td>
<td>包含</td>
</tr>
<tr>
<td>matches</td>
<td>-</td>
<td>匹配</td>
</tr>
</tbody></table>
<p>逻辑运算符有如下4个：</p>
<table>
<thead>
<tr>
<th>英文写法</th>
<th>C 语言写法</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>and</td>
<td>&amp;&amp;</td>
<td>逻辑与</td>
</tr>
<tr>
<td>or</td>
<td>||</td>
<td>逻辑或</td>
</tr>
<tr>
<td>xor</td>
<td>^^</td>
<td>逻辑异或</td>
</tr>
<tr>
<td>not</td>
<td>!</td>
<td>逻辑非</td>
</tr>
</tbody></table>
<h4 id="2-协议过滤命令"><a href="#2-协议过滤命令" class="headerlink" title="2. 协议过滤命令*"></a>2. 协议过滤命令*</h4><ul>
<li>arp: 显示所有ARP 流量，包括免费ARP 、ARP 请求和ARP 应答。</li>
<li>ip(v6): 显示所有IPv4&#x2F;IPv6 流量，包括有IPv4(IPv6) 头部嵌入式的包（如ICMP 目标不可达的数据包，返回到ICMP 头后进入到IPv4 头部）。<ul>
<li>ip(v6).src</li>
<li>ip(v6).dst</li>
<li>ip(v6).host</li>
<li>ip(v6).addr</li>
</ul>
</li>
<li>tcp: 显示所有基于TCP 的流量数据。</li>
</ul>
<h4 id="3-应用过滤命令"><a href="#3-应用过滤命令" class="headerlink" title="3. 应用过滤命令*"></a>3. 应用过滤命令*</h4><ul>
<li>bootp: 显示所有DHCP 流量（ipv4下基于BOOTP，ipv6下不是基于BOOTP，过滤时使用dhcpv6) 。</li>
<li>dns: 显示所有DNS 流量，包括基于TCP 传输和UDP 的DNS 请求和响应。</li>
<li>tftp: 显示所有TFTP （简单文件传输协议）流量。</li>
<li>http: 显示所有HTTP 命令、响应和数据传输包。但是不显示TCP 握手包、TCP确认包或TCP 断开连接的包。</li>
<li>http contains “GET”: 显示HTTP 客户端发送给HTTP 服务器的所有GET 请求数据。</li>
<li>icmp: 显示所有ICMP 流量。</li>
</ul>
<h4 id="4-字段存在过滤命令"><a href="#4-字段存在过滤命令" class="headerlink" title="4. 字段存在过滤命令"></a>4. 字段存在过滤命令</h4><ul>
<li>bootp.option.hostname: 显示所有DHCP 流量，包含主机名( DHCP 是基于BOOTP) 。</li>
<li>http.host: 显示所有包含有HTTP 主机名字段的HTTP 包。该包通常是由客户端发送给一个Web 服务器的请求。</li>
<li>ftp.request.command: 显示所有FTP 命令数据，如USER 、PASS 或RETR 命令。</li>
<li>ftp.request.arg matches “admin”: 显示匹配admin 字符串的数据。</li>
<li>tcp.analysis.flags: 显示所有与TCP 标识相关的包，包括丢包、重发或者零窗口标志。</li>
<li>tcp.analysis.zero_window: 显示被标志的包，来表示发送方的缓冲空间已满。</li>
</ul>
<h4 id="5-逻辑运算过滤命令"><a href="#5-逻辑运算过滤命令" class="headerlink" title="5. 逻辑运算过滤命令"></a>5. 逻辑运算过滤命令</h4><ul>
<li>&amp;＆或and: ip.src&#x3D;l0.2.2.2 &amp;&amp; tcp.port&#x3D;80，表示显示源地址10.2.2.2 主机，并且端口号为80 的所有IPv4 流量。</li>
<li>||或or: tcp.port&#x3D;80 || tcp.port&#x3D;43，表示显示到达&#x2F;来自80 或443 端口的所有TCP数据。</li>
<li>！或not: !arp，表示查看除ARP 外的所有数据。</li>
<li>!＝或ne: tcp.flags.syn !&#x3D; 1，表示查看TCP SYN 标志位不是1 的TCP 数据帧。</li>
</ul>
<blockquote>
<p>注：</p>
<p>ip.addr !&#x3D; 10.2.2.2  表示显示IP 源或目标地址字段非10.2.2.2 的数据包。如果一个包的源或目标IP 地址字段中不包含10.2.2.2, 则显示该数据包。在该语法中使用了一个隐含或，并且不会过滤掉任何数据包。</p>
<p>!ip.addr &#x3D;&#x3D; 10.2.2.2 表示显示在IP 源和目标地址字段不包含10.2.2.2 的数据包。当排除到达&#x2F;来自一个特定IP 地址的数据时，这是一个合适的过滤器语法。</p>
<p>!tcp.flags.syn&#x3D;&#x3D;l 表示显示TCP SYN 标志位不等于1的所有TCP 包和其他协议包，如UDP 、ARP数据包将匹配该过滤器。因为UDP 和ARP 协议中没有TCP SYN 标志位为1 的数据包。</p>
<p>tcp.flags.syn !&#x3D; 1 表示仅显示包括SYN 设置为0 的TCP 包。</p>
</blockquote>
<h4 id="6-时间过滤命令"><a href="#6-时间过滤命令" class="headerlink" title="6. 时间过滤命令"></a>6. 时间过滤命令</h4><ul>
<li>frame.time_delta &gt; 1，表示时间延迟超过1 秒的数据，显示捕获文件中所有包的时间。</li>
<li>tcp.time_delta &gt; 1，表示TCP 时间差大于1 秒的数据。</li>
</ul>
<blockquote>
<p>注：上述命令主要用于判断各种网络延迟。</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/09/26/linux/%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-IPv6%E7%8E%AF%E5%A2%83%E4%B8%8BVIP%E5%9C%B0%E5%9D%80%E4%B8%8D%E9%80%9A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/26/linux/%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-IPv6%E7%8E%AF%E5%A2%83%E4%B8%8BVIP%E5%9C%B0%E5%9D%80%E4%B8%8D%E9%80%9A/" class="post-title-link" itemprop="url">网络问题排查-IPv6环境下VIP地址不通</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-09-26 09:15:21" itemprop="dateCreated datePublished" datetime="2021-09-26T09:15:21+00:00">2021-09-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-07 09:04:28" itemprop="dateModified" datetime="2024-04-07T09:04:28+00:00">2024-04-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/linux/" itemprop="url" rel="index"><span itemprop="name">linux</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>IPv6环境下，在浏览器中通过<code>http://[vip:port]</code>访问<code>web</code>业务，提示无法访问此网站，<code>[vip]</code>的响应时间过长。</p>
<h2 id="分析过程"><a href="#分析过程" class="headerlink" title="分析过程"></a>分析过程</h2><p>之前碰到过多次在PC浏览器上无法访问<code>vip</code>的情况，排查方法也很明确：</p>
<ol>
<li>在集群的<code>vip</code>所在节点上访问是否正常；</li>
<li>在集群范围内其他节点上访问是否正常；</li>
<li>在集群之外的同网段<code>linux</code>环境上访问是否正常；</li>
<li>在其他环境的PC浏览器上访问是否正常；</li>
</ol>
<p>验证发现，直接在<code>vip</code>所在节点上访问竟然不通！登录<code>vip</code>所在节点执行<code>ip addr</code>可以看到该地址确实是正确配置了，但 <code>ping6</code>该地址无回应，对应的<code>ipv4</code>地址 <code>ping</code>有回应。按说<code>ping</code>本机的地址不应该和链路的状态有关系，那会是什么原因呢？在仔细检查地址配置情况后发现该地址有个标记<code>tentative dadfailed </code>；</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">17: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 0c:da:41:1d:a8:62 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.10.10.17/16 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 2000::10:18/128 scope global tentative dadfailed</span><br><span class="line">       valid_lft forever preferred_lft 0sec</span><br><span class="line">    inet6 fe80::eda:41ff:fe1d:a862/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>

<p><em><a target="_blank" rel="noopener" href="https://manpages.debian.org/ip-address(8)">ip-address(8)</a></em> 查到对该标记的解释如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tentative</span><br><span class="line">   (IPv6 only) only list addresses which have not yet passed duplicate address detection.</span><br></pre></td></tr></table></figure>

<p>显然该地址没有通过地址重复探测（<code>duplicate address detection</code>，简称<code>dad</code>），而且这种检查机制只针对<code>IPv6</code>。<strong>经确认，该环境的<code>IPv6</code>网段只有自己在用，且未手工配置过<code>IPv6</code>地址，但该环境曾经发生过切主</strong>；</p>
<p>至此问题基本明确了，切主时会把老的主节点上的<code>vip</code>删除，再到新的主节点上把<code>vip</code>添加上去。如果一切正常，按照这个顺序切主没有问题，但也存在某些异常情况（比如老主上的<code>vip</code>没有及时删掉，而新主上已经添加好了），此时就会触发<code>dad</code>机制。经过验证，一旦出现<code>dadfailed</code>，即使地址冲突解决了，该地址依然无法访问；</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>方案1：在<code>sysctl</code>配置中增加如下内核参数：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net.ipv6.conf.all.accept_dad = 0</span><br><span class="line">net.ipv6.conf.default.accept_dad = 0</span><br><span class="line">net.ipv6.conf.eth0.accept_dad = 0</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">IPv6 Privacy Extensions (RFC 4941)</span></span><br><span class="line">net.ipv6.conf.all.use_tempaddr = 0</span><br><span class="line">net.ipv6.conf.default.use_tempaddr = 0</span><br></pre></td></tr></table></figure>

<p>方案2：在<code>ip addr add</code>命令执行时增加<code>nodad</code>标识：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip addr add 2000::10:18/128 dev eth0 nodad</span><br></pre></td></tr></table></figure>

<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://blog.clanzx.net/network/ipv6-dad.html">https://blog.clanzx.net/network/ipv6-dad.html</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/09/25/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E9%99%84%E5%8A%A0%E7%BD%91%E7%BB%9CPod%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/25/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E9%99%84%E5%8A%A0%E7%BD%91%E7%BB%9CPod%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8/" class="post-title-link" itemprop="url">K8S问题排查-附加网络Pod无法启动</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-09-25 19:21:50" itemprop="dateCreated datePublished" datetime="2021-09-25T19:21:50+00:00">2021-09-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-07 09:04:28" itemprop="dateModified" datetime="2024-04-07T09:04:28+00:00">2024-04-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>使用附加网络的Pod在服务器重启后启动异常，报错信息如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Events:</span><br><span class="line">Type 		Reason 				  Age 		From 			Message</span><br><span class="line">Normal 		Scheduled 			  53m 		default-scheduler Successfully assigned xxx/xxx1-64784c458b-q67tx to node001</span><br><span class="line">Warning 	FailedCreatePodSandBox 53m 		 kubelet, node001   Failed to create pod sandbox: rpc er or: code = Unknown desc = failed to set up sandbox container &quot;xxx&quot; network for pod &quot;xxxl-64784c458b-q67tx&quot;: NetworkPlugin cni failed to set up pod &quot;xxx1-64784c458b-q67tx_xxx&quot; network: Multus: Err adding pod to network &quot;net-netl-nodeOOl&quot;: Multus: error in invoke Delegate add - &quot;macvlan&quot;: failed to create macvlan: device or resource busy</span><br><span class="line">Warning 	FailedCreatePodSandBox 53m 		 kubelet, node001   Failed to create pod sandbox: rpc er or: code = Unknown desc = failed to set up sandbox container &quot;xxx&quot; network for pod &quot;xxxl-64784c458b-q67tx&quot;: NetworkPlugin cni failed to set up pod &quot;xxx1-64784c458b-q67tx_xxx&quot; network: Multus: Err adding pod to network &quot;net-netl-nodeOOl&quot;: Multus: error in invoke Delegate add - &quot;macvlan&quot;: failed to create macvlan: device or resource busy</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h2 id="分析过程"><a href="#分析过程" class="headerlink" title="分析过程"></a>分析过程</h2><p>从日志初步看，创建Pod的<code>sandbox</code>异常，具体是Multus无法将Pod添加到<code>net-netl-nodeOOl</code>网络命名空间内，再具体点是Multus无法创建<code>macvlan</code>网络，原因是<code>device or resource busy</code>；</p>
<p>最后的这个错误信息还是比较常见的，从字面理解，就是设备或资源忙，常见于共享存储的卸载场景。那这里也应该类似，有什么设备或资源处于被占用状态，所以执行<code>macvlan</code>的创建失败，既然是附加网络的问题，那优先查看了下附加网络相关的CRD资源，没什么异常；</p>
<p>网上根据日志搜索一番，也没有什么比较相关的问题，那就看代码吧，首先找到Multus的源码，根据上述日志找相关处理逻辑，没有找到。再一想，Multus实现<code>macvlan</code>网络使用的是<code>macvlan</code>插件，再下载插件代码，找到了相关处理逻辑：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">plugins/main/macvlan/macvlan.<span class="keyword">go</span>:<span class="number">169</span></span><br><span class="line"><span class="keyword">if</span> err := netlink.LinkAdd(mv); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">&quot;failed to create macvlan: %v&quot;</span>, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// LinkAdd adds a new link device. The type and features of the device</span></span><br><span class="line"><span class="comment">// are taken from the parameters in the link object.</span></span><br><span class="line"><span class="comment">// Equivalent to: `ip link add $link`</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">LinkAdd</span><span class="params">(link Link)</span></span> <span class="type">error</span> &#123;</span><br><span class="line">	<span class="keyword">return</span> pkgHandle.LinkAdd(link)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// LinkAdd adds a new link device. The type and features of the device</span></span><br><span class="line"><span class="comment">// are taken from the parameters in the link object.</span></span><br><span class="line"><span class="comment">// Equivalent to: `ip link add $link`</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h *Handle)</span></span> LinkAdd(link Link) <span class="type">error</span> &#123;</span><br><span class="line">	<span class="keyword">return</span> h.linkModify(link, unix.NLM_F_CREATE|unix.NLM_F_EXCL|unix.NLM_F_ACK)</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>根据上述代码和注释简单的看，是在执行<code>ip link add $link</code>命令时报错，实际验证看看：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node001 ~] ip link add link bond1 name macvlan1 type macvlan mode bridge</span><br><span class="line">RTNETLINK answers: Device or resource busy</span><br></pre></td></tr></table></figure>

<p>确实如此，在<code>bond1</code>接口上无法配置<code>macvlan</code>，那换一个接口试试：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node001 ~] ip link add link bond0 name macvlan1 type macvlan mode bridge</span><br><span class="line">[root@node001 ~] ip link show</span><br><span class="line">...</span><br><span class="line">110: macvlan1@bond0: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether ea:31:c9:7f:d9:a4 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>配置成功，说明<code>bond1</code>接口有什么问题，看看这俩接口有没有差异：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@node001 ~] ip addr show</span><br><span class="line">...</span><br><span class="line">2: bond0: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000</span><br><span class="line">    link/ether 0c:da:41:1d:6f:ca brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet x.x.x.x/16 brd x.x.255.255 scope global bond0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::eda:41ff:fe1d:6fca/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">...</span><br><span class="line">17: bond1: &lt;BROADCAST,MULTICAST,MASTER,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000</span><br><span class="line">    link/ether 0c:da:41:1d:a8:62 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>对比两个接口可以发现两个差异点：</p>
<ol>
<li><code>bond0</code>配置了IP地址，而<code>bond1</code>没有配置；</li>
<li><code>bond0</code>是MASTER角色，<code>bond1</code>既是MASTER，又是SLAVE角色；</li>
</ol>
<p>考虑到<code>bond0</code>接口是用来建集群的，<code>bond1</code>接口是给<code>Multus</code>创建<code>macvlan</code>网络用的，所以第一个差异点属于正常现象。第二个是什么情况呢？一般来说，配置<code>bond</code>的目的是把几个物理接口作为SLAVE角色聚合成<code>bond</code>接口，这样既能增加服务器的可靠性，又增加了可用网络宽带，为用户提供不间断的网络服务。配置后，实际的物理接口应该是SLAVE角色，而聚合后的<code>bond</code>接口应该是MASTER角色，所以正常来说，不会同时出现两个角色才对；</p>
<p>查看两个<code>bond</code>的相关配置，没有发现什么异常，反过来讲，如果配置的有问题，那初次部署就应该报错了，而不是重启节点才发现。所以，<strong>问题的关键是重启导致的</strong>。也就是说，可能是在重启后的启动脚本里加了什么配置影响的；</p>
<p>搜索相关资料[1]，发现在配置过程中可能有这么一个操作：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">4、在/etc/rc.d/rc.local文件中加入如下语句，使系统启动自动运行</span><br><span class="line">ifenslave  bond0  eth0  eth1</span><br></pre></td></tr></table></figure>

<p>查看问题环境上怎么配置的：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node001 ~] cat /etc/rc.local</span><br><span class="line">...</span><br><span class="line">touch /var/lock/subsys/local</span><br><span class="line">ifenslave bond0 bond1 enp661s0f0 enp661s0f1 ens1f0 ens1f1</span><br></pre></td></tr></table></figure>

<p>发现有类似的配置，但不同的是，问题环境上配置了两个<code>bond</code>，并且配置在了一个命令里。感觉不是太对，个人理解这么配置应该会把<code>bond1</code>也认为是<code>bond0</code>的SLAVE，修改一下试试：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node001 ~] cat /etc/rc.local</span><br><span class="line">...</span><br><span class="line">touch /var/lock/subsys/local</span><br><span class="line">ifenslave bond0 enp661s0f0 enp661s0f1</span><br><span class="line">ifenslave bond1 ens1f0 ens1f1</span><br><span class="line">[root@node001 ~] systemctl restart network</span><br></pre></td></tr></table></figure>

<p>再观察两个bond接口的角色，发现恢复正常，再看看异常Pod，也都起来了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node001 ~] kubectl get pod -A |grep -v Running</span><br><span class="line">NAMESPACE		NAME		READY		STATUS		RESTARTS		AGE</span><br></pre></td></tr></table></figure>

<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>将<code>rc.local</code>里的两个<code>bond</code>的命令拆开分别配置即可。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/geaozhang/p/6763876.html">https://www.cnblogs.com/geaozhang/p/6763876.html</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/08/14/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E5%88%A0%E9%99%A4Pod%E5%90%8E%E5%A4%84%E4%BA%8ETerminating%E7%8A%B6%E6%80%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/14/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E5%88%A0%E9%99%A4Pod%E5%90%8E%E5%A4%84%E4%BA%8ETerminating%E7%8A%B6%E6%80%81/" class="post-title-link" itemprop="url">K8S问题排查-删除Pod后处于Terminating状态</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-08-14 17:20:51" itemprop="dateCreated datePublished" datetime="2021-08-14T17:20:51+00:00">2021-08-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-07 09:04:28" itemprop="dateModified" datetime="2024-04-07T09:04:28+00:00">2024-04-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>11 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>通过<code>kubectl delete</code>命令删除某个业务Pod后，该Pod一直处于<code>Terminating</code>状态。</p>
<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>根据现象看，应该是删除过程中有哪个流程异常，导致最终的删除卡在了<code>Terminating</code>状态。先<code>describe</code>看一下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl describe pod -n xxx cam1-78b6fc6bc8-cjsw5</span><br><span class="line">// 没有发现什么异常信息，这里就不贴日志了</span><br></pre></td></tr></table></figure>

<p><code>Event</code>事件中未见明显异常，那就看负责删除Pod的<code>kubelet</code>组件日志（已过滤出关键性日志）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">I0728 16:24:57.339295    9744 kubelet.go:1904] SyncLoop (DELETE, &quot;api&quot;): &quot;cam1-78b6fc6bc8-cjsw5_cam(5c948341-c030-4996-b888-f032577d97b0)&quot;</span><br><span class="line">I0728 16:24:57.339720    9744 kuberuntime_container.go:581] Killing container &quot;docker://a73082a4a9a4cec174bb0d1c256cc11d804d93137551b9bfd3e6fa1522e98589&quot; with 60 second grace period</span><br><span class="line">I0728 16:25:18.259418    9744 kubelet.go:1904] SyncLoop (DELETE, &quot;api&quot;): &quot;cam1-78b6fc6bc8-cjsw5_cam(5c948341-c030-4996-b888-f032577d97b0)&quot;</span><br><span class="line">2021-07-28 16:25:19.247 [INFO][394011] ipam.go 1173: Releasing all IPs with handle &#x27;cam.cam1-78b6fc6bc8-cjsw5&#x27;</span><br><span class="line">2021-07-28 16:25:19.254 [INFO][393585] k8s.go 498: Teardown processing complete.</span><br><span class="line"></span><br><span class="line">// 可疑点1：没有获取到pod IP</span><br><span class="line">W0728 16:25:19.303513    9744 docker_sandbox.go:384] failed to read pod IP from plugin/docker: NetworkPlugin cni failed on the status hook for pod &quot;cam1-78b6fc6bc8-cjsw5_cam&quot;: Unexpected command output Device &quot;eth0&quot; does not exist.</span><br><span class="line"> with error: exit status 1</span><br><span class="line"> </span><br><span class="line">I0728 16:25:19.341068    9744 kubelet.go:1933] SyncLoop (PLEG): &quot;cam1-78b6fc6bc8-cjsw5_cam(5c948341-c030-4996-b888-f032577d97b0)&quot;, event: &amp;pleg.PodLifecycleEvent&#123;ID:&quot;5c948341-c030-4996-b888-f032577d97b0&quot;, Type:&quot;ContainerDied&quot;, Data:&quot;a73082a4a9a4cec174bb0d1c256cc11d804d93137551b9bfd3e6fa1522e98589&quot;&#125;</span><br><span class="line">I0728 16:25:20.578095    9744 kubelet.go:1933] SyncLoop (PLEG): &quot;cam1-78b6fc6bc8-cjsw5_cam(5c948341-c030-4996-b888-f032577d97b0)&quot;, event: &amp;pleg.PodLifecycleEvent&#123;ID:&quot;5c948341-c030-4996-b888-f032577d97b0&quot;, Type:&quot;ContainerDied&quot;, Data:&quot;c3b992465cd2085300995066526a36665664558446ff6e1756135c3a5b6df2e6&quot;&#125;</span><br><span class="line"></span><br><span class="line">I0728 16:25:20.711967    9744 kubelet_pods.go:1090] Killing unwanted pod &quot;cam1-78b6fc6bc8-cjsw5&quot;</span><br><span class="line"></span><br><span class="line">// 可疑点2：Unmount失败</span><br><span class="line">E0728 16:25:20.939400    9744 nestedpendingoperations.go:301] Operation for &quot;&#123;volumeName:kubernetes.io/glusterfs/5c948341-c030-4996-b888-f032577d97b0-cam-pv-50g podName:5c948341-c030-4996-b888-f032577d97b0 nodeName:&#125;&quot; failed. No retries permitted until 2021-07-28 16:25:21.439325811 +0800 CST m=+199182.605079651 (durationBeforeRetry 500ms). Error: &quot;UnmountVolume.TearDown failed for volume \&quot;diag-log\&quot; (UniqueName: \&quot;kubernetes.io/glusterfs/5c948341-c030-4996-b888-f032577d97b0-cam-pv-50g\&quot;) pod \&quot;5c948341-c030-4996-b888-f032577d97b0\&quot; (UID: \&quot;5c948341-c030-4996-b888-f032577d97b0\&quot;) : Unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/5c948341-c030-4996-b888-f032577d97b0/volumes/kubernetes.io~glusterfs/cam-pv-50g\nOutput: umount: /var/lib/kubelet/pods/5c948341-c030-4996-b888-f032577d97b0/volumes/kubernetes.io~glusterfs/cam-pv-50g：目标忙。\n        (有些情况下通过 lsof(8) 或 fuser(1) 可以\n         找到有关使用该设备的进程的有用信息。)\n\n&quot;</span><br></pre></td></tr></table></figure>

<p>从删除Pod的日志看，有2个可疑点：</p>
<ol>
<li><code>docker_sandbox.go:384</code>打印的获取<code>pod IP</code>错误；</li>
<li><code>nestedpendingoperations.go:301</code>打印的<code>Unmount</code>失败错误；</li>
</ol>
<p>先看第1点，根据日志定位到代码[1]位置如下，<code>IP</code>没有拿到所以打印了个告警并返回空<code>IP</code>地址；</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">pkg/kubelet/dockershim/docker_sandbox.<span class="keyword">go</span>:<span class="number">348</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ds *dockerService)</span></span> getIP(podSandboxID <span class="type">string</span>, sandbox *dockertypes.ContainerJSON) <span class="type">string</span> &#123;</span><br><span class="line">	<span class="keyword">if</span> sandbox.NetworkSettings == <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> networkNamespaceMode(sandbox) == runtimeapi.NamespaceMode_NODE &#123;</span><br><span class="line">		<span class="comment">// For sandboxes using host network, the shim is not responsible for</span></span><br><span class="line">		<span class="comment">// reporting the IP.</span></span><br><span class="line">		<span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Don&#x27;t bother getting IP if the pod is known and networking isn&#x27;t ready</span></span><br><span class="line">	ready, ok := ds.getNetworkReady(podSandboxID)</span><br><span class="line">	<span class="keyword">if</span> ok &amp;&amp; !ready &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	ip, err := ds.getIPFromPlugin(sandbox)</span><br><span class="line">	<span class="keyword">if</span> err == <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> ip</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">if</span> sandbox.NetworkSettings.IPAddress != <span class="string">&quot;&quot;</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> sandbox.NetworkSettings.IPAddress</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> sandbox.NetworkSettings.GlobalIPv6Address != <span class="string">&quot;&quot;</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> sandbox.NetworkSettings.GlobalIPv6Address</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 错误日志在这里</span></span><br><span class="line">	klog.Warningf(<span class="string">&quot;failed to read pod IP from plugin/docker: %v&quot;</span>, err)</span><br><span class="line">	<span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>继续看<code>getIP</code>方法的调用处代码，这里如果没有拿到<code>IP</code>，也没有什么异常，直接把空值放到<code>PodSandboxStatusResponse</code>中并返回；</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">pkg/kubelet/dockershim/docker_sandbox.<span class="keyword">go</span>:<span class="number">404</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ds *dockerService)</span></span> PodSandboxStatus(ctx context.Context, req *runtimeapi.PodSandboxStatusRequest) (*runtimeapi.PodSandboxStatusResponse, <span class="type">error</span>) &#123;</span><br><span class="line">	podSandboxID := req.PodSandboxId</span><br><span class="line"></span><br><span class="line">	r, metadata, err := ds.getPodSandboxDetails(podSandboxID)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Parse the timestamps.</span></span><br><span class="line">	createdAt, _, _, err := getContainerTimestamps(r)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">&quot;failed to parse timestamp for container %q: %v&quot;</span>, podSandboxID, err)</span><br><span class="line">	&#125;</span><br><span class="line">	ct := createdAt.UnixNano()</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Translate container to sandbox state.</span></span><br><span class="line">	state := runtimeapi.PodSandboxState_SANDBOX_NOTREADY</span><br><span class="line">	<span class="keyword">if</span> r.State.Running &#123;</span><br><span class="line">		state = runtimeapi.PodSandboxState_SANDBOX_READY</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 调用getIP方法的位置</span></span><br><span class="line">	<span class="keyword">var</span> IP <span class="type">string</span></span><br><span class="line">	<span class="keyword">if</span> IP = ds.determinePodIPBySandboxID(podSandboxID); IP == <span class="string">&quot;&quot;</span> &#123;</span><br><span class="line">		IP = ds.getIP(podSandboxID, r)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">    labels, annotations := extractLabels(r.Config.Labels)</span><br><span class="line">	status := &amp;runtimeapi.PodSandboxStatus&#123;</span><br><span class="line">		Id:          r.ID,</span><br><span class="line">		State:       state,</span><br><span class="line">		CreatedAt:   ct,</span><br><span class="line">		Metadata:    metadata,</span><br><span class="line">		Labels:      labels,</span><br><span class="line">		Annotations: annotations,</span><br><span class="line">		Network: &amp;runtimeapi.PodSandboxNetworkStatus&#123;</span><br><span class="line">			Ip: IP,</span><br><span class="line">		&#125;,</span><br><span class="line">		Linux: &amp;runtimeapi.LinuxPodSandboxStatus&#123;</span><br><span class="line">			Namespaces: &amp;runtimeapi.Namespace&#123;</span><br><span class="line">				Options: &amp;runtimeapi.NamespaceOption&#123;</span><br><span class="line">					Network: networkNamespaceMode(r),</span><br><span class="line">					Pid:     pidNamespaceMode(r),</span><br><span class="line">					Ipc:     ipcNamespaceMode(r),</span><br><span class="line">				&#125;,</span><br><span class="line">			&#125;,</span><br><span class="line">		&#125;,</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> &amp;runtimeapi.PodSandboxStatusResponse&#123;Status: status&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>到此看不出这个错误会不会中断删除流程，那就本地构造一下试试。修改上面的代码，在调用<code>getIP</code>方法的位置后面增加调试日志（从本地验证结果看，Pod正常删除，说明异常问题与此处无关）；</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 调用getIP方法的位置</span></span><br><span class="line"><span class="keyword">var</span> IP <span class="type">string</span></span><br><span class="line"><span class="keyword">if</span> IP = ds.determinePodIPBySandboxID(podSandboxID); IP == <span class="string">&quot;&quot;</span> &#123;</span><br><span class="line">	IP = ds.getIP(podSandboxID, r)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 新加调试日志，如果是指定的Pod，强制将IP置空</span></span><br><span class="line">isTestPod := strings.Contains(metadata.GetName(), <span class="string">&quot;testpod&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> isTestPod &#123;</span><br><span class="line">	IP = <span class="string">&quot;&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>再看第2点，这个是<code>ERROR</code>级别的错误，问题出在<code>Unmount</code>挂载点时失败。那么卸载挂载点失败会导致卸载流程提前终止吗？网上关于Pod删除流程的源码分析文章很多，我们就直接找几篇[2,3,4]看看能不能解答上面的问题。</p>
<p><strong>简单总结来说，删除一个Pod的流程如下：</strong></p>
<ol>
<li>调用<code>kube-apiserver</code>的<code>DELETE</code>接口（默认带<code>grace-period=30s</code>）；</li>
<li>第一次的删除只是更新Pod对象的元信息（<code>DeletionTimestamp</code>字段和<code>DeletionGracePeriodSeconds</code>字段），并没有在<code>Etcd</code>中删除记录；</li>
<li><code>kubectl</code>命令的执行会阻塞并显示正在删除Pod；</li>
<li><code>kubelet</code>组件监听到Pod对象的更新事件，执行<code>killPod()</code>方法；</li>
<li><code>kubelet</code>组件监听到pod的删除事件，第二次调用<code>kube-apiserver</code>的<code>DELETE</code>接口（带<code>grace-period=0</code>）</li>
<li><code>kube-apiserver</code>的<code>DELETE</code>接口去<code>etcd</code>中删除Pod对象；</li>
<li><code>kubectl</code>命令的执行返回，删除Pod成功；</li>
</ol>
<p>从前面<code>kubelet</code>删除异常的日志看，确实有两次<code>DELETE</code>操作，并且中间有个<code>Killing container</code>的日志，但从上面的删除流程看，两次<code>DELETE</code>操作之间应该是调用<code>killPod()</code>方法，通过查看源码，对应的日志应该是<code>Killing unwanted pod</code>，所以，实际上第二次的<code>DELETE</code>操作并没有触发。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">pkg/kubelet/kubelet_pods.<span class="keyword">go</span>:<span class="number">1073</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kl *Kubelet)</span></span> podKiller() &#123;</span><br><span class="line">	killing := sets.NewString()</span><br><span class="line">	<span class="comment">// guard for the killing set</span></span><br><span class="line">	lock := sync.Mutex&#123;&#125;</span><br><span class="line">	<span class="keyword">for</span> podPair := <span class="keyword">range</span> kl.podKillingCh &#123;</span><br><span class="line">		runningPod := podPair.RunningPod</span><br><span class="line">		apiPod := podPair.APIPod</span><br><span class="line"></span><br><span class="line">		lock.Lock()</span><br><span class="line">		exists := killing.Has(<span class="type">string</span>(runningPod.ID))</span><br><span class="line">		<span class="keyword">if</span> !exists &#123;</span><br><span class="line">			killing.Insert(<span class="type">string</span>(runningPod.ID))</span><br><span class="line">		&#125;</span><br><span class="line">		lock.Unlock()</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 这里在调用killPod方法前会打印v2级别的日志</span></span><br><span class="line">		<span class="keyword">if</span> !exists &#123;</span><br><span class="line">			<span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(apiPod *v1.Pod, runningPod *kubecontainer.Pod)</span></span> &#123;</span><br><span class="line">				klog.V(<span class="number">2</span>).Infof(<span class="string">&quot;Killing unwanted pod %q&quot;</span>, runningPod.Name)</span><br><span class="line">				err := kl.killPod(apiPod, runningPod, <span class="literal">nil</span>, <span class="literal">nil</span>)</span><br><span class="line">				<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">					klog.Errorf(<span class="string">&quot;Failed killing the pod %q: %v&quot;</span>, runningPod.Name, err)</span><br><span class="line">				&#125;</span><br><span class="line">				lock.Lock()</span><br><span class="line">				killing.Delete(<span class="type">string</span>(runningPod.ID))</span><br><span class="line">				lock.Unlock()</span><br><span class="line">			&#125;(apiPod, runningPod)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>怎么确认第二次的<code>DELETE</code>操作有没有触发呢？很简单，看代码或者实际验证都可以。这里我就在测试环境删除个Pod看下相关日志：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@node2 ~]# kubectl delete pod -n xxx  testpodrc2-7b749f6c9c-qh68l</span><br><span class="line">pod &quot;testpodrc2-7b749f6c9c-qh68l&quot; deleted</span><br><span class="line"></span><br><span class="line">// 已过滤出关键性日志</span><br><span class="line">[root@node2 ~]# tailf kubelet.log</span><br><span class="line">I0730 13:27:31.854178   24588 kubelet.go:1904] SyncLoop (DELETE, &quot;api&quot;): &quot;testpodrc2-7b749f6c9c-qh68l_testpod(85ee282f-a843-4f10-a99c-79d447f83f2a)&quot;</span><br><span class="line">I0730 13:27:31.854511   24588 kuberuntime_container.go:581] Killing container &quot;docker://e2a1cd5f2165e12cf0b46e12f9cd4d656d593f75e85c0de058e0a2f376a5557e&quot; with 30 second grace period</span><br><span class="line">I0730 13:27:32.203167   24588 kubelet.go:1904] SyncLoop (DELETE, &quot;api&quot;): &quot;testpodrc2-7b749f6c9c-qh68l_testpod(85ee282f-a843-4f10-a99c-79d447f83f2a)&quot;</span><br><span class="line"></span><br><span class="line">I0730 13:27:32.993294   24588 kubelet.go:1933] SyncLoop (PLEG): &quot;testpodrc2-7b749f6c9c-qh68l_testpod(85ee282f-a843-4f10-a99c-79d447f83f2a)&quot;, event: &amp;pleg.PodLifecycleEvent&#123;ID:&quot;85ee282f-a843-4f10-a99c-79d447f83f2a&quot;, Type:&quot;ContainerDied&quot;, Data:&quot;e2a1cd5f2165e12cf0b46e12f9cd4d656d593f75e85c0de058e0a2f376a5557e&quot;&#125;</span><br><span class="line">I0730 13:27:32.993428   24588 kubelet.go:1933] SyncLoop (PLEG): &quot;testpodrc2-7b749f6c9c-qh68l_testpod(85ee282f-a843-4f10-a99c-79d447f83f2a)&quot;, event: &amp;pleg.PodLifecycleEvent&#123;ID:&quot;85ee282f-a843-4f10-a99c-79d447f83f2a&quot;, Type:&quot;ContainerDied&quot;, Data:&quot;c6a587614976beed0cbb6e5fabf70a2d039eec6c160154fce007fe2bb1ba3b4f&quot;&#125;</span><br><span class="line"></span><br><span class="line">I0730 13:27:34.072494   24588 kubelet_pods.go:1090] Killing unwanted pod &quot;testpodrc2-7b749f6c9c-qh68l&quot;</span><br><span class="line"></span><br><span class="line">I0730 13:27:40.084182   24588 kubelet.go:1904] SyncLoop (DELETE, &quot;api&quot;): &quot;testpodrc2-7b749f6c9c-qh68l_testpod(85ee282f-a843-4f10-a99c-79d447f83f2a)&quot;</span><br><span class="line">I0730 13:27:40.085735   24588 kubelet.go:1898] SyncLoop (REMOVE, &quot;api&quot;): &quot;testpodrc2-7b749f6c9c-qh68l_testpod(85ee282f-a843-4f10-a99c-79d447f83f2a)&quot;</span><br></pre></td></tr></table></figure>

<p>对比正常和异常场景下的日志可以看出，正常的删除操作下，<code>Killing unwanted pod</code>日志之后会有<code>DELETE</code>和<code>REMOVE</code>的操作，这也就说明问题出在第二次<code>DELETE</code>操作没有触发。查看相关代码：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">pkg/kubelet/status/status_manager.<span class="keyword">go</span>:<span class="number">470</span></span><br><span class="line"><span class="comment">//kubelet组件有一个statusManager模块，它会for循环调用syncPod()方法</span></span><br><span class="line"><span class="comment">//方法内部有机会调用kube-apiserver的DELETE接口(强制删除，非平滑)</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *manager)</span></span> syncPod(uid types.UID, status versionedPodStatus) &#123;</span><br><span class="line">	...</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//当pod带有DeletionTimestamp字段，并且其内容器已被删除、持久卷已被删除等的多条件下，才会进入if语句内部</span></span><br><span class="line">    <span class="keyword">if</span> m.canBeDeleted(pod, status.status) &#123;</span><br><span class="line">        deleteOptions := metav1.NewDeleteOptions(<span class="number">0</span>)</span><br><span class="line">        deleteOptions.Preconditions = metav1.NewUIDPreconditions(<span class="type">string</span>(pod.UID))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//强制删除pod对象：kubectl delete pod podA --grace-period=0</span></span><br><span class="line">        err = m.kubeClient.CoreV1().Pods(pod.Namespace).Delete(pod.Name, deleteOptions) </span><br><span class="line">	...</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从源码可以看出，第二次<code>DELETE</code>操作是否触发依赖于<code>canBeDeleted</code>方法的校验结果，而这个方法内会检查持久卷是否已经被删除：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">pkg/kubelet/status/status_manager.<span class="keyword">go</span>:<span class="number">538</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *manager)</span></span> canBeDeleted(pod *v1.Pod, status v1.PodStatus) <span class="type">bool</span> &#123;</span><br><span class="line">	<span class="keyword">if</span> pod.DeletionTimestamp == <span class="literal">nil</span> || kubepod.IsMirrorPod(pod) &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> m.podDeletionSafety.PodResourcesAreReclaimed(pod, status)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pkg/kubelet/kubelet_pods.<span class="keyword">go</span>:<span class="number">900</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kl *Kubelet)</span></span> PodResourcesAreReclaimed(pod *v1.Pod, status v1.PodStatus) <span class="type">bool</span> &#123;</span><br><span class="line">	...</span><br><span class="line">    </span><br><span class="line">	<span class="comment">// 这里会判断挂载卷是否已卸载</span></span><br><span class="line">	<span class="keyword">if</span> kl.podVolumesExist(pod.UID) &amp;&amp; !kl.keepTerminatedPodVolumes &#123;</span><br><span class="line">		<span class="comment">// We shouldnt delete pods whose volumes have not been cleaned up if we are not keeping terminated pod volumes</span></span><br><span class="line">		klog.V(<span class="number">3</span>).Infof(<span class="string">&quot;Pod %q is terminated, but some volumes have not been cleaned up&quot;</span>, format.Pod(pod))</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> kl.kubeletConfiguration.CgroupsPerQOS &#123;</span><br><span class="line">		pcm := kl.containerManager.NewPodContainerManager()</span><br><span class="line">		<span class="keyword">if</span> pcm.Exists(pod) &#123;</span><br><span class="line">			klog.V(<span class="number">3</span>).Infof(<span class="string">&quot;Pod %q is terminated, but pod cgroup sandbox has not been cleaned up&quot;</span>, format.Pod(pod))</span><br><span class="line">			<span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结合出问题的日志，基本能确认是<code>Unmount</code>挂载点失败导致的异常。那么，挂载点为啥会<code>Unmount</code>失败？</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// umount失败关键日志</span><br><span class="line">Unmount failed: exit status 32\nUnmounting arguments: /var/lib/kubelet/pods/xxx/volumes/kubernetes.io~glusterfs/cam-pv-50g\nOutput: umount: /var/lib/kubelet/pods/xxx/volumes/kubernetes.io~glusterfs/cam-pv-50g：目标忙。\n        (有些情况下通过 lsof(8) 或 fuser(1) 可以\n         找到有关使用该设备的进程的有用信息。)\n\n&quot;</span><br></pre></td></tr></table></figure>

<p>仔细看卸载失败的日志，可以看到这个挂载点的后端存储是<code>glusterfs</code>，而<code>目标忙</code>一般来说是存储设备侧在使用，所以无法卸载。那就找找看是不是哪个进程使用了这个挂载目录（以下定位由负责<code>glusterfs</code>的同事提供）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# fuser -mv /var/lib/kubelet/pods/xxx/volumes/kubernetes.io~glusterfs/cam-pv-50g</span><br><span class="line">用户  进程号  权限  命令</span><br><span class="line">root  kernel mount /var/lib/kubelet/pods/xxx/volumes/kubernetes.io~glusterfs/cam-dialog-gl.uster-pv-50g</span><br><span class="line">root  94549  f.... glusterfs</span><br></pre></td></tr></table></figure>

<p>除了内核的<code>mount</code>，还有个<code>pid=94549</code>的<code>glusterfs</code>进程在占用挂载点所在目录，看看是什么进程：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# ps -ef| grep 94549</span><br><span class="line">root 94549 1 0 7月26 ? 00:01:13 /usr/sbin/glusterfs --log-level=ERR0R --log-file=/var/lib/kubelet/plugins/kubernetes.io/glusterfs/global-diaglog-pv/web-fddf96444-stxpf-glusterfs.log --fuse-mountopts=auto_unmount --process-name fuse --volfile-server=xxx --volfile-server=xxx --tfolfile-server=xxx --volfile-id=global-diaglog --fuse-mountopts=auto_unmount /var/lib/kubelet/pods/xxx/volumes/kubernetef.io-glusterfs/global-diaglog-pv</span><br></pre></td></tr></table></figure>

<p>发现这个进程维护的是<code>web-xxx</code>的挂载信息，而<code>web-xxx</code>和<code>cam-xxx</code>没有任何关联。由此推断出是<code>glusterfs</code>管理的挂载信息发送错乱导致，具体错乱原因就转给相关负责的同事看了。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>从分析结果看，是共享存储卷未正常卸载导致的删除Pod异常，非K8S问题。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubernetes/tree/v1.15.12">Kubernetes v1.15.12源码</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/nangonghen/article/details/109305635">kubernetes删除pod的流程的源码简析</a></li>
<li><a target="_blank" rel="noopener" href="https://juejin.cn/post/6844903842321039368">Kubernetes源码分析之Pod的删除</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/608727">kubernetes grace period 失效问题排查</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/08/07/linux/%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Web%E5%BA%94%E7%94%A8%E9%A1%B5%E9%9D%A2%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/07/linux/%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Web%E5%BA%94%E7%94%A8%E9%A1%B5%E9%9D%A2%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE/" class="post-title-link" itemprop="url">网络问题排查-Web应用页面无法访问</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-08-07 21:30:51" itemprop="dateCreated datePublished" datetime="2021-08-07T21:30:51+00:00">2021-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-07 09:04:28" itemprop="dateModified" datetime="2024-04-07T09:04:28+00:00">2024-04-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/linux/" itemprop="url" rel="index"><span itemprop="name">linux</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>部署在服务器上的Web应用因为机房迁移，导致PC上无法正常访问Web页面。</p>
<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>本次遇到的问题纯属网络层面问题，不用多想，先登录到服务器上，查看服务端口的监听状态：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">[root@node2]# </span><span class="language-bash">netstat -anp|grep 443</span></span><br><span class="line">tcp6       0      0 :::443                 :::*                    LISTEN      8450/java</span><br></pre></td></tr></table></figure>

<p>在服务器所在节点、服务器之前的其他节点上<code>curl</code>监听端口看看是否有响应：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">[root@node2]# </span><span class="language-bash">curl -i -k https://192.168.10.10:443</span></span><br><span class="line">HTTP/1.1 302 Found</span><br><span class="line">Location: https://127.0.0.1:443</span><br><span class="line">Content-Length: 0</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">[root@node2]# </span><span class="language-bash">curl -i -k https://192.168.10.11:443</span></span><br><span class="line">HTTP/1.1 302 Found</span><br><span class="line">Location: https://192.168.10.11:443</span><br><span class="line">Content-Length: 0</span><br></pre></td></tr></table></figure>

<p>到此为止，说明Web服务运行正常，<strong>问题出在了PC到服务器这个通信过程</strong>。本地<code>wireshark</code>抓包看看，相关异常报文如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">371 70.961626   3.2.253.177     172.30.31.151   TCP     66  52541 → 443 [SYN] Seq=0 Win=8192 Len=0 MSS=1460 WS=4 SACK_PERM=1</span><br><span class="line">373 70.962516   172.30.31.151   3.2.253.177     TCP     66  443 → 52541 [SYN, ACK] Seq=0 Ack=1 Win=29200 Len=0 MSS=1460 SACK_PERM=1 WS=128</span><br><span class="line">375 70.962563   3.2.253.177     172.30.31.151   TCP     54  52541 → 443 [ACK] Seq=1 Ack=1 Win=65700 Len=0</span><br><span class="line">377 70.963248   3.2.253.177     172.30.31.151   TLSv1.2 571 Client Hello</span><br><span class="line">379 70.964323   172.30.31.151   3.2.253.177     TCP     60  443 → 52541 [ACK] Seq=1 Ack=518 Win=30336 Len=0</span><br><span class="line">381 70.965327   172.30.31.151   3.2.253.177     TLSv1.2 144 Server Hello</span><br><span class="line">383 70.965327   172.30.31.151   3.2.253.177     TLSv1.2 105 Change Cipher Spec, Encrypted Handshake Message</span><br><span class="line">385 70.965364   3.2.253.177     172.30.31.151   TCP     54  52541 → 443 [ACK] Seq=518 Ack=142 Win=65556 Len=0</span><br><span class="line">387 70.967194   3.2.253.177     172.30.31.151   TLSv1.2 61  Alert (Level: Fatal, Description: Certificate Unknown)</span><br><span class="line">388 70.967233   3.2.253.177     172.30.31.151   TCP     54  52541 → 443 [FIN, ACK] Seq=525 Ack=142 Win=65556 Len=0</span><br><span class="line">391 70.968320   172.30.31.151   3.2.253.177     TLSv1.2 85  Encrypted Alert</span><br><span class="line">392 70.968321   172.30.31.151   3.2.253.177     TCP     60  443 → 52541 [FIN, ACK] Seq=173 Ack=526 Win=30336 Len=0</span><br><span class="line">394 70.968356   3.2.253.177     172.30.31.151   TCP     54  52541 → 443 [RST, ACK] Seq=526 Ack=173 Win=0 Len=0</span><br><span class="line">395 70.968370   3.2.253.177     172.30.31.151   TCP     54  52541 → 443 [RST] Seq=526 Win=0 Len=0</span><br></pre></td></tr></table></figure>

<p>关键是最后两个，可以看出报文存在复位标志<code>RST</code>。与提供环境的人了解到PC与服务器之间使用的交换机是通过<code>GRE隧道</code>打通的网络，基本怀疑是交换机配置存在问题；</p>
<p>同时观察到PC访问集群的<code>ftp</code>也存在异常，说明是一个通用问题，而PC上<code>ping</code>和<code>ssh</code>服务器都没有问题，说明是配置导致的部分协议的连接问题；</p>
<p>后来提供环境的人排查交换机配置，发现<code>GRE隧道</code>的默认<code>MTU</code>为<code>1464</code>，而集群网卡上的<code>MTU</code>为<code>1500</code>，最后协商出的<code>MSS</code>为<code>1460</code>（见抓包中的前两个报文）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[leaf11]dis interface Tunnel</span><br><span class="line">Tunnel0</span><br><span class="line">Current state: UP</span><br><span class="line">Line protocol state: UP</span><br><span class="line">Description: Tunnel0 Interface</span><br><span class="line">Bandwidth: 64 kbps</span><br><span class="line">Maximum transmission unit: 1464</span><br><span class="line">Internet protocol processing: Disabled</span><br><span class="line">Last clearing of counters: Never</span><br><span class="line">Tunnel source 3.1.1.11, destination 2.1.1.222</span><br><span class="line">Tunnel protocol/transport UDP_VXLAN/IP</span><br><span class="line">Last 300 seconds input rate: 0 bytes/sec, 0 bits/sec, 0 packets/sec</span><br><span class="line">Last 300 seconds output rate: 0 bytes/sec, 0 bits/</span><br></pre></td></tr></table></figure>

<p>这种情况下，最大的报文发到交换机后，因为交换机允许的最大报文数为<code>1464-40=1424</code>字节，所以出现了上述现象，同时也解释了<code>http</code>和<code>ftp</code>有问题（长报文），而<code>ping</code>和<code>ssh</code>没有问题（短报文）。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>方案1：修改隧道口和物理口的<code>MTU</code>值，但是取值不好定，因为不知道应用最长报文的长度。<br>方案2：<code>GRE</code>隧道口配置<code>TCP</code>的<code>MSS</code>，超出后分片处理。</p>
<p>设置<code>TCP</code>的<code>MSS</code>参考命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">【命令】</span><br><span class="line">tcp mss value</span><br><span class="line">undo tcp mss</span><br><span class="line">【缺省情况】</span><br><span class="line">未配置接口的TCP最大报文段长度。</span><br><span class="line">【视图】</span><br><span class="line">接口视图</span><br><span class="line">【缺省用户角色】</span><br><span class="line">network-admin</span><br><span class="line">mdc-admin</span><br><span class="line">【参数】</span><br><span class="line">value：TCP最大报文段长度，取值范围为128～（接口的最大MTU值-40），单位为字节。</span><br><span class="line">【使用指导】</span><br><span class="line">TCP最大报文段长度（Max Segment Size，MSS）表示TCP连接的对端发往本端的最大TCP报文段的长度，目前作为TCP连接建立时的一个选项来协商：当一个TCP连接建立时，连接的双方要将MSS作为TCP报文的一个选项通告给对端，对端会记录下这个MSS值，后续在发送TCP报文时，会限制TCP报文的大小不超过该MSS值。当对端发送的TCP报文的长度小于本端的TCP最大报文段长度时，TCP报文不需要分段；否则，对端需要对TCP报文按照最大报文段长度进行分段处理后再发给本端。</span><br><span class="line">该配置仅对新建的TCP连接生效，对于配置前已建立的TCP连接不生效。</span><br><span class="line">该配置仅对IP报文生效，当接口上配置了MPLS功能后，不建议再配置本功能。 </span><br></pre></td></tr></table></figure>

<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43684922/article/details/105300934">https://blog.csdn.net/qq_43684922/article/details/105300934</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/07/24/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Pod%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E9%AB%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/24/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Pod%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E9%AB%98/" class="post-title-link" itemprop="url">K8S问题排查-Pod内存占用高</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-24 16:17:21" itemprop="dateCreated datePublished" datetime="2021-07-24T16:17:21+00:00">2021-07-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-07 09:04:28" itemprop="dateModified" datetime="2024-04-07T09:04:28+00:00">2024-04-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>如下所示，用户使用<code>kubectl top</code>命令看到其中一个节点上的Harbor占用内存约3.7G（其他业务Pod也存在类似现象），整体上来说，有点偏高。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@node02 ~]# kubectl get node -owide</span><br><span class="line">NAME   STATUS   ROLES    AGE   VERSION    INTERNAL-IP   EXTERNAL-IP       </span><br><span class="line">node01   Ready    master   10d   v1.15.12   100.1.0.10    &lt;none&gt;   </span><br><span class="line">node02   Ready    master   12d   v1.15.12   100.1.0.11    &lt;none&gt;  </span><br><span class="line">node03   Ready    master   10d   v1.15.12   100.1.0.12    &lt;none&gt; </span><br><span class="line"></span><br><span class="line">[root@node02 ~]# kubectl top pod -A |grep harbor</span><br><span class="line">kube-system         harbor-master1-sxg2l                          15m          150Mi</span><br><span class="line">kube-system         harbor-master2-ncvb8                          8m           3781Mi</span><br><span class="line">kube-system         harbor-master3-2gdsn                          14m          227Mi</span><br></pre></td></tr></table></figure>

<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>我们知道，查看容器的内存占用，可以使用<code>kubectl top</code>命令，也可以使用<code>docker stats</code>命令，并且理论上来说，<code>docker stats</code>命令查的结果应该比<code>kubectl top</code>查到的更准确。查看并统计发现，实际上Harbor总内存占用约为140M左右，远没有达到3.7G：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@node02 ~]# docker stats |grep harbor</span><br><span class="line">CONTAINER ID        NAME                                      CPU %    MEM USAGE / LIMIT     MEM %</span><br><span class="line">10a230bee3c7        k8s_nginx_harbor-master2-xxx              0.02%    14.15MiB / 94.26GiB   0.01%</span><br><span class="line">6ba14a04fd77        k8s_harbor-portal_harbor-master2-xxx      0.01%    13.73MiB / 94.26GiB   0.01%</span><br><span class="line">324413da20a9        k8s_harbor-jobservice_harbor-master2-xxx  0.11%    21.54MiB / 94.26GiB   0.02%</span><br><span class="line">d880b61cf4cb        k8s_harbor-core_harbor-master2-xxx        0.12%    33.2MiB / 94.26GiB    0.03%</span><br><span class="line">186c064d0930        k8s_harbor-registryctl_harbor-master2-xxx 0.01%    8.34MiB / 94.26GiB    0.01%</span><br><span class="line">52a50204a962        k8s_harbor-registry_harbor-master2-xxx    0.06%    29.99MiB / 94.26GiB   0.03%</span><br><span class="line">86031ddd0314        k8s_harbor-redis_harbor-master2-xxx       0.14%    11.51MiB / 94.26GiB   0.01%</span><br><span class="line">6366207680f2        k8s_harbor-database_harbor-master2-xxx    0.45%    8.859MiB / 94.26GiB   0.01%</span><br></pre></td></tr></table></figure>

<p>这是什么情况？两个命令查到的结果差距也太大了。查看资料[1]可以知道：</p>
<ol>
<li><code>kubectl top</code>命令的计算公式：<code>memory.usage_in_bytes - inactive_file</code>；</li>
<li><code>docker stats</code>命令的计算公式：<code>memory.usage_in_bytes - cache</code>；</li>
</ol>
<p>可以看出，两种方式收集机制不一样，如果<code>cache</code>比较大，<code>kubectl top</code>命令看到的结果会偏高。根据上面的计算公式验证看看是否正确：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">curl -s --unix-socket /var/run/docker.sock http:/v1.24/containers/xxx/stats | jq .&quot;memory_stats&quot;</span><br><span class="line">&quot;memory_stats&quot;: &#123;</span><br><span class="line">    &quot;usage&quot;: 14913536,</span><br><span class="line">    &quot;max_usage&quot;: 15183872,</span><br><span class="line">    &quot;stats&quot;: &#123;</span><br><span class="line">      &quot;active_anon&quot;: 14835712,</span><br><span class="line">      &quot;active_file&quot;: 0,</span><br><span class="line">      &quot;cache&quot;: 77824,</span><br><span class="line">      &quot;dirty&quot;: 0,</span><br><span class="line">      &quot;hierarchical_memory_limit&quot;: 101205622784,</span><br><span class="line">      &quot;hierarchical_memsw_limit&quot;: 9223372036854772000,</span><br><span class="line">      &quot;inactive_anon&quot;: 4096,</span><br><span class="line">      &quot;inactive_file&quot;: 73728,</span><br><span class="line">      ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&quot;memory_stats&quot;: &#123;</span><br><span class="line">    &quot;usage&quot;: 14405632,</span><br><span class="line">    &quot;max_usage&quot;: 14508032,</span><br><span class="line">    &quot;stats&quot;: &#123;</span><br><span class="line">      &quot;active_anon&quot;: 14397440,</span><br><span class="line">      &quot;active_file&quot;: 0,</span><br><span class="line">      &quot;cache&quot;: 8192,</span><br><span class="line">      &quot;dirty&quot;: 0,</span><br><span class="line">      &quot;hierarchical_memory_limit&quot;: 101205622784,</span><br><span class="line">      &quot;hierarchical_memsw_limit&quot;: 9223372036854772000,</span><br><span class="line">      &quot;inactive_anon&quot;: 4096,</span><br><span class="line">      &quot;inactive_file&quot;: 4096,</span><br><span class="line">      ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&quot;memory_stats&quot;: &#123;</span><br><span class="line">    &quot;usage&quot;: 26644480,</span><br><span class="line">    &quot;max_usage&quot;: 31801344,</span><br><span class="line">    &quot;stats&quot;: &#123;</span><br><span class="line">      &quot;active_anon&quot;: 22810624,</span><br><span class="line">      &quot;active_file&quot;: 790528,</span><br><span class="line">      &quot;cache&quot;: 3833856,</span><br><span class="line">      &quot;dirty&quot;: 0,</span><br><span class="line">      &quot;hierarchical_memory_limit&quot;: 101205622784,</span><br><span class="line">      &quot;hierarchical_memsw_limit&quot;: 9223372036854772000,</span><br><span class="line">      &quot;inactive_anon&quot;: 0,</span><br><span class="line">      &quot;inactive_file&quot;: 3043328,</span><br><span class="line">      ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&quot;memory_stats&quot;: &#123;</span><br><span class="line">    &quot;usage&quot;: 40153088,</span><br><span class="line">    &quot;max_usage&quot;: 90615808,</span><br><span class="line">    &quot;stats&quot;: &#123;</span><br><span class="line">      &quot;active_anon&quot;: 35123200,</span><br><span class="line">      &quot;active_file&quot;: 1372160,</span><br><span class="line">      &quot;cache&quot;: 5029888,</span><br><span class="line">      &quot;dirty&quot;: 0,</span><br><span class="line">      &quot;hierarchical_memory_limit&quot;: 101205622784,</span><br><span class="line">      &quot;hierarchical_memsw_limit&quot;: 9223372036854772000,</span><br><span class="line">      &quot;inactive_anon&quot;: 0,</span><br><span class="line">      &quot;inactive_file&quot;: 3657728,</span><br><span class="line">      ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&quot;memory_stats&quot;: &#123;</span><br><span class="line">    &quot;usage&quot;: 10342400,</span><br><span class="line">    &quot;max_usage&quot;: 12390400,</span><br><span class="line">    &quot;stats&quot;: &#123;</span><br><span class="line">    &quot;active_anon&quot;: 8704000,</span><br><span class="line">    &quot;active_file&quot;: 241664,</span><br><span class="line">    &quot;cache&quot;: 1638400,</span><br><span class="line">    &quot;dirty&quot;: 0,</span><br><span class="line">    &quot;hierarchical_memory_limit&quot;: 101205622784,</span><br><span class="line">    &quot;hierarchical_memsw_limit&quot;: 9223372036854772000,</span><br><span class="line">    &quot;inactive_anon&quot;: 0,</span><br><span class="line">    &quot;inactive_file&quot;: 1396736,</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&quot;memory_stats&quot;: &#123;</span><br><span class="line">    &quot;usage&quot;: 5845127168,</span><br><span class="line">    &quot;max_usage&quot;: 22050988032,</span><br><span class="line">    &quot;stats&quot;: &#123;</span><br><span class="line">    &quot;active_anon&quot;: 31576064,</span><br><span class="line">    &quot;active_file&quot;: 3778052096,</span><br><span class="line">    &quot;cache&quot;: 5813551104,</span><br><span class="line">    &quot;dirty&quot;: 0,</span><br><span class="line">    &quot;hierarchical_memory_limit&quot;: 101205622784,</span><br><span class="line">    &quot;hierarchical_memsw_limit&quot;: 9223372036854772000,</span><br><span class="line">    &quot;inactive_anon&quot;: 0,</span><br><span class="line">    &quot;inactive_file&quot;: 2035499008,</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&quot;memory_stats&quot;: &#123;</span><br><span class="line">    &quot;usage&quot;: 13250560,</span><br><span class="line">    &quot;max_usage&quot;: 34791424,</span><br><span class="line">    &quot;stats&quot;: &#123;</span><br><span class="line">    &quot;active_anon&quot;: 12070912,</span><br><span class="line">    &quot;active_file&quot;: 45056,</span><br><span class="line">    &quot;cache&quot;: 1179648,</span><br><span class="line">    &quot;dirty&quot;: 0,</span><br><span class="line">    &quot;hierarchical_memory_limit&quot;: 101205622784,</span><br><span class="line">    &quot;hierarchical_memsw_limit&quot;: 9223372036854772000,</span><br><span class="line">    &quot;inactive_anon&quot;: 0,</span><br><span class="line">    &quot;inactive_file&quot;: 1134592,</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&quot;memory_stats&quot;: &#123;</span><br><span class="line">    &quot;usage&quot;: 50724864,</span><br><span class="line">    &quot;max_usage&quot;: 124682240,</span><br><span class="line">    &quot;stats&quot;: &#123;</span><br><span class="line">    &quot;active_anon&quot;: 23502848,</span><br><span class="line">    &quot;active_file&quot;: 13864960,</span><br><span class="line">    &quot;cache&quot;: 41435136,</span><br><span class="line">    &quot;dirty&quot;: 0,</span><br><span class="line">    &quot;hierarchical_memory_limit&quot;: 101205622784,</span><br><span class="line">    &quot;hierarchical_memsw_limit&quot;: 9223372036854772000,</span><br><span class="line">    &quot;inactive_anon&quot;: 6836224,</span><br><span class="line">    &quot;inactive_file&quot;: 6520832,</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>根据上面提供的计算公式和实际获取的<code>memory_stats</code>数据，验证<code>kubectl top</code>结果和<code>docker stats</code>结果符合预期。那为什么Harbor缓存会占用那么高呢？</p>
<p>通过实际环境分析看，Harbor中占用缓存较高的组件是<code>registry</code>（如下所示，缓存有5.4G），考虑到<code>registry</code>负责<code>docker</code>镜像的存储，在处理镜像时会有大量的镜像层文件的读写操作，所以正常情况下这些操作确实会比较耗缓存；</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&quot;memory_stats&quot;: &#123;</span><br><span class="line">    &quot;usage&quot;: 5845127168,</span><br><span class="line">    &quot;max_usage&quot;: 22050988032,</span><br><span class="line">    &quot;stats&quot;: &#123;</span><br><span class="line">    &quot;active_anon&quot;: 31576064,</span><br><span class="line">    &quot;active_file&quot;: 3778052096,</span><br><span class="line">    &quot;cache&quot;: 5813551104,</span><br><span class="line">    &quot;dirty&quot;: 0,</span><br><span class="line">    &quot;hierarchical_memory_limit&quot;: 101205622784,</span><br><span class="line">    &quot;hierarchical_memsw_limit&quot;: 9223372036854772000,</span><br><span class="line">    &quot;inactive_anon&quot;: 0,</span><br><span class="line">    &quot;inactive_file&quot;: 2035499008,</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>与用户沟通，说明<code>kubectl top</code>看到的结果包含了容器内使用的<code>cache</code>，结果会偏高，这部分缓存在内存紧张情况下会被系统回收，或者手工操作也可以释放，建议使用<code>docker stats</code>命令查看实际内存使用率。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/xyclianying/article/details/108513122">https://blog.csdn.net/xyclianying/article/details/108513122</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/07/16/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E4%B8%9A%E5%8A%A1%E9%AB%98%E5%B9%B6%E5%8F%91%E5%AF%BC%E8%87%B4Pod%E5%8F%8D%E5%A4%8D%E9%87%8D%E5%90%AF(%E7%BB%AD)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/16/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E4%B8%9A%E5%8A%A1%E9%AB%98%E5%B9%B6%E5%8F%91%E5%AF%BC%E8%87%B4Pod%E5%8F%8D%E5%A4%8D%E9%87%8D%E5%90%AF(%E7%BB%AD)/" class="post-title-link" itemprop="url">K8S问题排查-业务高并发导致Pod反复重启(续)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-16 21:10:40" itemprop="dateCreated datePublished" datetime="2021-07-16T21:10:40+00:00">2021-07-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-07 09:04:28" itemprop="dateModified" datetime="2024-04-07T09:04:28+00:00">2024-04-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>接上次的<a href="https://lyyao09.github.io/2021/06/19/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-%E4%B8%9A%E5%8A%A1%E9%AB%98%E5%B9%B6%E5%8F%91%E5%AF%BC%E8%87%B4Pod%E5%8F%8D%E5%A4%8D%E9%87%8D%E5%90%AF/">问题</a>，一段时间后，环境再次出现<code>harbor</code>和<code>calico</code>因为健康检查不过反复重启的问题，并且使用<code>kubectl</code>命令进入Pod也响应非常慢甚至超时。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# kubectl exec -it -n system node1-59c9475bc6-zkhq5 bash</span><br><span class="line">^</span><br></pre></td></tr></table></figure>

<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>反复重启的原因上次已定位，这次上环境简单看还是因为健康检查超时的问题，并且现象也一样，<code>TCP</code>的连接卡在了第一次握手的<code>SYN_SENT</code>阶段。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# netstat -anp|grep 23380</span><br><span class="line">tcp        0      0 127.0.0.1:23380         0.0.0.0:*               LISTEN      38914/kubelet</span><br><span class="line">tcp        0      0 127.0.0.1:38983         127.0.0.1:23380         SYN_SENT    -</span><br></pre></td></tr></table></figure>

<p>也就是说，除了<code>TCP</code>连接队列的问题，还存在其他问题会导致该现象。先看看上次的参数还在不在：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# cat /etc/sysctl.conf</span><br><span class="line">net.ipv4.tcp_max_syn_backlog = 32768</span><br><span class="line">net.core.somaxconn = 32768</span><br></pre></td></tr></table></figure>

<p>再看下上次修改的参数是否生效：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# ss -lnt</span><br><span class="line">State      Recv-Q   Send-Q     Local Address:Port      Peer Address:Port              </span><br><span class="line">LISTEN     0        32768      127.0.0.1:23380         *:*</span><br></pre></td></tr></table></figure>

<p>参数的修改也生效了，那为什么还会卡在<code>SYN_SENT</code>阶段呢？从现有情况，看不出还有什么原因会导致该问题，只能摸索看看。</p>
<ol>
<li>在问题节点和非问题节点上分别抓包，看报文交互是否存在什么异常；</li>
<li>根据参考资料[1]，排查是否为相同问题；</li>
<li>根据参考资料[2]，排查是否相同问题；</li>
<li>…</li>
</ol>
<p>摸索一番，没发现什么异常。回过头来想想，既然是业务下发大量配置导致的，并且影响是全局的（除了业务Pod自身，其他组件也受到了影响），说明大概率原因还是系统层面存在的性能瓶颈。业务量大的影响除了CPU、一般还有内存、磁盘、连接数等等，与开发人员确认他们的连接还是长连接，那么连接数很大的情况下会受到什么内核参数的影响呢？其中一个就是我们熟知的文件句柄数。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# lsof -p 45775 | wc -l</span><br><span class="line">17974</span><br><span class="line"></span><br><span class="line">[root@node01 ~]# lsof -p 45775|grep &quot;sock&quot;| wc -l</span><br><span class="line">12051</span><br></pre></td></tr></table></figure>

<p>嗯，打开了<strong>1w+的文件句柄数并且基本都是<code>sock</code>连接</strong>，而我们使用的操作系统默认情况下每个进程的文件句柄数限制为1024，查看确认一下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# ulimit  -n</span><br><span class="line">1024</span><br></pre></td></tr></table></figure>

<p>超额使用了这么多，业务Pod竟然没有<code>too many open files</code>错误：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# kubectl logs -n system node1-59c9475bc6-zkhq5</span><br><span class="line">start config</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>临时修改一下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# ulimit -n 65535</span><br><span class="line">[root@node01 ~]# ulimit  -n</span><br><span class="line">65535</span><br></pre></td></tr></table></figure>

<p>再次使用<code>kubectl</code>命令进入业务Pod，响应恢复正常，并且查看连接也不再有卡住的<code>SYN_SENT</code>阶段：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# kubectl exec -it -n system node1-59c9475bc6-zkhq5 bash</span><br><span class="line"><span class="meta prompt_">[root@node1-59c9475bc6-zkhq5]# </span><span class="language-bash"><span class="built_in">exit</span></span></span><br><span class="line">[root@node01 ~]# kubectl exec -it -n system node1-59c9475bc6-zkhq5 bash</span><br><span class="line"><span class="meta prompt_">[root@node1-59c9475bc6-zkhq5]# </span><span class="language-bash"><span class="built_in">exit</span></span></span><br><span class="line">[root@node01 ~]# kubectl exec -it -n system node1-59c9475bc6-zkhq5 bash</span><br><span class="line"><span class="meta prompt_">[root@node1-59c9475bc6-zkhq5]# </span><span class="language-bash"><span class="built_in">exit</span></span></span><br><span class="line"></span><br><span class="line">[root@node01 ~]# netstat -anp|grep 23380</span><br><span class="line">tcp        0      0 127.0.0.1:23380         0.0.0.0:*               LISTEN      38914/kubelet</span><br><span class="line">tcp        0      0 127.0.0.1:56369         127.0.0.1:23380         TIME_WAIT   -</span><br><span class="line">tcp        0      0 127.0.0.1:23380         127.0.0.1:57601         TIME_WAIT   -</span><br><span class="line">tcp        0      0 127.0.0.1:23380         127.0.0.1:57479         TIME_WAIT   -</span><br></pre></td></tr></table></figure>

<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ol>
<li>业务根据实际情况调整文件句柄数。</li>
<li>针对业务量大的环境，强烈建议整体做一下操作系统层面的性能优化，否则，不定哪个系统参数就成了性能瓶颈，网上找了个调优案例[3]，感兴趣的可以参考。</li>
</ol>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/pyxllq/article/details/80351827">https://blog.csdn.net/pyxllq/article/details/80351827</a></li>
<li><a target="_blank" rel="noopener" href="http://mdba.cn/2015/03/10/tcp-socket%E6%96%87%E4%BB%B6%E5%8F%A5%E6%9F%84%E6%B3%84%E6%BC%8F">http://mdba.cn/2015/03/10/tcp-socket文件句柄泄漏</a></li>
<li><a target="_blank" rel="noopener" href="https://www.shuzhiduo.com/A/RnJW7NLyJq/">https://www.shuzhiduo.com/A/RnJW7NLyJq/</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/07/10/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Influxdb%E7%9B%91%E6%8E%A7%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96%E5%BC%82%E5%B8%B8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/10/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Influxdb%E7%9B%91%E6%8E%A7%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96%E5%BC%82%E5%B8%B8/" class="post-title-link" itemprop="url">K8S问题排查-Influxdb监控数据获取异常</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-10 11:11:46" itemprop="dateCreated datePublished" datetime="2021-07-10T11:11:46+00:00">2021-07-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-07 09:04:28" itemprop="dateModified" datetime="2024-04-07T09:04:28+00:00">2024-04-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>K8S集群内，<code>Influxdb</code>监控数据获取异常，最终CPU、内存和磁盘使用率都无法获取。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">监控项         使用率</span><br><span class="line">CPU(核)        3%</span><br><span class="line">内存(GB)       18%</span><br><span class="line">磁盘空间(GB)    0%</span><br><span class="line"></span><br><span class="line">监控项         使用率</span><br><span class="line">CPU(核)        7%</span><br><span class="line">内存(GB)       18%</span><br><span class="line">磁盘空间(GB)    1%</span><br><span class="line"></span><br><span class="line">监控项         使用率</span><br><span class="line">CPU(核)        0%</span><br><span class="line">内存(GB)       0%</span><br><span class="line">磁盘空间(GB)    0%</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p><code>Influxdb</code>监控架构图参考[1]，其中<code>Load Balancer</code>采用<code>nginx</code>实现：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">        ┌─────────────────┐                 </span><br><span class="line">        │writes &amp; queries │                 </span><br><span class="line">        └─────────────────┘                 </span><br><span class="line">                 │                          </span><br><span class="line">                 ▼                          </span><br><span class="line">         ┌───────────────┐                  </span><br><span class="line">         │               │                  </span><br><span class="line">┌────────│ Load Balancer │─────────┐        </span><br><span class="line">│        │               │         │        </span><br><span class="line">│        └──────┬─┬──────┘         │        </span><br><span class="line">│               │ │                │        </span><br><span class="line">│               │ │                │        </span><br><span class="line">│        ┌──────┘ └────────┐       │        </span><br><span class="line">│        │ ┌─────────────┐ │       │┌──────┐</span><br><span class="line">│        │ │/write or UDP│ │       ││/query│</span><br><span class="line">│        ▼ └─────────────┘ ▼       │└──────┘</span><br><span class="line">│  ┌──────────┐      ┌──────────┐  │        </span><br><span class="line">│  │ InfluxDB │      │ InfluxDB │  │        </span><br><span class="line">│  │ Relay    │      │ Relay    │  │        </span><br><span class="line">│  └──┬────┬──┘      └────┬──┬──┘  │        </span><br><span class="line">│     │    |              |  │     │        </span><br><span class="line">│     |  ┌─┼──────────────┘  |     │        </span><br><span class="line">│     │  │ └──────────────┐  │     │        </span><br><span class="line">│     ▼  ▼                ▼  ▼     │        </span><br><span class="line">│  ┌──────────┐      ┌──────────┐  │        </span><br><span class="line">│  │          │      │          │  │        </span><br><span class="line">└─▶│ InfluxDB │      │ InfluxDB │◀─┘        </span><br><span class="line">   │          │      │          │           </span><br><span class="line">   └──────────┘      └──────────┘</span><br></pre></td></tr></table></figure>

<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>因为获取的数据来源是<code>influxdb</code>数据库，所以先搞清楚异常的原因是请求路径上的问题，还是<code>influxdb</code>数据库自身没有数据的问题：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">找到influxdb-nginx的service</span></span><br><span class="line">kubectl get svc  -n kube-system -owide</span><br><span class="line">NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE   SELECTOR</span><br><span class="line">grafana-service          ClusterIP   10.96.177.245   &lt;none&gt;        3000/TCP                 21d   app=grafana</span><br><span class="line">heapster                 ClusterIP   10.96.239.225   &lt;none&gt;        80/TCP                   21d   app=heapster</span><br><span class="line">influxdb-nginx-service   ClusterIP   10.96.170.72    &lt;none&gt;        7076/TCP                 21d   app=influxdb-nginx</span><br><span class="line">influxdb-relay-service   ClusterIP   10.96.196.45    &lt;none&gt;        9096/TCP                 21d   app=influxdb-relay</span><br><span class="line">influxdb-service         ClusterIP   10.96.127.45    &lt;none&gt;        8086/TCP                 21d   app=influxdb</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在集群节点上检查访问influxdb-nginx的service是否正常</span></span><br><span class="line">curl -i 10.96.170.72:7076/query</span><br><span class="line">HTTP/1.1 401 Unauthorized</span><br><span class="line">Server: nginx/1.17.2</span><br></pre></td></tr></table></figure>

<p>可以看出，请求发送到<code>influxdb-nginx</code>的<code>service</code>是正常的，也就是请求可以正常发送到后端的<code>influxdb</code>数据库。那就继续确认<code>influxdb</code>数据库自身没有数据的问题：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">找到influxdb数据库的pod</span></span><br><span class="line">kubectl get pod -n kube-system -owide |grep influxdb</span><br><span class="line">influxdb-nginx-4x8pr                       1/1     Running   3          21d   177.177.52.201    node3</span><br><span class="line">influxdb-nginx-tpngh                       1/1     Running   6          21d   177.177.41.214    node1</span><br><span class="line">influxdb-nginx-wh6kc                       1/1     Running   5          21d   177.177.250.180   node2</span><br><span class="line">influxdb-relay-rs-65c94bbf5f-dp7s4         1/1     Running   2          21d   177.177.250.148   node2</span><br><span class="line">influxdb1-6ff9466d46-q6w5r                 1/1     Running   3          21d   177.177.41.230    node1</span><br><span class="line">influxdb2-d6d6697f5-zzcnk                  1/1     Running   3          21d   177.177.250.161   node2</span><br><span class="line">influxdb3-65ddfc7476-hxhr8                 1/1     Running   4          21d   177.177.52.217    node3</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">登录任意一个influxdb容器内并进入交互式命令</span></span><br><span class="line">kubectl exec -it -n kube-systme influxdb-rs3-65ddfc7476-hxhr8 bash</span><br><span class="line">root@influxdb-rs3-65ddfc7476-hxhr8:/# influx</span><br><span class="line">Connected to http://localhost:8086 version 1.7.7</span><br><span class="line">InfluxDB shell version: 1.7.7</span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">auth</span></span><br><span class="line">username: admin</span><br><span class="line">password: xxx</span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">use xxx;</span></span><br><span class="line">Using database xxx</span><br></pre></td></tr></table></figure>

<p>根据业务层面的查询语句，在<code>influxdb</code>交互式命令下手工查询验证：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="keyword">select</span> <span class="built_in">sum</span>(value) from <span class="string">&quot;cpu/node_capacity&quot;</span> <span class="built_in">where</span> <span class="string">&quot;type&quot;</span> = <span class="string">&#x27;node&#x27;</span> and <span class="string">&quot;nodename&quot;</span> = <span class="string">&#x27;node1&#x27;</span> and time &gt; now() - 2m</span></span><br><span class="line"><span class="meta prompt_">&gt;</span></span><br></pre></td></tr></table></figure>

<p>结果发现确实没有查到数据，既然<code>2min</code>内的数据没有，那把时间线拉长一些看看呢？</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">不限制时间范围的查询</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="keyword">select</span> <span class="built_in">sum</span>(value) from <span class="string">&quot;cpu/node_capacity&quot;</span>&gt; <span class="keyword">select</span> <span class="built_in">sum</span>(value) from <span class="string">&quot;cpu/node_capacity&quot;</span>;</span></span><br><span class="line">name: cpu/node_capacity</span><br><span class="line">time sum</span><br><span class="line">---- ---</span><br><span class="line">0    5301432000</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查询72min内的数据</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="keyword">select</span> <span class="built_in">sum</span>(value) from <span class="string">&quot;cpu/node_capacity&quot;</span> <span class="built_in">where</span> <span class="string">&quot;type&quot;</span> = <span class="string">&#x27;node&#x27;</span> and <span class="string">&quot;nodename&quot;</span> = <span class="string">&#x27;node1&#x27;</span> and time &gt; now() - 72m</span></span><br><span class="line">name: cpu/node_capacity</span><br><span class="line">time                sum</span><br><span class="line">----                ---</span><br><span class="line">1624348319900503945 72000</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">sleep</span> 1min，继续查询72min内的数据</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="keyword">select</span> <span class="built_in">sum</span>(value) from <span class="string">&quot;cpu/node_capacity&quot;</span> <span class="built_in">where</span> <span class="string">&quot;type&quot;</span> = <span class="string">&#x27;node&#x27;</span> and <span class="string">&quot;nodename&quot;</span> = <span class="string">&#x27;node1&#x27;</span> and time &gt; now() - 72m</span></span><br><span class="line">name: cpu/node_capacity</span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash"></span><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查询73min内的数据</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash"><span class="keyword">select</span> <span class="built_in">sum</span>(value) from <span class="string">&quot;cpu/node_capacity&quot;</span> <span class="built_in">where</span> <span class="string">&quot;type&quot;</span> = <span class="string">&#x27;node&#x27;</span> and <span class="string">&quot;nodename&quot;</span> = <span class="string">&#x27;node1&#x27;</span> and time &gt; now() - 73m</span></span><br><span class="line">name: cpu/node_capacity</span><br><span class="line">time                sum</span><br><span class="line">----                ---</span><br><span class="line">1624348319900503945 72000</span><br></pre></td></tr></table></figure>

<p>根据查询结果看，不添加时间范围的查询是有记录的，并且通过多次验证看，<strong>数据无法获取的原因是数据在某个时间点不再写入导致的</strong>。查看<code>influxdb</code>的日志看看有没有什么相关日志：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs -n kube-systme influxdb-rs3-65ddfc7476-hxhr8</span><br><span class="line">ts=2021-06-22T09:56:49.658621Z lvl=warn msg=&quot;max-values-per-tag limit may be exceeded soon&quot; log_id=0UYIcREl000 service=store perc=100% n=100000 max=100000 db_instance=xxx measurement=network/rx tag=pod_name</span><br><span class="line">ts=2021-06-22T09:56:49.658702Z lvl=warn msg=&quot;max-values-per-tag limit may be exceeded soon&quot; log_id=0UYIcREl000 service=store perc=100% n=100000 max=100000 db_instance=xxx measurement=network/rx_errors tag=pod_name</span><br><span class="line">ts=2021-06-22T09:56:49.658815Z lvl=warn msg=&quot;max-values-per-tag limit may be exceeded soon&quot; log_id=0UYIcREl000 service=store perc=100% n=100000 max=100000 db_instance=xxx measurement=network/tx tag=pod_name</span><br><span class="line">ts=2021-06-22T09:56:49.658893Z lvl=warn msg=&quot;max-values-per-tag limit may be exceeded soon&quot; log_id=0UYIcREl000 service=store perc=100% n=100000 max=100000 db_instance=xxx measurement=network/tx_errors tag=pod_name</span><br><span class="line">ts=2021-06-22T09:56:49.659062Z lvl=warn msg=&quot;max-values-per-tag limit may be exceeded soon&quot; log_id=0UYIcREl000 service=store perc=100% n=100003 max=100000 db_instance=xxx measurement=uptime tag=pod_name</span><br></pre></td></tr></table></figure>

<p>果然，有大量<code>warn</code>日志，提示<code>max-values-per-tag limit may be exceeded soon</code>，从日志可以看出，这个参数的默认值为<code>100000</code>。通过搜索，找到了这个参数引入的issue[2]，引入原因大概意思是：</p>
<blockquote>
<p>如果不小心加载了大量的cardinality数据，那么当我们删除数据的时候，InfluxDB很容易会发生OOM。</p>
</blockquote>
<p>通过临时修改<code>max-values-per-tag</code>参数，验证问题是否解决</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cat influxdb.conf</span><br><span class="line">[meta]</span><br><span class="line">  dir = &quot;/var/lib/influxdb/meta&quot;</span><br><span class="line">[data]</span><br><span class="line">  dir = &quot;/var/lib/influxdb/data&quot;</span><br><span class="line">  engine = &quot;tsm1&quot;</span><br><span class="line">  wal-dir = &quot;/var/lib/influxdb/wal&quot;</span><br><span class="line">  max-series-per-database = 0</span><br><span class="line">  max-values-per-tag = 0</span><br><span class="line">[http]</span><br><span class="line">  auth-enabled = true</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete pod -n kube-system influxdb-rs1-6ff9466d46-q6w5r</span><br><span class="line">pod &quot;influxdb-rs1-6ff9466d46-q6w5r&quot; deleted</span><br><span class="line"></span><br><span class="line">kubectl delete pod -n kube-system influxdb-rs2-d6d6697f5-zzcnk</span><br><span class="line">pod &quot;influxdb-rs2-d6d6697f5-zzcnk&quot; deleted</span><br><span class="line"></span><br><span class="line">kubectl delete pod -n kube-system influxdb-rs3-65ddfc7476-hxhr8</span><br><span class="line">pod &quot;influxdb-rs3-65ddfc7476-hxhr8&quot; deleted</span><br></pre></td></tr></table></figure>

<p>再次观察业务层面获取的<code>Influxdb</code>监控数据，最终CPU、内存和磁盘使用率正常获取。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">监控项         使用率</span><br><span class="line">CPU(核)        19%</span><br><span class="line">内存(GB)       22%</span><br><span class="line">磁盘空间(GB)    2%</span><br></pre></td></tr></table></figure>

<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>根据业务情况，将<code>influxdb</code>的<code>max-values-per-tag</code>参数调整到合适值。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/influxdata/influxdb-relay">https://github.com/influxdata/influxdb-relay</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/influxdata/influxdb/issues/7146">https://github.com/influxdata/influxdb/issues/7146</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/06/26/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Pod%E9%97%B4%E9%80%9A%E8%BF%87%E6%9C%8D%E5%8A%A1%E5%90%8D%E8%AE%BF%E9%97%AE%E5%BC%82%E5%B8%B8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/26/k8s/K8S%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5-Pod%E9%97%B4%E9%80%9A%E8%BF%87%E6%9C%8D%E5%8A%A1%E5%90%8D%E8%AE%BF%E9%97%AE%E5%BC%82%E5%B8%B8/" class="post-title-link" itemprop="url">K8S问题排查-Pod间通过服务名访问异常</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-26 16:27:14" itemprop="dateCreated datePublished" datetime="2021-06-26T16:27:14+00:00">2021-06-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-07 09:04:28" itemprop="dateModified" datetime="2024-04-07T09:04:28+00:00">2024-04-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>K8S集群内，PodA使用服务名称访问PodB，请求出现异常。其中，PodA在<code>node1</code>节点上，PodB在<code>node2</code>节点上。</p>
<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>先上<code>tcpdump</code>，观察请求是否有异常：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# tcpdump -n -i ens192 port 50300</span><br><span class="line">...</span><br><span class="line">13:48:17.630335 IP 177.177.176.150.distinct -&gt; 10.96.22.136.50300:  UDP, length 214</span><br><span class="line">13:48:17.630407 IP 192.168.7.21.distinct  -&gt;  10.96.22.136.50300:   UDP, length 214</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>从抓包数据可以看出，请求源地址端口号为<code>177.177.176.150:50901</code>，目标地址端口号为<code>10.96.22.136:50300 </code>，其中<code>10.96.22.136</code>是PodA使用<code>server-svc</code>这个<code>serviceName</code>请求得到的目的地址，也就是<code>server-svc</code>对应的<code>serviceIP</code>，那就确认一下这个地址有没有问题：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl get pod -A -owide|grep server</span><br><span class="line">ss  server-xxx-xxx  1/1  Running 0 20h  177.177.176.150  node1</span><br><span class="line">ss  server-xxx-xxx  1/1  Running 0 20h  177.177.254.245  node2</span><br><span class="line">ss  server-xxx-xxx  1/1  Running 0 20h  177.177.18.152   node3</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl get svc -A -owide|grep server</span><br><span class="line">ss  server-svc  ClusterIP  10.96.182.195 &lt;none&gt;  50300/UDP</span><br></pre></td></tr></table></figure>

<p>可以看出，源地址没有问题，但目标地址跟预期不符，实际查到的服务名<code>server-svc</code>对应的地址为<code>10.96.182.195</code>，这是怎么回事儿呢？我们知道，K8S从v1.13版本开始默认使用<code>CoreDNS</code>作为服务发现，PodA使用服务名<code>server-svc</code>发起请求时，需要经过<code>CoreDNS</code>的解析，将服务名解析为<code>serviceIP</code>，那就登录到PodA内，验证域名解析是不是有问题：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl exec -it -n ss server-xxx-xxx -- cat /etc/resolve.conf</span><br><span class="line">nameserver 10.96.0.10</span><br><span class="line">search ss.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# kubectl exec -it -n ss server-xxx-xxx -- nslookup server-svc</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line"></span><br><span class="line">Name:    ss</span><br><span class="line">Address: 10.96.182.195</span><br></pre></td></tr></table></figure>

<p>从查看结果看，域名解析没有问题，PodA内也可以正确解析出<code>server-svc</code>对应的<code>serviceIP</code>为<code>10.96.182.195</code>，那最初使用<code>tcpdump</code>命令抓到的<code>serviceIP</code> 为<code>10.96.22.136</code>，难道这个地址是其他业务的服务，或者是残留的iptables规则，或者是有什么相关路由？分别查一下看看：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl get svc -A -owide|grep 10.96.22.136</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# iptables-save|grep 10.96.22.136</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# ip route|grep 10.96.22.136</span><br></pre></td></tr></table></figure>

<p>结果是，集群上根本不存在<code>10.96.22.136</code>这个地址，那PodA请求的目标地址为什么是它？既然主机上抓包时，目标地址已经是<code>10.96.22.136</code>，那再确认下出PodA时目标地址是什么：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# ip route|grep 177.177.176.150</span><br><span class="line">177.177.176.150 dev cali9afa4438787 scope link</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# tcpdump -n -i cali9afa4438787 port 50300</span><br><span class="line">...</span><br><span class="line">14:16:40.821511 IP 177.177.176.150.50902 -&gt;  10.96.22.136.50300:  UDP, length 214</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>原来出PodA时，目标地址已经是错误的<code>serviceIP</code>。而结合上面的域名解析的验证结果看，请求出PodA时的域名解析应该不存在问题。综合上面的定位情况，基本可以推测出，<strong>问题出在发送方</strong>。</p>
<p>为了进一步区分出，是PodA内的所有发送请求都存在问题，还是只有业务自身的发送请求存在问题，我们使用<code>nc</code>命令在PodA内模拟发送一个<code>UDP</code>数据包，然后在主机上抓包验证（PodA内恰巧有<code>nc</code>命令，如果没有，感兴趣的同学可以使用&#x2F;dev&#x2F;{tcp|udp}模拟[1]）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl exec -it -n ss server-xxx-xxx -- echo “test” | nc -u server-svc 50300 -p 9999</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# tcpdump -n -i cali9afa4438787 port 50300</span><br><span class="line">...</span><br><span class="line">15:46:45.871580 IP 177.177.176.150.50902 -&gt;  10.96.182.195.50300:  UDP, length 54</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>可以看出，PodA内模拟发送的请求，目标地址是可以正确解析的，也就把问题限定在了<strong>业务自身的发送请求存在问题</strong>。因为问题是服务名没有解析为正确的IP地址，所以怀疑是业务使用了什么缓存，如果猜想正确，那么重启PodA，理论上可以解决。而考虑到业务是多副本的，我们重启其中一个，其他副本上的问题环境还可以保留，跟开发沟通后重启并验证业务的请求：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# docker ps |grep server-xxx-xxx | grep -v POD |awk &#x27;&#123;print $1&#125;&#x27; |xargs docker restart</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# tcpdump -n -i ens192 port 50300</span><br><span class="line">...</span><br><span class="line">15:58:17.150535 IP 177.177.176.150.distinct -&gt; 10.96.182.195.50300:  UDP, length 214</span><br><span class="line">15:58:17.150607 IP 192.168.7.21.distinct  -&gt;  10.96.182.195.50300:   UDP, length 214</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>验证符合预期，进一步证明了业务可能是使用了什么缓存。与开发同学了解，业务的发送使用的是java原生的API发送<code>UDP</code>数据，会不会是java在使用域名建立socket时默认会做缓存呢？</p>
<p>通过一番搜索，找了一篇相关博客[2]，关键内容附上：</p>
<blockquote>
<p>在通过DNS查找域名的过程中，可能会经过多台中间DNS服务器才能找到指定的域名，因此，在DNS服务器上查找域名是非常昂贵的操作。在Java中为了缓解这个问题，提供了DNS缓存。当InetAddress类第一次使用某个域名创建InetAddress对象后，JVM就会将这个域名和它从DNS上获得的信息（如IP地址）都保存在DNS缓存中。当下一次InetAddress类再使用这个域名时，就直接从DNS缓存里获得所需的信息，而无需再访问DNS服务器。</p>
</blockquote>
<p>还真是，继续看怎么解决：</p>
<blockquote>
<p>DNS缓存在默认时将永远保留曾经访问过的域名信息，但我们可以修改这个默认值。一般有两种方法可以修改这个默认值：</p>
<ol>
<li><p>在程序中通过java.security.Security.setProperty方法设置安全属性networkaddress.cache.ttl的值（单位：秒）</p>
</li>
<li><p>设置java.security文件中的networkaddress.cache.negative.ttl属性。假设JDK的安装目录是C:&#x2F;jdk1.6，那么java.security文件位于c:&#x2F;jdk1.6&#x2F;jre&#x2F;lib&#x2F;security目录中。打开这个文件，找到networkaddress.cache.ttl属性，并将这个属性值设为相应的缓存超时（单位：秒）</p>
</li>
</ol>
<p> 注：如果将networkaddress.cache.ttl属性值设为-1，那么DNS缓存数据将永远不会释放。</p>
</blockquote>
<p>至此，问题定位结束。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>业务侧根据业务场景调整DNS缓存的设置。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/michaelwoshi/article/details/101107042">https://blog.csdn.net/michaelwoshi/article/details/101107042</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/turkeyzhou/article/details/5510960">https://blog.csdn.net/turkeyzhou/article/details/5510960</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lyyao09.github.io/2021/06/20/tools/%E5%B7%A5%E5%85%B7%E5%88%86%E4%BA%AB-%E4%BD%BF%E7%94%A8%E9%98%BF%E9%87%8C%E5%BC%80%E6%BA%90%E7%9A%84Sealer%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E9%9B%86%E7%BE%A4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LeaoYao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云原生知识星球">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/20/tools/%E5%B7%A5%E5%85%B7%E5%88%86%E4%BA%AB-%E4%BD%BF%E7%94%A8%E9%98%BF%E9%87%8C%E5%BC%80%E6%BA%90%E7%9A%84Sealer%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E9%9B%86%E7%BE%A4/" class="post-title-link" itemprop="url">工具分享-使用阿里开源的Sealer快速部署K8S集群</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-20 22:07:47" itemprop="dateCreated datePublished" datetime="2021-06-20T22:07:47+00:00">2021-06-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-07 09:04:28" itemprop="dateModified" datetime="2024-04-07T09:04:28+00:00">2024-04-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tools/" itemprop="url" rel="index"><span itemprop="name">tools</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>32 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="什么是Sealer"><a href="#什么是Sealer" class="headerlink" title="什么是Sealer"></a>什么是Sealer</h2><p>引用官方文档的介绍[1]：</p>
<blockquote>
<ul>
<li>sealer[ˈsiːlər]是一款分布式应用打包交付运行的解决方案，通过把分布式应用及其数据库中间件等依赖一起打包以解决复杂应用的交付问题。</li>
<li>sealer构建出来的产物我们称之为“集群镜像”， 集群镜像里内嵌了一个kubernetes，解决了分布式应用的交付一致性问题。</li>
<li>集群镜像可以push到registry中共享给其他用户使用，也可以在官方仓库中找到非常通用的分布式软件直接使用。</li>
<li>Docker可以把一个操作系统的rootfs+应用 build成一个容器镜像，sealer把kubernetes看成操作系统，在这个更高的抽象纬度上做出来的镜像就是集群镜像。 实现整个集群的Build Share Run !!!</li>
</ul>
</blockquote>
<h2 id="快速部署K8S集群"><a href="#快速部署K8S集群" class="headerlink" title="快速部署K8S集群"></a>快速部署K8S集群</h2><p>准备一个节点，先下载并安装Sealer：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">[root@node1]# </span><span class="language-bash">wget https://github.com/alibaba/sealer/releases/download/v0.1.5/sealer-v0.1.5-linux-amd64.tar.gz &amp;&amp; tar zxvf sealer-v0.1.5-linux-amd64.tar.gz &amp;&amp; <span class="built_in">mv</span> sealer /usr/bin</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">[root@node1]# </span><span class="language-bash">sealer version</span></span><br><span class="line">&#123;&quot;gitVersion&quot;:&quot;v0.1.5&quot;,&quot;gitCommit&quot;:&quot;9143e60&quot;,&quot;buildDate&quot;:&quot;2021-06-04 07:41:03&quot;,&quot;goVersion&quot;:&quot;go1.14.15&quot;,&quot;compiler&quot;:&quot;gc&quot;,&quot;platform&quot;:&quot;linux/amd64&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>根据官方文档，如果要在一个已存在的机器上部署kubernetes，直接执行以下命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">[root@node1]# </span><span class="language-bash">sealer run kubernetes:v1.19.9 --masters xx.xx.xx.xx --passwd xxxx</span></span><br><span class="line">2021-06-19 17:22:14 [WARN] [registry_client.go:37] failed to get auth info for registry.cn-qingdao.aliyuncs.com, err: auth for registry.cn-qingdao.aliyuncs.com doesn&#x27;t exist</span><br><span class="line">2021-06-19 17:22:15 [INFO] [current_cluster.go:39] current cluster not found, will create a new cluster new kube build config failed: stat /root/.kube/config: no such file or directory</span><br><span class="line">2021-06-19 17:22:15 [WARN] [default_image.go:89] failed to get auth info, err: auth for registry.cn-qingdao.aliyuncs.com doesn&#x27;t exist</span><br><span class="line">Start to Pull Image kubernetes:v1.19.9</span><br><span class="line">191908a896ce: pull completed </span><br><span class="line">2021-06-19 17:22:49 [INFO] [filesystem.go:88] image name is registry.cn-qingdao.aliyuncs.com/sealer-io/kubernetes:v1.19.9.alpha.1</span><br><span class="line">2021-06-19 17:22:49 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : mkdir -p /var/lib/sealer/data/my-cluster || true</span><br><span class="line">copying files to 10.10.11.49: 198/198 </span><br><span class="line">2021-06-19 17:25:22 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : cd /var/lib/sealer/data/my-cluster/rootfs  &amp;&amp; chmod +x scripts/* &amp;&amp; cd scripts &amp;&amp; sh init.sh</span><br><span class="line">+ storage=/var/lib/docker</span><br><span class="line">+ mkdir -p /var/lib/docker</span><br><span class="line">+ command_exists docker</span><br><span class="line">+ command -v docker</span><br><span class="line">+ systemctl daemon-reload</span><br><span class="line">+ systemctl restart docker.service</span><br><span class="line">++ docker info</span><br><span class="line">++ grep Cg</span><br><span class="line">+ cgroupDriver=&#x27; Cgroup Driver: cgroupfs&#x27;</span><br><span class="line">+ driver=cgroupfs</span><br><span class="line">+ echo &#x27;driver is cgroupfs&#x27;</span><br><span class="line">driver is cgroupfs</span><br><span class="line">+ export criDriver=cgroupfs</span><br><span class="line">+ criDriver=cgroupfs</span><br><span class="line">* Applying /usr/lib/sysctl.d/00-system.conf ...</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 0</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 0</span><br><span class="line">net.bridge.bridge-nf-call-arptables = 0</span><br><span class="line">* Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...</span><br><span class="line">kernel.yama.ptrace_scope = 0</span><br><span class="line">* Applying /usr/lib/sysctl.d/50-default.conf ...</span><br><span class="line">kernel.sysrq = 16</span><br><span class="line">kernel.core_uses_pid = 1</span><br><span class="line">net.ipv4.conf.default.rp_filter = 1</span><br><span class="line">net.ipv4.conf.all.rp_filter = 1</span><br><span class="line">net.ipv4.conf.default.accept_source_route = 0</span><br><span class="line">net.ipv4.conf.all.accept_source_route = 0</span><br><span class="line">net.ipv4.conf.default.promote_secondaries = 1</span><br><span class="line">net.ipv4.conf.all.promote_secondaries = 1</span><br><span class="line">fs.protected_hardlinks = 1</span><br><span class="line">fs.protected_symlinks = 1</span><br><span class="line">* Applying /usr/lib/sysctl.d/60-libvirtd.conf ...</span><br><span class="line">fs.aio-max-nr = 1048576</span><br><span class="line">* Applying /etc/sysctl.d/99-sysctl.conf ...</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">net.ipv4.conf.all.rp_filter = 1</span><br><span class="line">* Applying /etc/sysctl.d/k8s.conf ...</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.ipv4.conf.all.rp_filter = 0</span><br><span class="line">* Applying /etc/sysctl.conf ...</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">net.ipv4.conf.all.rp_filter = 1</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">2021-06-19 17:25:26 [INFO] [runtime.go:107] metadata version v1.19.9</span><br><span class="line">2021-06-19 17:25:26 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : cd /var/lib/sealer/data/my-cluster/rootfs &amp;&amp; echo &quot;</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta1</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.19.9</span><br><span class="line">controlPlaneEndpoint: &quot;apiserver.cluster.local:6443&quot;</span><br><span class="line">imageRepository: sea.hub:5000/library</span><br><span class="line">networking:</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">dnsDomain: cluster.local</span></span><br><span class="line">  podSubnet: 100.64.0.0/10</span><br><span class="line">  serviceSubnet: 10.96.0.0/22</span><br><span class="line">apiServer:</span><br><span class="line">  certSANs:</span><br><span class="line">  - 127.0.0.1</span><br><span class="line">  - apiserver.cluster.local</span><br><span class="line">  - 10.10.11.49</span><br><span class="line">  - aliyun-inc.com</span><br><span class="line">  - 10.0.0.2</span><br><span class="line">  - 127.0.0.1</span><br><span class="line">  - apiserver.cluster.local</span><br><span class="line">  - 10.103.97.2</span><br><span class="line">  - 10.10.11.49</span><br><span class="line">  - 10.103.97.2</span><br><span class="line">  extraArgs:</span><br><span class="line">    etcd-servers: https://10.10.11.49:2379</span><br><span class="line">    feature-gates: TTLAfterFinished=true,EphemeralContainers=true</span><br><span class="line">    audit-policy-file: &quot;/etc/kubernetes/audit-policy.yml&quot;</span><br><span class="line">    audit-log-path: &quot;/var/log/kubernetes/audit.log&quot;</span><br><span class="line">    audit-log-format: json</span><br><span class="line">    audit-log-maxbackup: &#x27;&quot;10&quot;&#x27;</span><br><span class="line">    audit-log-maxsize: &#x27;&quot;100&quot;&#x27;</span><br><span class="line">    audit-log-maxage: &#x27;&quot;7&quot;&#x27;</span><br><span class="line">    enable-aggregator-routing: &#x27;&quot;true&quot;&#x27;</span><br><span class="line">  extraVolumes:</span><br><span class="line">  - name: &quot;audit&quot;</span><br><span class="line">    hostPath: &quot;/etc/kubernetes&quot;</span><br><span class="line">    mountPath: &quot;/etc/kubernetes&quot;</span><br><span class="line">    pathType: DirectoryOrCreate</span><br><span class="line">  - name: &quot;audit-log&quot;</span><br><span class="line">    hostPath: &quot;/var/log/kubernetes&quot;</span><br><span class="line">    mountPath: &quot;/var/log/kubernetes&quot;</span><br><span class="line">    pathType: DirectoryOrCreate</span><br><span class="line">  - name: localtime</span><br><span class="line">    hostPath: /etc/localtime</span><br><span class="line">    mountPath: /etc/localtime</span><br><span class="line">    readOnly: true</span><br><span class="line">    pathType: File</span><br><span class="line">controllerManager:</span><br><span class="line">  extraArgs:</span><br><span class="line">    feature-gates: TTLAfterFinished=true,EphemeralContainers=true</span><br><span class="line">    experimental-cluster-signing-duration: 876000h</span><br><span class="line">  extraVolumes:</span><br><span class="line">  - hostPath: /etc/localtime</span><br><span class="line">    mountPath: /etc/localtime</span><br><span class="line">    name: localtime</span><br><span class="line">    readOnly: true</span><br><span class="line">    pathType: File</span><br><span class="line">scheduler:</span><br><span class="line">  extraArgs:</span><br><span class="line">    feature-gates: TTLAfterFinished=true,EphemeralContainers=true</span><br><span class="line">  extraVolumes:</span><br><span class="line">  - hostPath: /etc/localtime</span><br><span class="line">    mountPath: /etc/localtime</span><br><span class="line">    name: localtime</span><br><span class="line">    readOnly: true</span><br><span class="line">    pathType: File</span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-metrics-urls: http://0.0.0.0:2381</span><br><span class="line">&quot; &gt; kubeadm-config.yaml</span><br><span class="line">2021-06-19 17:25:27 [INFO] [kube_certs.go:234] APIserver altNames :  &#123;map[aliyun-inc.com:aliyun-inc.com apiserver.cluster.local:apiserver.cluster.local kubernetes:kubernetes kubernetes.default:kubernetes.default kubernetes.default.svc:kubernetes.default.svc kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local localhost:localhost node1:node1] map[10.0.0.2:10.0.0.2 10.103.97.2:10.103.97.2 10.96.0.1:10.96.0.1 127.0.0.1:127.0.0.1 10.10.11.49:10.10.11.49]&#125;</span><br><span class="line">2021-06-19 17:25:27 [INFO] [kube_certs.go:254] Etcd altnames : &#123;map[localhost:localhost node1:node1] map[127.0.0.1:127.0.0.1 10.10.11.49:10.10.11.49 ::1:::1]&#125;, commonName : node1</span><br><span class="line">2021-06-19 17:25:30 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : mkdir -p /etc/kubernetes || true</span><br><span class="line">copying files to 10.10.11.49: 22/22 </span><br><span class="line">2021-06-19 17:25:43 [INFO] [kubeconfig.go:267] [kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file</span><br><span class="line">2021-06-19 17:25:43 [INFO] [kubeconfig.go:267] [kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file</span><br><span class="line">2021-06-19 17:25:43 [INFO] [kubeconfig.go:267] [kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file</span><br><span class="line">2021-06-19 17:25:43 [INFO] [kubeconfig.go:267] [kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file</span><br><span class="line">2021-06-19 17:25:44 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : mkdir -p /etc/kubernetes &amp;&amp; cp -f /var/lib/sealer/data/my-cluster/rootfs/statics/audit-policy.yml /etc/kubernetes/audit-policy.yml</span><br><span class="line">2021-06-19 17:25:44 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : cd /var/lib/sealer/data/my-cluster/rootfs/scripts &amp;&amp; sh init-registry.sh 5000 /var/lib/sealer/data/my-cluster/rootfs/registry</span><br><span class="line">++ dirname init-registry.sh</span><br><span class="line">+ cd .</span><br><span class="line">+ REGISTRY_PORT=5000</span><br><span class="line">+ VOLUME=/var/lib/sealer/data/my-cluster/rootfs/registry</span><br><span class="line">+ container=sealer-registry</span><br><span class="line">+ mkdir -p /var/lib/sealer/data/my-cluster/rootfs/registry</span><br><span class="line">+ docker load -q -i ../images/registry.tar</span><br><span class="line">Loaded image: registry:2.7.1</span><br><span class="line">+ docker run -d --restart=always --name sealer-registry -p 5000:5000 -v /var/lib/sealer/data/my-cluster/rootfs/registry:/var/lib/registry registry:2.7.1</span><br><span class="line">e35aeefcfb415290764773f28dd843fc53dab8d1210373ca2c0f1f4773391686</span><br><span class="line">2021-06-19 17:25:45 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : mkdir -p /etc/kubernetes || true</span><br><span class="line">copying files to 10.10.11.49: 1/1 </span><br><span class="line">2021-06-19 17:25:46 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : mkdir -p /etc/kubernetes || true</span><br><span class="line">copying files to 10.10.11.49: 1/1 </span><br><span class="line">2021-06-19 17:25:47 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : mkdir -p /etc/kubernetes || true</span><br><span class="line">copying files to 10.10.11.49: 1/1 </span><br><span class="line">2021-06-19 17:25:48 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : mkdir -p /etc/kubernetes || true</span><br><span class="line">copying files to 10.10.11.49: 1/1 </span><br><span class="line">2021-06-19 17:25:49 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : echo 10.10.11.49 apiserver.cluster.local &gt;&gt; /etc/hosts</span><br><span class="line">2021-06-19 17:25:50 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : echo 10.10.11.49 sea.hub &gt;&gt; /etc/hosts</span><br><span class="line">2021-06-19 17:25:50 [INFO] [init.go:211] start to init master0...</span><br><span class="line">[ssh][10.10.11.49]failed to run command [kubeadm init --config=/var/lib/sealer/data/my-cluster/rootfs/kubeadm-config.yaml --upload-certs -v 0 --ignore-preflight-errors=SystemVerification],output is: W0619 17:25:50.649054  122163 common.go:77] your configuration file uses a deprecated API spec: &quot;kubeadm.k8s.io/v1beta1&quot;. Please use &#x27;kubeadm config migrate --old-config old.yaml --new-config new.yaml&#x27;, which will write the new, similar spec using a newer API version.</span><br><span class="line"></span><br><span class="line">W0619 17:25:50.702549  122163 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]</span><br><span class="line">[init] Using Kubernetes version: v1.19.9</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/</span><br><span class="line">[WARNING FileExisting-socat]: socat not found in system path</span><br><span class="line">[WARNING Hostname]: hostname &quot;node1&quot; could not be reached</span><br><span class="line">[WARNING Hostname]: hostname &quot;node1&quot;: lookup node1 on 10.72.66.37:53: no such host</span><br><span class="line">[preflight] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action in beforehand using &#x27;kubeadm config images pull&#x27;</span><br><span class="line">error execution phase preflight: [preflight] Some fatal errors occurred:</span><br><span class="line"></span><br><span class="line">[ERROR ImagePull]: failed to pull image sea.hub:5000/library/kube-apiserver:v1.19.9: output: Error response from daemon: Get https://sea.hub:5000/v2/: http: server gave HTTP response to HTTPS client, error: exit status 1</span><br><span class="line">[ERROR ImagePull]: failed to pull image sea.hub:5000/library/kube-controller-manager:v1.19.9: output: Error response from daemon: Get https://sea.hub:5000/v2/: http: server gave HTTP response to HTTPS client, error: exit status 1</span><br><span class="line">[ERROR ImagePull]: failed to pull image sea.hub:5000/library/kube-scheduler:v1.19.9: output: Error response from daemon: Get https://sea.hub:5000/v2/: http: server gave HTTP response to HTTPS client, error: exit status 1</span><br><span class="line">[ERROR ImagePull]: failed to pull image sea.hub:5000/library/kube-proxy:v1.19.9: output: Error response from daemon: Get https://sea.hub:5000/v2/: http: server gave HTTP response to HTTPS client, error: exit status 1</span><br><span class="line">[ERROR ImagePull]: failed to pull image sea.hub:5000/library/pause:3.2: output: Error response from daemon: Get https://sea.hub:5000/v2/: http: server gave HTTP response to HTTPS client, error: exit status 1</span><br><span class="line">[ERROR ImagePull]: failed to pull image sea.hub:5000/library/etcd:3.4.13-0: output: Error response from daemon: Get https://sea.hub:5000/v2/: http: server gave HTTP response to HTTPS client, error: exit status 1</span><br><span class="line">[ERROR ImagePull]: failed to pull image sea.hub:5000/library/coredns:1.7.0: output: Error response from daemon: Get https://sea.hub:5000/v2/: http: server gave HTTP response to HTTPS client, error: exit status 1</span><br><span class="line">[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`</span><br><span class="line">To see the stack trace of this error execute with --v=5 or higher</span><br><span class="line">2021-06-19 17:25:52 [EROR] [run.go:55] init master0 failed, error: [ssh][10.10.11.49]run command failed [kubeadm init --config=/var/lib/sealer/data/my-cluster/rootfs/kubeadm-config.yaml --upload-certs -v 0 --ignore-preflight-errors=SystemVerification]. Please clean and reinstall</span><br></pre></td></tr></table></figure>

<p>部署报错，从错误日志看，是尝试访问Sealer自己搭建的私有registry异常。从报错信息<code>server gave HTTP response to HTTPS client</code>可以知道，应该是docker中没有配置<code>insecure-registries</code>字段导致的。查看docker的配置文件确认一下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">[root@node1]# </span><span class="language-bash"><span class="built_in">cat</span> /etc/docker/daemon.json</span> </span><br><span class="line">&#123;</span><br><span class="line">  &quot;max-concurrent-downloads&quot;: 10,</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-level&quot;: &quot;warn&quot;,</span><br><span class="line">  &quot;insecure-registries&quot;:[&quot;127.0.0.1&quot;],</span><br><span class="line">  &quot;data-root&quot;:&quot;/var/lib/docker&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看出，<code>insecure-registries</code>字段配置的不对，考虑到该节点在部署之前已经安装过docker，所以不确定这个配置是之前就存在，还是Sealer配置错了，那就自己修改一下吧：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">[root@node1]# </span><span class="language-bash"><span class="built_in">cat</span> /etc/docker/daemon.json</span> </span><br><span class="line">&#123;</span><br><span class="line">  &quot;max-concurrent-downloads&quot;: 10,</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-level&quot;: &quot;warn&quot;,</span><br><span class="line">  &quot;insecure-registries&quot;:[&quot;sea.hub:5000&quot;],</span><br><span class="line">  &quot;data-root&quot;:&quot;/var/lib/docker&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>再次执行部署命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br></pre></td><td class="code"><pre><span class="line">sealer run kubernetes:v1.19.9 --masters xx.xx.xx.xx --passwd xxxx</span><br><span class="line">...</span><br><span class="line">2021-06-19 17:43:56 [INFO] [kubeconfig.go:277] [kubeconfig] Using existing kubeconfig file: &quot;/var/lib/sealer/data/my-cluster/admin.conf&quot;</span><br><span class="line">2021-06-19 17:43:57 [INFO] [kubeconfig.go:277] [kubeconfig] Using existing kubeconfig file: &quot;/var/lib/sealer/data/my-cluster/controller-manager.conf&quot;</span><br><span class="line">2021-06-19 17:43:57 [INFO] [kubeconfig.go:277] [kubeconfig] Using existing kubeconfig file: &quot;/var/lib/sealer/data/my-cluster/scheduler.conf&quot;</span><br><span class="line">2021-06-19 17:43:57 [INFO] [kubeconfig.go:277] [kubeconfig] Using existing kubeconfig file: &quot;/var/lib/sealer/data/my-cluster/kubelet.conf&quot;</span><br><span class="line">2021-06-19 17:43:57 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : mkdir -p /etc/kubernetes &amp;&amp; cp -f /var/lib/sealer/data/my-cluster/rootfs/statics/audit-policy.yml /etc/kubernetes/audit-policy.yml</span><br><span class="line">2021-06-19 17:43:57 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : cd /var/lib/sealer/data/my-cluster/rootfs/scripts &amp;&amp; sh init-registry.sh 5000 /var/lib/sealer/data/my-cluster/rootfs/registry</span><br><span class="line">++ dirname init-registry.sh</span><br><span class="line">+ cd .</span><br><span class="line">+ REGISTRY_PORT=5000</span><br><span class="line">+ VOLUME=/var/lib/sealer/data/my-cluster/rootfs/registry</span><br><span class="line">+ container=sealer-registry</span><br><span class="line">+ mkdir -p /var/lib/sealer/data/my-cluster/rootfs/registry</span><br><span class="line">+ docker load -q -i ../images/registry.tar</span><br><span class="line">Loaded image: registry:2.7.1</span><br><span class="line">+ docker run -d --restart=always --name sealer-registry -p 5000:5000 -v /var/lib/sealer/data/my-cluster/rootfs/registry:/var/lib/registry registry:2.7.1</span><br><span class="line">docker: Error response from daemon: Conflict. The container name &quot;/sealer-registry&quot; is already in use by container &quot;e35aeefcfb415290764773f28dd843fc53dab8d1210373ca2c0f1f4773391686&quot;. You have to remove (or rename) that container to be able to reuse that name.</span><br><span class="line">See &#x27;docker run --help&#x27;.</span><br><span class="line">+ true</span><br><span class="line">2021-06-19 17:43:58 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : mkdir -p /etc/kubernetes || true</span><br><span class="line">copying files to 10.10.11.49: 1/1 </span><br><span class="line">2021-06-19 17:43:59 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : mkdir -p /etc/kubernetes || true</span><br><span class="line">copying files to 10.10.11.49: 1/1 </span><br><span class="line">2021-06-19 17:44:00 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : mkdir -p /etc/kubernetes || true</span><br><span class="line">copying files to 10.10.11.49: 1/1 </span><br><span class="line">2021-06-19 17:44:01 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : mkdir -p /etc/kubernetes || true</span><br><span class="line">copying files to 10.10.11.49: 1/1 </span><br><span class="line">2021-06-19 17:44:02 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : echo 10.10.11.49 apiserver.cluster.local &gt;&gt; /etc/hosts</span><br><span class="line">2021-06-19 17:44:02 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : echo 10.10.11.49 sea.hub &gt;&gt; /etc/hosts</span><br><span class="line">2021-06-19 17:44:03 [INFO] [init.go:211] start to init master0...</span><br><span class="line">2021-06-19 17:46:53 [INFO] [init.go:286] [globals]join command is:  apiserver.cluster.local:6443 --token comygj.c0kj18d7fh2h4xta \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:cd8988f9a061765914dddb24d4e578ad446d8d31b0e30dba96a89e0c4f1e7240 \</span><br><span class="line">    --control-plane --certificate-key b27f10340d2f89790f7e980af72cf9d54d790b53bfd4da823947d914359d6e81</span><br><span class="line"></span><br><span class="line">2021-06-19 17:46:53 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : rm -rf .kube/config &amp;&amp; mkdir -p /root/.kube &amp;&amp; cp /etc/kubernetes/admin.conf /root/.kube/config</span><br><span class="line">2021-06-19 17:46:53 [INFO] [init.go:230] start to install CNI</span><br><span class="line">2021-06-19 17:46:53 [INFO] [init.go:250] render cni yaml success</span><br><span class="line">2021-06-19 17:46:54 [INFO] [sshcmd.go:48] [ssh][10.10.11.49] : echo &#x27;</span><br><span class="line">---</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Source: calico/templates/calico-config.yaml</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">This ConfigMap is used to configure a self-hosted Calico installation.</span></span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-config</span><br><span class="line">  namespace: kube-system</span><br><span class="line">data:</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">Typha is disabled.</span></span><br><span class="line">  typha_service_name: &quot;none&quot;</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">Configure the backend to use.</span></span><br><span class="line">  calico_backend: &quot;bird&quot;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">Configure the MTU to use</span></span><br><span class="line">  veth_mtu: &quot;1550&quot;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">The CNI network configuration to install on each node.  The special</span></span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">values <span class="keyword">in</span> this config will be automatically populated.</span></span><br><span class="line">  cni_network_config: |-</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;k8s-pod-network&quot;,</span><br><span class="line">      &quot;cniVersion&quot;: &quot;0.3.1&quot;,</span><br><span class="line">      &quot;plugins&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;type&quot;: &quot;calico&quot;,</span><br><span class="line">          &quot;log_level&quot;: &quot;info&quot;,</span><br><span class="line">          &quot;datastore_type&quot;: &quot;kubernetes&quot;,</span><br><span class="line">          &quot;nodename&quot;: &quot;__KUBERNETES_NODE_NAME__&quot;,</span><br><span class="line">          &quot;mtu&quot;: __CNI_MTU__,</span><br><span class="line">          &quot;ipam&quot;: &#123;</span><br><span class="line">              &quot;type&quot;: &quot;calico-ipam&quot;</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;policy&quot;: &#123;</span><br><span class="line">              &quot;type&quot;: &quot;k8s&quot;</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;kubernetes&quot;: &#123;</span><br><span class="line">              &quot;kubeconfig&quot;: &quot;__KUBECONFIG_FILEPATH__&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;type&quot;: &quot;portmap&quot;,</span><br><span class="line">          &quot;snat&quot;: true,</span><br><span class="line">          &quot;capabilities&quot;: &#123;&quot;portMappings&quot;: true&#125;</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">---</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Source: calico/templates/kdd-crds.yaml</span></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">   name: felixconfigurations.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: FelixConfiguration</span><br><span class="line">    plural: felixconfigurations</span><br><span class="line">    singular: felixconfiguration</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: ipamblocks.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: IPAMBlock</span><br><span class="line">    plural: ipamblocks</span><br><span class="line">    singular: ipamblock</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: blockaffinities.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: BlockAffinity</span><br><span class="line">    plural: blockaffinities</span><br><span class="line">    singular: blockaffinity</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: ipamhandles.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: IPAMHandle</span><br><span class="line">    plural: ipamhandles</span><br><span class="line">    singular: ipamhandle</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: ipamconfigs.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: IPAMConfig</span><br><span class="line">    plural: ipamconfigs</span><br><span class="line">    singular: ipamconfig</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: bgppeers.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: BGPPeer</span><br><span class="line">    plural: bgppeers</span><br><span class="line">    singular: bgppeer</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: bgpconfigurations.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: BGPConfiguration</span><br><span class="line">    plural: bgpconfigurations</span><br><span class="line">    singular: bgpconfiguration</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: ippools.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: IPPool</span><br><span class="line">    plural: ippools</span><br><span class="line">    singular: ippool</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: hostendpoints.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: HostEndpoint</span><br><span class="line">    plural: hostendpoints</span><br><span class="line">    singular: hostendpoint</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: clusterinformations.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: ClusterInformation</span><br><span class="line">    plural: clusterinformations</span><br><span class="line">    singular: clusterinformation</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: globalnetworkpolicies.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: GlobalNetworkPolicy</span><br><span class="line">    plural: globalnetworkpolicies</span><br><span class="line">    singular: globalnetworkpolicy</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: globalnetworksets.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Cluster</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: GlobalNetworkSet</span><br><span class="line">    plural: globalnetworksets</span><br><span class="line">    singular: globalnetworkset</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: networkpolicies.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Namespaced</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: NetworkPolicy</span><br><span class="line">    plural: networkpolicies</span><br><span class="line">    singular: networkpolicy</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: networksets.crd.projectcalico.org</span><br><span class="line">spec:</span><br><span class="line">  scope: Namespaced</span><br><span class="line">  group: crd.projectcalico.org</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: NetworkSet</span><br><span class="line">    plural: networksets</span><br><span class="line">    singular: networkset</span><br><span class="line">---</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Source: calico/templates/rbac.yaml</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Include a clusterrole <span class="keyword">for</span> the kube-controllers component,</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">and <span class="built_in">bind</span> it to the calico-kube-controllers serviceaccount.</span></span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-kube-controllers</span><br><span class="line">rules:</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">Nodes are watched to monitor <span class="keyword">for</span> deletions.</span></span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - nodes</span><br><span class="line">    verbs:</span><br><span class="line">      - watch</span><br><span class="line">      - list</span><br><span class="line">      - get</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">Pods are queried to check <span class="keyword">for</span> existence.</span></span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - pods</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">IPAM resources are manipulated when nodes are deleted.</span></span><br><span class="line">  - apiGroups: [&quot;crd.projectcalico.org&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - ippools</span><br><span class="line">    verbs:</span><br><span class="line">      - list</span><br><span class="line">  - apiGroups: [&quot;crd.projectcalico.org&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - blockaffinities</span><br><span class="line">      - ipamblocks</span><br><span class="line">      - ipamhandles</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - create</span><br><span class="line">      - update</span><br><span class="line">      - delete</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">Needs access to update clusterinformations.</span></span><br><span class="line">  - apiGroups: [&quot;crd.projectcalico.org&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - clusterinformations</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - create</span><br><span class="line">      - update</span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-kube-controllers</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: calico-kube-controllers</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: calico-kube-controllers</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Include a clusterrole <span class="keyword">for</span> the calico-node DaemonSet,</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">and <span class="built_in">bind</span> it to the calico-node serviceaccount.</span></span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-node</span><br><span class="line">rules:</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">The CNI plugin needs to get pods, nodes, and namespaces.</span></span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - pods</span><br><span class="line">      - nodes</span><br><span class="line">      - namespaces</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - endpoints</span><br><span class="line">      - services</span><br><span class="line">    verbs:</span><br><span class="line">      # Used to discover service IPs for advertisement.</span><br><span class="line">      - watch</span><br><span class="line">      - list</span><br><span class="line">      # Used to discover Typhas.</span><br><span class="line">      - get</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - nodes/status</span><br><span class="line">    verbs:</span><br><span class="line">      # Needed for clearing NodeNetworkUnavailable flag.</span><br><span class="line">      - patch</span><br><span class="line">      # Calico stores some configuration information in node annotations.</span><br><span class="line">      - update</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">Watch <span class="keyword">for</span> changes to Kubernetes NetworkPolicies.</span></span><br><span class="line">  - apiGroups: [&quot;networking.k8s.io&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - networkpolicies</span><br><span class="line">    verbs:</span><br><span class="line">      - watch</span><br><span class="line">      - list</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">Used by Calico <span class="keyword">for</span> policy information.</span></span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - pods</span><br><span class="line">      - namespaces</span><br><span class="line">      - serviceaccounts</span><br><span class="line">    verbs:</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">The CNI plugin patches pods/status.</span></span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - pods/status</span><br><span class="line">    verbs:</span><br><span class="line">      - patch</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">Calico monitors various CRDs <span class="keyword">for</span> config.</span></span><br><span class="line">  - apiGroups: [&quot;crd.projectcalico.org&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - globalfelixconfigs</span><br><span class="line">      - felixconfigurations</span><br><span class="line">      - bgppeers</span><br><span class="line">      - globalbgpconfigs</span><br><span class="line">      - bgpconfigurations</span><br><span class="line">      - ippools</span><br><span class="line">      - ipamblocks</span><br><span class="line">      - globalnetworkpolicies</span><br><span class="line">      - globalnetworksets</span><br><span class="line">      - networkpolicies</span><br><span class="line">      - networksets</span><br><span class="line">      - clusterinformations</span><br><span class="line">      - hostendpoints</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">Calico must create and update some CRDs on startup.</span></span><br><span class="line">  - apiGroups: [&quot;crd.projectcalico.org&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - ippools</span><br><span class="line">      - felixconfigurations</span><br><span class="line">      - clusterinformations</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">      - update</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">Calico stores some configuration information on the node.</span></span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - nodes</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">These permissions are only required <span class="keyword">for</span> upgrade from v2.6, and can</span></span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">be removed after upgrade or on fresh installations.</span></span><br><span class="line">  - apiGroups: [&quot;crd.projectcalico.org&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - bgpconfigurations</span><br><span class="line">      - bgppeers</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">      - update</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">These permissions are required <span class="keyword">for</span> Calico CNI to perform IPAM allocations.</span></span><br><span class="line">  - apiGroups: [&quot;crd.projectcalico.org&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - blockaffinities</span><br><span class="line">      - ipamblocks</span><br><span class="line">      - ipamhandles</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - create</span><br><span class="line">      - update</span><br><span class="line">      - delete</span><br><span class="line">  - apiGroups: [&quot;crd.projectcalico.org&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - ipamconfigs</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">Block affinities must also be watchable by confd <span class="keyword">for</span> route aggregation.</span></span><br><span class="line">  - apiGroups: [&quot;crd.projectcalico.org&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - blockaffinities</span><br><span class="line">    verbs:</span><br><span class="line">      - watch</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">The Calico IPAM migration needs to get daemonsets. These permissions can be</span></span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">removed <span class="keyword">if</span> not upgrading from an installation using host-local IPAM.</span></span><br><span class="line">  - apiGroups: [&quot;apps&quot;]</span><br><span class="line">    resources:</span><br><span class="line">      - daemonsets</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-node</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: calico-node</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: calico-node</span><br><span class="line">  namespace: kube-system</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Source: calico/templates/calico-node.yaml</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">This manifest installs the calico-node container, as well</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">as the CNI plugins and network config on</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">each master and worker node <span class="keyword">in</span> a Kubernetes cluster.</span></span><br><span class="line">kind: DaemonSet</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-node</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: calico-node</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: calico-node</span><br><span class="line">  updateStrategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: calico-node</span><br><span class="line">      annotations:</span><br><span class="line">        # This, along with the CriticalAddonsOnly toleration below,</span><br><span class="line">        # marks the pod as a critical add-on, ensuring it gets</span><br><span class="line">        # priority scheduling and that its resources are reserved</span><br><span class="line">        # if it ever gets evicted.</span><br><span class="line">    spec:</span><br><span class="line">      nodeSelector:</span><br><span class="line">        beta.kubernetes.io/os: linux</span><br><span class="line">      hostNetwork: true</span><br><span class="line">      tolerations:</span><br><span class="line">        # Make sure calico-node gets scheduled on all nodes.</span><br><span class="line">        - effect: NoSchedule</span><br><span class="line">          operator: Exists</span><br><span class="line">        # Mark the pod as a critical add-on for rescheduling.</span><br><span class="line">        - key: CriticalAddonsOnly</span><br><span class="line">          operator: Exists</span><br><span class="line">        - effect: NoExecute</span><br><span class="line">          operator: Exists</span><br><span class="line">      serviceAccountName: calico-node</span><br><span class="line">      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a &quot;force</span><br><span class="line">      # deletion&quot;: https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.</span><br><span class="line">      terminationGracePeriodSeconds: 0</span><br><span class="line">      priorityClassName: system-node-critical</span><br><span class="line">      initContainers:</span><br><span class="line">        # This container performs upgrade from host-local IPAM to calico-ipam.</span><br><span class="line">        # It can be deleted if this is a fresh installation, or if you have already</span><br><span class="line">        # upgraded to use calico-ipam.</span><br><span class="line">        - name: upgrade-ipam</span><br><span class="line">          image: sea.hub:5000/calico/cni:v3.8.2</span><br><span class="line">          command: [&quot;/opt/cni/bin/calico-ipam&quot;, &quot;-upgrade&quot;]</span><br><span class="line">          env:</span><br><span class="line">            - name: KUBERNETES_NODE_NAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: spec.nodeName</span><br><span class="line">            - name: CALICO_NETWORKING_BACKEND</span><br><span class="line">              valueFrom:</span><br><span class="line">                configMapKeyRef:</span><br><span class="line">                  name: calico-config</span><br><span class="line">                  key: calico_backend</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - mountPath: /var/lib/cni/networks</span><br><span class="line">              name: host-local-net-dir</span><br><span class="line">            - mountPath: /host/opt/cni/bin</span><br><span class="line">              name: cni-bin-dir</span><br><span class="line">        # This container installs the CNI binaries</span><br><span class="line">        # and CNI network config file on each node.</span><br><span class="line">        - name: install-cni</span><br><span class="line">          image: sea.hub:5000/calico/cni:v3.8.2</span><br><span class="line">          command: [&quot;/install-cni.sh&quot;]</span><br><span class="line">          env:</span><br><span class="line">            # Name of the CNI config file to create.</span><br><span class="line">            - name: CNI_CONF_NAME</span><br><span class="line">              value: &quot;10-calico.conflist&quot;</span><br><span class="line">            # The CNI network config to install on each node.</span><br><span class="line">            - name: CNI_NETWORK_CONFIG</span><br><span class="line">              valueFrom:</span><br><span class="line">                configMapKeyRef:</span><br><span class="line">                  name: calico-config</span><br><span class="line">                  key: cni_network_config</span><br><span class="line">            # Set the hostname based on the k8s node name.</span><br><span class="line">            - name: KUBERNETES_NODE_NAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: spec.nodeName</span><br><span class="line">            # CNI MTU Config variable</span><br><span class="line">            - name: CNI_MTU</span><br><span class="line">              valueFrom:</span><br><span class="line">                configMapKeyRef:</span><br><span class="line">                  name: calico-config</span><br><span class="line">                  key: veth_mtu</span><br><span class="line">            # Prevents the container from sleeping forever.</span><br><span class="line">            - name: SLEEP</span><br><span class="line">              value: &quot;false&quot;</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - mountPath: /host/opt/cni/bin</span><br><span class="line">              name: cni-bin-dir</span><br><span class="line">            - mountPath: /host/etc/cni/net.d</span><br><span class="line">              name: cni-net-dir</span><br><span class="line">        # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes</span><br><span class="line">        # to communicate with Felix over the Policy Sync API.</span><br><span class="line">        - name: flexvol-driver</span><br><span class="line">          image: sea.hub:5000/calico/pod2daemon-flexvol:v3.8.2</span><br><span class="line">          volumeMounts:</span><br><span class="line">          - name: flexvol-driver-host</span><br><span class="line">            mountPath: /host/driver</span><br><span class="line">      containers:</span><br><span class="line">        # Runs calico-node container on each Kubernetes node.  This</span><br><span class="line">        # container programs network policy and routes on each</span><br><span class="line">        # host.</span><br><span class="line">        - name: calico-node</span><br><span class="line">          image: sea.hub:5000/calico/node:v3.8.2</span><br><span class="line">          env:</span><br><span class="line">            # Use Kubernetes API as the backing datastore.</span><br><span class="line">            - name: DATASTORE_TYPE</span><br><span class="line">              value: &quot;kubernetes&quot;</span><br><span class="line">            # Wait for the datastore.</span><br><span class="line">            - name: WAIT_FOR_DATASTORE</span><br><span class="line">              value: &quot;true&quot;</span><br><span class="line">            # Set based on the k8s node name.</span><br><span class="line">            - name: NODENAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: spec.nodeName</span><br><span class="line">            # Choose the backend to use.</span><br><span class="line">            - name: CALICO_NETWORKING_BACKEND</span><br><span class="line">              valueFrom:</span><br><span class="line">                configMapKeyRef:</span><br><span class="line">                  name: calico-config</span><br><span class="line">                  key: calico_backend</span><br><span class="line">            # Cluster type to identify the deployment type</span><br><span class="line">            - name: CLUSTER_TYPE</span><br><span class="line">              value: &quot;k8s,bgp&quot;</span><br><span class="line">            # Auto-detect the BGP IP address.</span><br><span class="line">            - name: IP</span><br><span class="line">              value: &quot;autodetect&quot;</span><br><span class="line">            - name: IP_AUTODETECTION_METHOD</span><br><span class="line">              value: &quot;interface=eth0&quot;</span><br><span class="line">            # Enable IPIP</span><br><span class="line">            - name: CALICO_IPV4POOL_IPIP</span><br><span class="line">              value: &quot;Off&quot;</span><br><span class="line">            # Set MTU for tunnel device used if ipip is enabled</span><br><span class="line">            - name: FELIX_IPINIPMTU</span><br><span class="line">              valueFrom:</span><br><span class="line">                configMapKeyRef:</span><br><span class="line">                  name: calico-config</span><br><span class="line">                  key: veth_mtu</span><br><span class="line">            # The default IPv4 pool to create on startup if none exists. Pod IPs will be</span><br><span class="line">            # chosen from this range. Changing this value after installation will have</span><br><span class="line">            - name: CALICO_IPV4POOL_CIDR</span><br><span class="line">              value: &quot;100.64.0.0/10&quot;</span><br><span class="line">            - name: CALICO_DISABLE_FILE_LOGGING</span><br><span class="line">              value: &quot;true&quot;</span><br><span class="line">            # Set Felix endpoint to host default action to ACCEPT.</span><br><span class="line">            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION</span><br><span class="line">              value: &quot;ACCEPT&quot;</span><br><span class="line">            # Disable IPv6 on Kubernetes.</span><br><span class="line">            - name: FELIX_IPV6SUPPORT</span><br><span class="line">              value: &quot;false&quot;</span><br><span class="line">            # Set Felix logging to &quot;info&quot;</span><br><span class="line">            - name: FELIX_LOGSEVERITYSCREEN</span><br><span class="line">              value: &quot;info&quot;</span><br><span class="line">            - name: FELIX_HEALTHENABLED</span><br><span class="line">              value: &quot;true&quot;</span><br><span class="line">          securityContext:</span><br><span class="line">            privileged: true</span><br><span class="line">          resources:</span><br><span class="line">            requests:</span><br><span class="line">              cpu: 250m</span><br><span class="line">          livenessProbe:</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /liveness</span><br><span class="line">              port: 9099</span><br><span class="line">              host: localhost</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            initialDelaySeconds: 10</span><br><span class="line">            failureThreshold: 6</span><br><span class="line">          readinessProbe:</span><br><span class="line">            exec:</span><br><span class="line">              command:</span><br><span class="line">              - /bin/calico-node</span><br><span class="line">              - -bird-ready</span><br><span class="line">              - -felix-ready</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - mountPath: /lib/modules</span><br><span class="line">              name: lib-modules</span><br><span class="line">              readOnly: true</span><br><span class="line">            - mountPath: /run/xtables.lock</span><br><span class="line">              name: xtables-lock</span><br><span class="line">              readOnly: false</span><br><span class="line">            - mountPath: /var/run/calico</span><br><span class="line">              name: var-run-calico</span><br><span class="line">              readOnly: false</span><br><span class="line">            - mountPath: /var/lib/calico</span><br><span class="line">              name: var-lib-calico</span><br><span class="line">              readOnly: false</span><br><span class="line">            - name: policysync</span><br><span class="line">              mountPath: /var/run/nodeagent</span><br><span class="line">      volumes:</span><br><span class="line">        # Used by calico-node.</span><br><span class="line">        - name: lib-modules</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /lib/modules</span><br><span class="line">        - name: var-run-calico</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /var/run/calico</span><br><span class="line">        - name: var-lib-calico</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /var/lib/calico</span><br><span class="line">        - name: xtables-lock</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /run/xtables.lock</span><br><span class="line">            type: FileOrCreate</span><br><span class="line">        # Used to install CNI.</span><br><span class="line">        - name: cni-bin-dir</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /opt/cni/bin</span><br><span class="line">        - name: cni-net-dir</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /etc/cni/net.d</span><br><span class="line">        # Mount in the directory for host-local IPAM allocations. This is</span><br><span class="line">        # used when upgrading from host-local to calico-ipam, and can be removed</span><br><span class="line">        # if not using the upgrade-ipam init container.</span><br><span class="line">        - name: host-local-net-dir</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /var/lib/cni/networks</span><br><span class="line">        # Used to create per-pod Unix Domain Sockets</span><br><span class="line">        - name: policysync</span><br><span class="line">          hostPath:</span><br><span class="line">            type: DirectoryOrCreate</span><br><span class="line">            path: /var/run/nodeagent</span><br><span class="line">        # Used to install Flex Volume Driver</span><br><span class="line">        - name: flexvol-driver-host</span><br><span class="line">          hostPath:</span><br><span class="line">            type: DirectoryOrCreate</span><br><span class="line">            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-node</span><br><span class="line">  namespace: kube-system</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Source: calico/templates/calico-kube-controllers.yaml</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">See https://github.com/projectcalico/kube-controllers</span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-kube-controllers</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: calico-kube-controllers</span><br><span class="line">spec:</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">The controllers can only have a single active instance.</span></span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: calico-kube-controllers</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: calico-kube-controllers</span><br><span class="line">      namespace: kube-system</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: calico-kube-controllers</span><br><span class="line">      annotations:</span><br><span class="line">    spec:</span><br><span class="line">      nodeSelector:</span><br><span class="line">        beta.kubernetes.io/os: linux</span><br><span class="line">      tolerations:</span><br><span class="line">        # Mark the pod as a critical add-on for rescheduling.</span><br><span class="line">        - key: CriticalAddonsOnly</span><br><span class="line">          operator: Exists</span><br><span class="line">        - key: node-role.kubernetes.io/master</span><br><span class="line">          effect: NoSchedule</span><br><span class="line">      serviceAccountName: calico-kube-controllers</span><br><span class="line">      priorityClassName: system-cluster-critical</span><br><span class="line">      containers:</span><br><span class="line">        - name: calico-kube-controllers</span><br><span class="line">          image: sea.hub:5000/calico/kube-controllers:v3.8.2</span><br><span class="line">          env:</span><br><span class="line">            # Choose which controllers to run.</span><br><span class="line">            - name: ENABLED_CONTROLLERS</span><br><span class="line">              value: node</span><br><span class="line">            - name: DATASTORE_TYPE</span><br><span class="line">              value: kubernetes</span><br><span class="line">          readinessProbe:</span><br><span class="line">            exec:</span><br><span class="line">              command:</span><br><span class="line">              - /usr/bin/check-status</span><br><span class="line">              - -r</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-kube-controllers</span><br><span class="line">  namespace: kube-system</span><br><span class="line">&#x27; | kubectl apply -f -</span><br><span class="line">configmap/calico-config created</span><br><span class="line">Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/calico-node created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/calico-node created</span><br><span class="line">daemonset.apps/calico-node created</span><br><span class="line">serviceaccount/calico-node created</span><br><span class="line">deployment.apps/calico-kube-controllers created</span><br><span class="line">serviceaccount/calico-kube-controllers created</span><br></pre></td></tr></table></figure>

<p>至此，kubernetes集群部署完成，查看集群状态：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">[root@node1]# </span><span class="language-bash">kubectl get node -owide</span></span><br><span class="line">NAME       STATUS   ROLES    AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME</span><br><span class="line">node1   Ready    master   2m50s   v1.19.9   10.10.11.49   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-862.11.6.el7.x86_64   docker://19.3.0</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">[root@node1]# </span><span class="language-bash">kubectl get pod -A  -owide</span></span><br><span class="line">NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP              NODE       NOMINATED NODE   READINESS GATES</span><br><span class="line">kube-system   calico-kube-controllers-5565b777b6-w9mhw   1/1     Running   0          2m32s   100.76.153.65   node1</span><br><span class="line">kube-system   calico-node-mwkg2                          1/1     Running   0          2m32s   10.10.11.49    node1</span><br><span class="line">kube-system   coredns-597c5579bc-dpqbx                   1/1     Running   0          2m32s   100.76.153.64   node1</span><br><span class="line">kube-system   coredns-597c5579bc-fjnmq                   1/1     Running   0          2m32s   100.76.153.66   node1</span><br><span class="line">kube-system   etcd-node1                                 1/1     Running   0          2m51s   10.10.11.49    node1</span><br><span class="line">kube-system   kube-apiserver-node1                       1/1     Running   0          2m51s   10.10.11.49    node1</span><br><span class="line">kube-system   kube-controller-manager-node1              1/1     Running   0          2m51s   10.10.11.49    node1</span><br><span class="line">kube-system   kube-proxy-qgt9w                           1/1     Running   0          2m32s   10.10.11.49    node1</span><br><span class="line">kube-system   kube-scheduler-node1                       1/1     Running   0          2m51s   10.10.11.49    node1</span><br></pre></td></tr></table></figure>

<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/alibaba/sealer/blob/main/docs/README_zh.md">https://github.com/alibaba/sealer/blob/main/docs/README_zh.md</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">LeaoYao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">87</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LeaoYao</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
